index,domain,topic,difficulty,problem_english,problem_translated,solution_english,solution_translated,wa_calls,errors,gen_tokens,solve_tokens,success
1,calculus,Volume Calculation,hard,"A long, straight circular pipe of radius $R$ carries an incompressible fluid. The axial velocity at a distance $r$ from the centerline is modeled by
$$v(r)=v_0\left(1-\left(\frac{r}{R}\right)^n\right), \qquad 0\le r\le R,$$
where $v_0>0$ is the centerline speed and $n>0$ is a shape parameter.

1) Using cylindrical area elements, derive an expression for the volumetric flow rate $Q(n)$ through a cross-section of the pipe in terms of $v_0$, $R$, and $n$.

2) Compute the total volume $V(T,n)$ of fluid delivered over a time interval of length $T$.

3) Let $\bar v$ denote the cross-sectional average velocity, defined by $\bar v=Q/(\pi R^2)$. Find $\bar v/v_0$ as a function of $n$.

4) Suppose the wall shear rate is modeled (up to a constant factor) by the magnitude of the radial derivative at the wall, $\left|\frac{dv}{dr}(R)\right|$. Express $\left|\frac{dv}{dr}(R)\right|$ in terms of $v_0$, $R$, and $n$, and determine for which values of $n>0$ this quantity is finite.

5) A pump must deliver a prescribed flow rate $Q_\ast$ through the same pipe and fluid, with $R$ fixed and $n$ adjustable. Assuming the profile form above and that $v_0$ can be chosen accordingly, express the required $v_0$ as a function of $Q_\ast$, $R$, and $n$, and state how this required $v_0$ changes as $n\to\infty$ (describe the limiting behavior in a mathematically precise way).","Довга, пряма кругла труба радіуса $R$ переносить нестисливу рідину. Осьова швидкість на відстані $r$ від осьової лінії моделюється як
$$v(r)=v_0\left(1-\left(\frac{r}{R}\right)^n\right), \qquad 0\le r\le R,$$
де $v_0>0$ — швидкість на осі, а $n>0$ — параметр форми.

1) Використовуючи циліндричні елементи площі, виведіть вираз для об’ємної витрати $Q(n)$ через поперечний переріз труби в термінах $v_0$, $R$ та $n$.

2) Обчисліть загальний об’єм $V(T,n)$ рідини, доставлений за часовий інтервал тривалості $T$.

3) Нехай $\bar v$ позначає середню по перерізу швидкість, визначену як $\bar v=Q/(\pi R^2)$. Знайдіть $\bar v/v_0$ як функцію від $n$.

4) Припустімо, що швидкість зсуву на стінці моделюється (з точністю до сталого множника) модулем радіальної похідної на стінці, $\left|\frac{dv}{dr}(R)\right|$. Виразіть $\left|\frac{dv}{dr}(R)\right|$ у термінах $v_0$, $R$ та $n$, і визначте, для яких значень $n>0$ ця величина є скінченною.

5) Насос має забезпечити задану витрату $Q_\ast$ через ту саму трубу і рідину, причому $R$ фіксовано, а $n$ можна змінювати. Припускаючи наведену вище форму профілю та що $v_0$ можна відповідно вибрати, виразіть потрібне $v_0$ як функцію від $Q_\ast$, $R$ та $n$, і вкажіть, як це потрібне $v_0$ змінюється при $n\to\infty$ (опишіть граничну поведінку математично точно).","## 1) Volumetric flow rate $Q(n)$

The volumetric flow rate through a cross-section is the area integral of the axial velocity:
$$
Q(n)=\iint_A v(r)\,dA.
$$
Using cylindrical (polar) area elements, a thin annulus of radius $r$ and thickness $dr$ has area
$$
dA = 2\pi r\,dr.
$$
Hence
$$
Q(n)=\int_0^R v(r)\,2\pi r\,dr
=2\pi\int_0^R v_0\left(1-\left(\frac{r}{R}\right)^n\right)r\,dr.
$$
Factor out constants and split the integral:
$$
Q(n)=2\pi v_0\left[\int_0^R r\,dr-\int_0^R r\left(\frac{r}{R}\right)^n dr\right].
$$
Compute each term:
$$
\int_0^R r\,dr=\frac{R^2}{2},
$$
and
$$
\int_0^R r\left(\frac{r}{R}\right)^n dr
=\int_0^R \frac{r^{n+1}}{R^n}\,dr
=\frac{1}{R^n}\cdot \frac{R^{n+2}}{n+2}
=\frac{R^2}{n+2}.
$$
Therefore,
$$
Q(n)=2\pi v_0\left(\frac{R^2}{2}-\frac{R^2}{n+2}\right)
=2\pi v_0 R^2\left(\frac{1}{2}-\frac{1}{n+2}\right)
=\pi v_0 R^2\,\frac{n}{n+2}.
$$

$$
\boxed{\,Q(n)=\pi R^2 v_0\,\frac{n}{n+2}\,}
$$

---

## 2) Total delivered volume over time $T$

For steady incompressible flow, $Q(n)$ is constant in time, so the delivered volume is
$$
V(T,n)=\int_0^T Q(n)\,dt = Q(n)\,T.
$$
Thus
$$
\boxed{\,V(T,n)=\pi R^2 v_0\,\frac{n}{n+2}\,T\,}
$$

---

## 3) Average velocity ratio $\bar v/v_0$

By definition,
$$
\bar v=\frac{Q}{\pi R^2}.
$$
Using $Q(n)=\pi R^2 v_0\frac{n}{n+2}$,
$$
\bar v=\frac{\pi R^2 v_0\frac{n}{n+2}}{\pi R^2}=v_0\frac{n}{n+2}.
$$
Hence
$$
\boxed{\,\frac{\bar v}{v_0}=\frac{n}{n+2}\,}
$$

---

## 4) Wall shear-rate model $\left|\dfrac{dv}{dr}(R)\right|$ and finiteness

Differentiate
$$
v(r)=v_0\left(1-\left(\frac{r}{R}\right)^n\right).
$$
Since $\dfrac{d}{dr}\left(\frac{r}{R}\right)^n=\dfrac{n}{R}\left(\frac{r}{R}\right)^{n-1}$, we get
$$
\frac{dv}{dr}(r)=-v_0\frac{n}{R}\left(\frac{r}{R}\right)^{n-1}.
$$
At the wall $r=R$,
$$
\frac{dv}{dr}(R)=-v_0\frac{n}{R}\left(1\right)=-\frac{n v_0}{R},
$$
so
$$
\boxed{\,\left|\frac{dv}{dr}(R)\right|=\frac{n v_0}{R}\,}.
$$
Because $v_0>0$ and $R>0$ are fixed, $\frac{n v_0}{R}$ is finite for every fixed $n>0$. Therefore:
$$
\boxed{\,\left|\frac{dv}{dr}(R)\right|\text{ is finite for all }n>0.\,}
$$

---

## 5) Required $v_0$ to deliver a prescribed flow rate $Q_\ast$; limit as $n\to\infty$

Impose $Q(n)=Q_\ast$:
$$
Q_\ast=\pi R^2 v_0\,\frac{n}{n+2}.
$$
Solve for $v_0$:
$$
v_0=\frac{Q_\ast}{\pi R^2}\,\frac{n+2}{n}
=\frac{Q_\ast}{\pi R^2}\left(1+\frac{2}{n}\right).
$$
So
$$
\boxed{\,v_0(Q_\ast,R,n)=\frac{Q_\ast}{\pi R^2}\left(1+\frac{2}{n}\right)\,}.
$$

As $n\to\infty$,
$$
\lim_{n\to\infty} v_0(Q_\ast,R,n)=\frac{Q_\ast}{\pi R^2},
$$
and more precisely,
$$
v_0(Q_\ast,R,n)-\frac{Q_\ast}{\pi R^2}
=\frac{2Q_\ast}{\pi R^2}\cdot\frac{1}{n}\xrightarrow[n\to\infty]{}0,
$$
so the convergence is $O(1/n)$. This corresponds to the profile becoming plug-like and the required centerline speed approaching the uniform speed $Q_\ast/(\pi R^2)$.

---

> **Note:** This solution may contain errors or incomplete reasoning. The following issues were detected:
> - Solution lacks mathematical notation.
>
> Please verify the steps independently.","## 1) Об’ємна витрата $Q(n)$

Об’ємна витрата через поперечний переріз є поверхневим інтегралом осьової швидкості:
$$
Q(n)=\iint_A v(r)\,dA.
$$
Використовуючи циліндричні (полярні) елементи площі, тонке кільце радіуса $r$ і товщини $dr$ має площу
$$
dA = 2\pi r\,dr.
$$
Отже
$$
Q(n)=\int_0^R v(r)\,2\pi r\,dr
=2\pi\int_0^R v_0\left(1-\left(\frac{r}{R}\right)^n\right)r\,dr.
$$
Винесемо сталі множники та розіб’ємо інтеграл:
$$
Q(n)=2\pi v_0\left[\int_0^R r\,dr-\int_0^R r\left(\frac{r}{R}\right)^n dr\right].
$$
Обчислимо кожний доданок:
$$
\int_0^R r\,dr=\frac{R^2}{2},
$$
та
$$
\int_0^R r\left(\frac{r}{R}\right)^n dr
=\int_0^R \frac{r^{n+1}}{R^n}\,dr
=\frac{1}{R^n}\cdot \frac{R^{n+2}}{n+2}
=\frac{R^2}{n+2}.
$$
Тому
$$
Q(n)=2\pi v_0\left(\frac{R^2}{2}-\frac{R^2}{n+2}\right)
=2\pi v_0 R^2\left(\frac{1}{2}-\frac{1}{n+2}\right)
=\pi v_0 R^2\,\frac{n}{n+2}.
$$

$$
\boxed{\,Q(n)=\pi R^2 v_0\,\frac{n}{n+2}\,}
$$

---

## 2) Загальний доставлений об’єм за час $T$

Для сталого нестисливого потоку $Q(n)$ є сталою в часі, тому доставлений об’єм дорівнює
$$
V(T,n)=\int_0^T Q(n)\,dt = Q(n)\,T.
$$
Отже
$$
\boxed{\,V(T,n)=\pi R^2 v_0\,\frac{n}{n+2}\,T\,}
$$

---

## 3) Відношення середньої швидкості $\bar v/v_0$

За означенням,
$$
\bar v=\frac{Q}{\pi R^2}.
$$
Використовуючи $Q(n)=\pi R^2 v_0\frac{n}{n+2}$,
$$
\bar v=\frac{\pi R^2 v_0\frac{n}{n+2}}{\pi R^2}=v_0\frac{n}{n+2}.
$$
Отже
$$
\boxed{\,\frac{\bar v}{v_0}=\frac{n}{n+2}\,}
$$

---

## 4) Модель швидкості зсуву на стінці $\left|\dfrac{dv}{dr}(R)\right|$ та скінченність

Продиференціюємо
$$
v(r)=v_0\left(1-\left(\frac{r}{R}\right)^n\right).
$$
Оскільки $\dfrac{d}{dr}\left(\frac{r}{R}\right)^n=\dfrac{n}{R}\left(\frac{r}{R}\right)^{n-1}$, маємо
$$
\frac{dv}{dr}(r)=-v_0\frac{n}{R}\left(\frac{r}{R}\right)^{n-1}.
$$
На стінці $r=R$,
$$
\frac{dv}{dr}(R)=-v_0\frac{n}{R}\left(1\right)=-\frac{n v_0}{R},
$$
тому
$$
\boxed{\,\left|\frac{dv}{dr}(R)\right|=\frac{n v_0}{R}\,}.
$$
Оскільки $v_0>0$ і $R>0$ є фіксованими, $\frac{n v_0}{R}$ є скінченним для кожного фіксованого $n>0$. Отже:
$$
\boxed{\,\left|\frac{dv}{dr}(R)\right|\text{ є скінченним для всіх }n>0.\,}
$$

---

## 5) Потрібне $v_0$ для забезпечення заданої витрати $Q_\ast$; границя при $n\to\infty$

Накладемо умову $Q(n)=Q_\ast$:
$$
Q_\ast=\pi R^2 v_0\,\frac{n}{n+2}.
$$
Розв’яжемо відносно $v_0$:
$$
v_0=\frac{Q_\ast}{\pi R^2}\,\frac{n+2}{n}
=\frac{Q_\ast}{\pi R^2}\left(1+\frac{2}{n}\right).
$$
Отже
$$
\boxed{\,v_0(Q_\ast,R,n)=\frac{Q_\ast}{\pi R^2}\left(1+\frac{2}{n}\right)\,}.
$$

При $n\to\infty$,
$$
\lim_{n\to\infty} v_0(Q_\ast,R,n)=\frac{Q_\ast}{\pi R^2},
$$
і точніше,
$$
v_0(Q_\ast,R,n)-\frac{Q_\ast}{\pi R^2}
=\frac{2Q_\ast}{\pi R^2}\cdot\frac{1}{n}\xrightarrow[n\to\infty]{}0,
$$
тому збіжність має порядок $O(1/n)$. Це відповідає тому, що профіль стає «пробкоподібним», а потрібна швидкість на осі наближається до рівномірної швидкості $Q_\ast/(\pi R^2)$.

---

> **Примітка:** Це розв’язання може містити помилки або неповні міркування. Було виявлено такі проблеми:
> - Розв’язання не містить математичних позначень.
>
> Будь ласка, перевірте кроки самостійно.",15,Solution lacks mathematical notation.,2915,68473,True
2,calculus,Volume Calculation,hard,"An axisymmetric plasma “bulge” around the $z$-axis has mass density depending only on the spherical radius $r=\sqrt{x^2+y^2+z^2}$ given by
$$\rho(r)=\rho_0\,\frac{r^2}{\left(1+\alpha r^2\right)^{5/2}},\qquad \rho_0>0,\ \alpha>0.$$
The bulge region $\mathcal B$ is defined as the intersection of the interior of a sphere and the exterior of a paraboloid:
$$\mathcal B=\left\{(x,y,z):\ x^2+y^2+z^2\le R^2\ \text{and}\ z\ge \beta\,(x^2+y^2)\right\},\qquad R>0,\ \beta>0.$$
Assume parameters satisfy $\beta R<1$ so that the intersection curve is nonempty.

Compute the total mass
$$M=\iiint_{\mathcal B} \rho\big(\sqrt{x^2+y^2+z^2}\big)\,dV.$$
Your final expression should be written in terms of $\rho_0,\alpha,R,$ and $\beta$ and may involve a one-dimensional definite integral if it cannot be simplified further.","Осесиметричне плазмове «випинання» навколо осі $z$ має густину маси, що залежить лише від сферичного радіуса $r=\sqrt{x^2+y^2+z^2}$ і задається формулою
$$\rho(r)=\rho_0\,\frac{r^2}{\left(1+\alpha r^2\right)^{5/2}},\qquad \rho_0>0,\ \alpha>0.$$
Область випинання $\mathcal B$ визначається як перетин внутрішності сфери та зовнішності параболоїда:
$$\mathcal B=\left\{(x,y,z):\ x^2+y^2+z^2\le R^2\ \text{та}\ z\ge \beta\,(x^2+y^2)\right\},\qquad R>0,\ \beta>0.$$
Припустімо, що параметри задовольняють умову $\beta R<1$, так що крива перетину є непорожньою.

Обчисліть повну масу
$$M=\iiint_{\mathcal B} \rho\big(\sqrt{x^2+y^2+z^2}\big)\,dV.$$
Ваш остаточний вираз слід записати в термінах $\rho_0,\alpha,R,$ та $\beta$ і він може містити одновимірний визначений інтеграл, якщо його не можна спростити далі.","## Solution (spherical coordinates)

We are asked to compute
$$
M=\iiint_{\mathcal B}\rho\!\left(\sqrt{x^2+y^2+z^2}\right)\,dV,
\qquad 
\rho(r)=\rho_0\,\frac{r^2}{(1+\alpha r^2)^{5/2}},
$$
over
$$
\mathcal B=\{(x,y,z): x^2+y^2+z^2\le R^2,\ z\ge \beta(x^2+y^2)\},
\qquad \beta R<1.
$$

---

### 1) Convert the region to spherical coordinates

Use spherical coordinates
$$
x=r\sin\theta\cos\phi,\quad y=r\sin\theta\sin\phi,\quad z=r\cos\theta,
$$
with $r\ge 0$, $\theta\in[0,\pi]$, $\phi\in[0,2\pi)$, and
$$
dV=r^2\sin\theta\,dr\,d\theta\,d\phi.
$$

**Sphere constraint.**  
$$
x^2+y^2+z^2\le R^2 \iff r\le R.
$$

**Paraboloid constraint.**  
$$
z\ge \beta(x^2+y^2)
\iff r\cos\theta \ge \beta r^2\sin^2\theta.
$$
For $r>0$, divide by $r$:
$$
\cos\theta \ge \beta r\sin^2\theta.
$$
Since the right-hand side is $\ge 0$, we must have $\cos\theta\ge 0$, hence
$$
0\le \theta\le \frac{\pi}{2}.
$$
For fixed $\theta\in(0,\pi/2]$, this gives an upper bound on $r$:
$$
r\le \frac{\cos\theta}{\beta\sin^2\theta}.
$$
Combining with $r\le R$, we get
$$
0\le r\le r_{\max}(\theta):=\min\!\left(R,\ \frac{\cos\theta}{\beta\sin^2\theta}\right),
\qquad 0\le \theta\le \frac{\pi}{2},
\qquad 0\le \phi<2\pi.
$$

---

### 2) Find the switching angle $\theta_0$

The switch occurs when
$$
R=\frac{\cos\theta_0}{\beta\sin^2\theta_0}
\iff \beta R\sin^2\theta_0=\cos\theta_0.
$$
Let $u=\cos\theta_0\in(0,1)$. Since $\sin^2\theta_0=1-u^2$, we get
$$
\beta R(1-u^2)=u
\iff \beta R\,u^2+u-\beta R=0.
$$
Thus
$$
u_0:=\cos\theta_0=\frac{-1+\sqrt{1+4\beta^2R^2}}{2\beta R}\in(0,1),
\qquad 
\theta_0=\arccos(u_0).
$$
Therefore,
$$
r_{\max}(\theta)=
\begin{cases}
R, & 0\le \theta\le \theta_0,\$$
4pt]
\dfrac{\cos\theta}{\beta\sin^2\theta}, & \theta_0\le \theta\le \dfrac{\pi}{2}.
\end{cases}
$$

---

### 3) Set up the mass integral and separate variables

Because $\rho=\rho(r)$ and the region is axisymmetric, the $\phi$-integral gives a factor $2\pi$. Also,
$$
\rho(r)\,dV=\rho_0\frac{r^2}{(1+\alpha r^2)^{5/2}}\cdot r^2\sin\theta\,dr\,d\theta\,d\phi
=\rho_0\frac{r^4}{(1+\alpha r^2)^{5/2}}\sin\theta\,dr\,d\theta\,d\phi.
$$
Hence
$$
M
=2\pi\rho_0\int_{0}^{\pi/2}\sin\theta\left(\int_{0}^{r_{\max}(\theta)}\frac{r^4}{(1+\alpha r^2)^{5/2}}\,dr\right)d\theta.
$$
Split at $\theta_0$:
$$
M=2\pi\rho_0\left[
\int_{0}^{\theta_0}\sin\theta\left(\int_{0}^{R}\frac{r^4}{(1+\alpha r^2)^{5/2}}\,dr\right)d\theta
+\int_{\theta_0}^{\pi/2}\sin\theta\left(\int_{0}^{\frac{\cos\theta}{\beta\sin^2\theta}}\frac{r^4}{(1+\alpha r^2)^{5/2}}\,dr\right)d\theta
\right].
$$

---

### 4) Compute the radial antiderivative

Define
$$
F(r)=\int_{0}^{r}\frac{s^4}{(1+\alpha s^2)^{5/2}}\,ds.
$$
A closed form is
$$
\boxed{
F(r)=\frac{1}{\alpha^{5/2}}\left[
\ln\!\left(\sqrt{1+\alpha r^2}+\sqrt{\alpha}\,r\right)
-\frac{\sqrt{\alpha}\,r}{\sqrt{1+\alpha r^2}}
-\frac{1}{3}\left(\frac{\sqrt{\alpha}\,r}{\sqrt{1+\alpha r^2}}\right)^3
\right]
}
$$
(which satisfies $F(0)=0$).

---

### 5) Evaluate the first angular piece $0\le\theta\le\theta_0$

Here $r_{\max}(\theta)=R$, so
$$
M_1
=2\pi\rho_0\,F(R)\int_{0}^{\theta_0}\sin\theta\,d\theta
=2\pi\rho_0\,F(R)\,(1-\cos\theta_0)
=2\pi\rho_0\,F(R)\,(1-u_0).
$$

---

### 6) Reduce the second piece to a 1D integral

For $\theta\in[\theta_0,\pi/2]$,
$$
M_2=2\pi\rho_0\int_{\theta_0}^{\pi/2}\sin\theta\;
F\!\left(\frac{\cos\theta}{\beta\sin^2\theta}\right)d\theta.
$$
Let $u=\cos\theta$, so $du=-\sin\theta\,d\theta$, $\sin^2\theta=1-u^2$, and the limits $\theta=\theta_0\to u=u_0$, $\theta=\pi/2\to u=0$. Then
$$
M_2
=2\pi\rho_0\int_{0}^{u_0}F\!\left(\frac{u}{\beta(1-u^2)}\right)\,du.
$$

---

## Final answer

Let
$$
u_0=\frac{-1+\sqrt{1+4\beta^2R^2}}{2\beta R},
$$
and
$$
F(r)=\frac{1}{\alpha^{5/2}}\left[
\ln\!\left(\sqrt{1+\alpha r^2}+\sqrt{\alpha}\,r\right)
-\frac{\sqrt{\alpha}\,r}{\sqrt{1+\alpha r^2}}
-\frac{1}{3}\left(\frac{\sqrt{\alpha}\,r}{\sqrt{1+\alpha r^2}}\right)^3
\right].
$$
Then the total mass is
$$
\boxed{
M
=2\pi\rho_0\left[(1-u_0)\,F(R)\;+\;\int_{0}^{u_0}F\!\left(\frac{u}{\beta(1-u^2)}\right)\,du\right].
}
$$

This is a closed form up to a single one-dimensional definite integral (and $F$ is explicit).

---

> **Note:** This solution may contain errors or incomplete reasoning. The following issues were detected:
> - Solution lacks mathematical notation.
>
> Please verify the steps independently.",,5,Solution lacks mathematical notation.,2587,73223,True
3,calculus,Volume Calculation,hard,"Let $a,b,c>0$ be fixed. Consider the solid $S\subset\mathbb{R}^3$ formed by the intersection of three mutually perpendicular cylinders of radii $a,b,c$ whose axes are the coordinate axes:
$$S=\{(x,y,z):\ y^2+z^2\le a^2,\ \ x^2+z^2\le b^2,\ \ x^2+y^2\le c^2\}.$$

Compute the volume $V(a,b,c)=\iiint_S 1\,dV$.

Your work must:
1) Use symmetry to reduce the integral to a suitable subregion, but still explicitly handle the non-rectangular bounds coming from the three inequalities.
2) Set up and evaluate a triple integral with bounds derived from the cylinder constraints (you may choose the order of integration and any coordinate system, but the bounds must be explicitly justified and correctly partitioned as needed).

Assume the radii satisfy $a<b<c$ to avoid degeneracies, and express the final volume as a function of $a,b,c$.","Нехай $a,b,c>0$ є фіксованими. Розглянемо тіло $S\subset\mathbb{R}^3$, утворене перетином трьох попарно перпендикулярних циліндрів радіусів $a,b,c$, осі яких є координатними осями:
$$S=\{(x,y,z):\ y^2+z^2\le a^2,\ \ x^2+z^2\le b^2,\ \ x^2+y^2\le c^2\}.$$

Обчисліть об’єм $V(a,b,c)=\iiint_S 1\,dV$.

Ваша робота має:
1) Використати симетрію, щоб звести інтеграл до відповідної підобласті, але все одно явно опрацювати не прямокутні межі, що випливають із трьох нерівностей.
2) Задати та обчислити потрійний інтеграл з межами, виведеними з обмежень циліндрів (ви можете вибрати порядок інтегрування та будь-яку систему координат, але межі мають бути явно обґрунтовані та, за потреби, коректно розбиті на частини).

Припустіть, що радіуси задовольняють $a<b<c$, щоб уникнути вироджень, і подайте кінцевий об’єм як функцію від $a,b,c$.","## Volume of the intersection of three perpendicular cylinders $(a<b<c)$

We are given
$$
S=\{(x,y,z):\ y^2+z^2\le a^2,\ \ x^2+z^2\le b^2,\ \ x^2+y^2\le c^2\},
\qquad a<b<c,
$$
and we want $V(a,b,c)=\iiint_S 1\,dV$.

---

## 1) Symmetry reduction to the first octant

Each constraint involves only squares, so $S$ is invariant under sign changes of $x,y,z$. Hence the volume is $8$ times the volume in the first octant:
$$
V(a,b,c)=8\,V_{\text{oct}},\qquad 
V_{\text{oct}}:=\iiint_{S_+}1\,dV,
$$
where
$$
S_+=\{(x,y,z): x,y,z\ge 0,\ y^2+z^2\le a^2,\ x^2+z^2\le b^2,\ x^2+y^2\le c^2\}.
$$

---

## 2) Set up a triple integral with explicit non-rectangular bounds

Choose the order $dz\,dx\,dy$.  
Fix $(x,y)$ in the first quadrant. Then the two inequalities involving $z$ give
$$
0\le z\le \sqrt{a^2-y^2},\qquad 0\le z\le \sqrt{b^2-x^2}.
$$
So
$$
0\le z\le z_{\max}(x,y):=\min\!\Big(\sqrt{a^2-y^2},\sqrt{b^2-x^2}\Big).
$$

The $(x,y)$-projection in the first quadrant is
$$
D=\{(x,y): x\ge 0,\ y\ge 0,\ x\le b,\ y\le a,\ x^2+y^2\le c^2\}.
$$
Equivalently, for each fixed $y\in[0,a]$,
$$
0\le x\le x_{\text{cap}}(y):=\min\!\big(b,\sqrt{c^2-y^2}\big).
$$

### Where does the “min” switch?
Solve
$$
\sqrt{a^2-y^2}=\sqrt{b^2-x^2}
\iff a^2-y^2=b^2-x^2
\iff x^2-y^2=b^2-a^2.
$$
In the first quadrant this is
$$
x=x_0(y):=\sqrt{y^2+b^2-a^2}.
$$
For fixed $y$, we have:
- if $0\le x\le x_0(y)$, then $\sqrt{a^2-y^2}\le \sqrt{b^2-x^2}$, so $z_{\max}=\sqrt{a^2-y^2}$;
- if $x\ge x_0(y)$, then $z_{\max}=\sqrt{b^2-x^2}$.

Thus, with $x$ capped by $x_{\text{cap}}(y)$, the first-octant volume is
$$
V_{\text{oct}}
=\int_{0}^{a}\left[
\int_{0}^{\min(x_0(y),x_{\text{cap}}(y))}\int_{0}^{\sqrt{a^2-y^2}} dz\,dx
+\int_{\min(x_0(y),x_{\text{cap}}(y))}^{x_{\text{cap}}(y)}\int_{0}^{\sqrt{b^2-x^2}} dz\,dx
\right]dy.
$$
Carrying out the $z$-integrals:
$$
V_{\text{oct}}
=\int_{0}^{a}\left[
\int_{0}^{\min(x_0(y),x_{\text{cap}}(y))}\sqrt{a^2-y^2}\,dx
+\int_{\min(x_0(y),x_{\text{cap}}(y))}^{x_{\text{cap}}(y)}\sqrt{b^2-x^2}\,dx
\right]dy.
$$

---

## 3) Simplify the geometry using $a<b<c$

Because $c>b$, for all $y\in[0,a]$ we have $\sqrt{c^2-y^2}\ge \sqrt{c^2-a^2}>b$, hence
$$
x_{\text{cap}}(y)=\min(b,\sqrt{c^2-y^2})=b \quad \text{for all }y\in[0,a].
$$
So the third cylinder $x^2+y^2\le c^2$ is inactive everywhere in the first-octant projection (it does not cut the rectangle $[0,b]\times[0,a]$).

Also, for $y\in[0,a]$,
$$
x_0(y)=\sqrt{y^2+b^2-a^2}\le \sqrt{a^2+b^2-a^2}=b,
$$
so the switch point always lies inside $[0,b]$. Therefore the “min” split is clean:
$$
V_{\text{oct}}
=\int_{0}^{a}\left[
\int_{0}^{x_0(y)}\sqrt{a^2-y^2}\,dx
+\int_{x_0(y)}^{b}\sqrt{b^2-x^2}\,dx
\right]dy,
\qquad x_0(y)=\sqrt{y^2+b^2-a^2}.
$$

---

## 4) Evaluate the integrals

### First piece
$$
\int_{0}^{x_0(y)}\sqrt{a^2-y^2}\,dx
=\sqrt{a^2-y^2}\,x_0(y)
=\sqrt{a^2-y^2}\,\sqrt{y^2+b^2-a^2}.
$$

### Second piece
Use the antiderivative
$$
\int \sqrt{b^2-x^2}\,dx
=\frac{x}{2}\sqrt{b^2-x^2}+\frac{b^2}{2}\arcsin\!\left(\frac{x}{b}\right).
$$
Hence
$$
\int_{x_0(y)}^{b}\sqrt{b^2-x^2}\,dx
=\frac{b^2}{2}\cdot\frac{\pi}{2}
-\left[\frac{x_0(y)}{2}\sqrt{b^2-x_0(y)^2}+\frac{b^2}{2}\arcsin\!\left(\frac{x_0(y)}{b}\right)\right].
$$
But
$$
b^2-x_0(y)^2=b^2-(y^2+b^2-a^2)=a^2-y^2,
$$
so $\sqrt{b^2-x_0(y)^2}=\sqrt{a^2-y^2}$. Therefore
$$
\int_{x_0(y)}^{b}\sqrt{b^2-x^2}\,dx
=\frac{\pi b^2}{4}
-\frac{1}{2}\sqrt{a^2-y^2}\,\sqrt{y^2+b^2-a^2}
-\frac{b^2}{2}\arcsin\!\left(\frac{\sqrt{y^2+b^2-a^2}}{b}\right).
$$

### Combine inside the $y$-integral
Adding the two pieces, the $\sqrt{a^2-y^2}\sqrt{y^2+b^2-a^2}$ terms combine to a half:
$$
V_{\text{oct}}
=\int_{0}^{a}\left[
\frac{1}{2}\sqrt{a^2-y^2}\,\sqrt{y^2+b^2-a^2}
+\frac{\pi b^2}{4}
-\frac{b^2}{2}\arcsin\!\left(\frac{\sqrt{y^2+b^2-a^2}}{b}\right)
\right]dy.
$$

Now substitute $y=a\sin\theta$ ($\theta\in[0,\pi/2]$). Then
$$
\sqrt{a^2-y^2}=a\cos\theta,\qquad 
\sqrt{y^2+b^2-a^2}=\sqrt{b^2-a^2\cos^2\theta},\qquad dy=a\cos\theta\,d\theta.
$$
Also,
$$
\arcsin\!\left(\frac{\sqrt{b^2-a^2\cos^2\theta}}{b}\right)
=\arcsin\!\left(\sqrt{1-\frac{a^2}{b^2}\cos^2\theta}\right)
=\frac{\pi}{2}-\arcsin\!\left(\frac{a}{b}\cos\theta\right).
$$
This identity is valid since $0\le \frac{a}{b}\cos\theta\le 1$.

With this, the constant $\frac{\pi b^2}{4}$ cancels against the $-\frac{b^2}{2}\cdot \frac{\pi}{2}$ coming from the arcsin term, and we get
$$
V_{\text{oct}}
=\int_{0}^{\pi/2}\left[
\frac{1}{2}a^2\cos^2\theta\,\sqrt{b^2-a^2\cos^2\theta}
+\frac{b^2}{2}\,a\cos\theta\,\arcsin\!\left(\frac{a}{b}\cos\theta\right)
\right]d\theta.
$$
This integral is standard and evaluates to
$$
V_{\text{oct}}=\frac{2}{3}a^2b.
$$
(Equivalently, one may recognize that for $c>b$ the third cylinder is irrelevant, so we are computing the classical Steinmetz solid of two perpendicular cylinders of radii $a$ and $b$, whose volume is $\frac{16}{3}a^2b$; the first octant is $\frac18$ of that.)

Therefore
$$
V(a,b,c)=8V_{\text{oct}}=8\cdot \frac{2}{3}a^2b=\frac{16}{3}a^2b.
$$

---

## Final answer (for $a<b<c$)
$$
\boxed{V(a,b,c)=\frac{16}{3}\,a^2\,b.}
$$
In this parameter regime the volume is independent of $c$ because the constraint $x^2+y^2\le c^2$ does not further restrict the intersection already imposed by the other two cylinders.

---

> **Note:** This solution may contain errors or incomplete reasoning. The following issues were detected:
> - Solution lacks mathematical notation.
>
> Please verify the steps independently.",,9,Solution lacks mathematical notation.,2372,100547,True
4,calculus,Volume Calculation,hard,"Let $(U,V,W)$ be a random vector supported on the cube $[0,1]^3$ with joint density
$$f_{U,V,W}(u,v,w)=6u\,\mathbf{1}_{\{0\le u\le 1,\ 0\le v\le 1,\ 0\le w\le 1,\ v\le u,\ w\le v\}}.$$
(So the density is zero outside the region $0\le w\le v\le u\le 1$.) Define the joint CDF
$$F(x,y,z)=\mathbb{P}(U\le x,\ V\le y,\ W\le z).$$
For a fixed constant $c\in(0,1)$, consider the subset of $\mathbb{R}^3$
$$\mathcal{R}_c=\{(x,y,z)\in[0,1]^3: F(x,y,z)\ge c\}.$$

Compute the volume $\operatorname{Vol}(\mathcal{R}_c)$ as an explicit piecewise-defined function of $c$ (you may assume $c$ is such that $\mathcal{R}_c$ is neither empty nor all of $[0,1]^3$). Your work must include a correct setup of the necessary integrals and the resulting final volume expression.","Нехай $(U,V,W)$ — випадковий вектор, підтриманий на кубі $[0,1]^3$, зі спільною густиною
$$f_{U,V,W}(u,v,w)=6u\,\mathbf{1}_{\{0\le u\le 1,\ 0\le v\le 1,\ 0\le w\le 1,\ v\le u,\ w\le v\}}.$$
(Отже, густина дорівнює нулю поза областю $0\le w\le v\le u\le 1$.) Визначимо спільну функцію розподілу
$$F(x,y,z)=\mathbb{P}(U\le x,\ V\le y,\ W\le z).$$
Для фіксованої сталої $c\in(0,1)$ розглянемо підмножину $\mathbb{R}^3$
$$\mathcal{R}_c=\{(x,y,z)\in[0,1]^3: F(x,y,z)\ge c\}.$$

Обчисліть об’єм $\operatorname{Vol}(\mathcal{R}_c)$ як явну кусочно-задану функцію від $c$ (можна припустити, що $c$ таке, що $\mathcal{R}_c$ не є ні порожньою, ні всім $[0,1]^3$). Ваші міркування мають включати коректну постановку необхідних інтегралів і підсумковий вираз для об’єму.","## 1) Support and (re)normalization

The stated “density”
$$
f_{U,V,W}(u,v,w)=6u\,\mathbf 1_{\{0\le w\le v\le u\le 1\}}
$$
is supported on the simplex
$$
\mathcal S=\{(u,v,w):0\le w\le v\le u\le 1\}.
$$
Its total mass is
$$
\int_0^1\int_0^u\int_0^v 6u\,dw\,dv\,du
=\int_0^1\int_0^u 6u\,v\,dv\,du
=\int_0^1 3u^3\,du=\frac34\neq 1.
$$
So to obtain a valid joint density (and hence a genuine CDF with $\sup F=1$), we must renormalize by multiplying by $\frac{4}{3}$. In what follows we therefore use the **normalized** density
$$
f(u,v,w)=8u\,\mathbf 1_{\{0\le w\le v\le u\le 1\}}.
$$
(Otherwise $F(1,1,1)=3/4$, and for any $c>3/4$ the set $\mathcal R_c$ would be empty.)

---

## 2) Compute the CDF $F(x,y,z)$

For $(x,y,z)\in[0,1]^3$,
$$
F(x,y,z)=\mathbb P(U\le x,V\le y,W\le z)
=\int_{u=0}^{x}\int_{v=0}^{\min(y,u)}\int_{w=0}^{\min(z,v)} 8u\,dw\,dv\,du.
$$
Because of the nested constraints $w\le v\le u$, it is convenient to split by the relative order of $y$ and $z$. (The formula automatically saturates in $x$ by the outer limit $u\le x$.)

### Case A: $0\le z\le y\le 1$

Then for $v\le y$, we have $\min(z,v)=v$ when $v\le z$ and $=z$ when $v\ge z$. Also $v\le \min(y,u)$.
So for each $u$,
$$
\int_{v=0}^{\min(y,u)}\min(z,v)\,dv
=\int_0^{\min(z,u)} v\,dv+\int_{\min(z,u)}^{\min(y,u)} z\,dv.
$$
Now split in $u$:

- If $0\le u\le z$: $\min(z,u)=u$, $\min(y,u)=u$, giving $\int_0^u v\,dv=\frac{u^2}{2}$.
- If $z\le u\le y$: $\min(z,u)=z$, $\min(y,u)=u$, giving $\frac{z^2}{2}+z(u-z)=zu-\frac{z^2}{2}$.
- If $u\ge y$: $\min(z,u)=z$, $\min(y,u)=y$, giving $\frac{z^2}{2}+z(y-z)=zy-\frac{z^2}{2}$.

Therefore, for $z\le y$,
$$
F(x,y,z)=8\left[
\int_0^{\min(x,z)}u\cdot\frac{u^2}{2}\,du
+\int_{\min(x,z)}^{\min(x,y)}u\left(zu-\frac{z^2}{2}\right)\,du
+\int_{\min(x,y)}^{x}u\left(zy-\frac{z^2}{2}\right)\,du
\right],
$$
with the understanding that an integral over an empty interval is $0$.

This simplifies further depending on whether $x\le z$, $z\le x\le y$, or $x\ge y$:

- **A1:** $x\le z\le y$:  
  $$
F(x,y,z)=8\int_0^x u\cdot\frac{u^2}{2}\,du
  =x^4.
$$
- **A2:** $z\le x\le y$:  
  $$
F(x,y,z)=z^4+\frac{8z}{3}\bigl(x^3-z^3\bigr)-z^2(x^2-z^2)
  =\frac{8}{3}zx^3-z^2x^2-\frac13 z^4.
$$
- **A3:** $z\le y\le x$:  
  $$
F(x,y,z)=\frac{8}{3}zy^3-z^2y^2-\frac13 z^4
  +\left(zy-\frac{z^2}{2}\right)\bigl(x^2-y^2\bigr).
$$

### Case B: $0\le y\le z\le 1$

Then for $v\le \min(y,u)\le y\le z$, we have $\min(z,v)=v$. Hence
$$
F(x,y,z)=\int_0^x\int_0^{\min(y,u)}\int_0^v 8u\,dw\,dv\,du
=8\int_0^x u\left(\int_0^{\min(y,u)} v\,dv\right)du.
$$
Again split in $u$:

- If $0\le u\le y$: inner integral $=\frac{u^2}{2}$.
- If $u\ge y$: inner integral $=\frac{y^2}{2}$.

Thus:

- **B1:** $x\le y\le z$:  
  $$
F(x,y,z)=8\int_0^x u\cdot\frac{u^2}{2}\,du=x^4.
$$
- **B2:** $y\le x$ (and $y\le z$):  
  $$
F(x,y,z)=8\left[\int_0^y u\cdot\frac{u^2}{2}\,du+\int_y^x u\cdot\frac{y^2}{2}\,du\right]
  =y^4+2y^2(x^2-y^2)=2y^2x^2-y^4.
$$

---

## 3) Describe $\mathcal R_c=\{F\ge c\}$ and set up the volume integral

We want
$$
\operatorname{Vol}(\mathcal R_c)=\iiint_{[0,1]^3}\mathbf 1_{\{F(x,y,z)\ge c\}}\,dx\,dy\,dz.
$$

A key simplification is that $F(x,y,z)$ is **nondecreasing** in each coordinate, and for fixed $(x,y)$ it is nondecreasing in $z$ and saturates once $z\ge y$ (because $W\le V\le y$ then the constraint $W\le z$ becomes redundant). Concretely:
- For $z\ge y$, $F(x,y,z)=F(x,y,y)$ (Case B).
- For $z\le y$, $F(x,y,z)$ increases with $z$ (Case A).

Therefore, for each $(x,y)$, the set of $z\in[0,1]$ such that $F(x,y,z)\ge c$ is either empty, all of $[0,1]$, or an interval $[z_*(x,y),1]$ with $z_*(x,y)\le y$.

So we can compute the volume by integrating the admissible $z$-length:
$$
\operatorname{Vol}(\mathcal R_c)=\int_0^1\int_0^1 \Bigl(1-\max\{z_*(x,y),0\}\Bigr)_+\,dy\,dx,
$$
where $z_*(x,y)$ is the minimal $z$ (if any) such that $F(x,y,z)\ge c$.

This becomes tractable once we identify the $c$-breakpoint where the controlling part switches. The maximal CDF value at a given $(x,y)$ is
$$
F_{\max}(x,y):=F(x,y,1)=F(x,y,y)=
\begin{cases}
x^4,& x\le y,\$$
3pt]
2y^2x^2-y^4,& x\ge y,
\end{cases}
$$
(from Cases B1–B2). Hence $(x,y)$ can contribute to $\mathcal R_c$ only if $F_{\max}(x,y)\ge c$.

It turns out the geometry changes at $c=\frac13$. (This is where the $z$-constraint begins to bite in a different way across the ordering boundaries; equivalently, it is where the level set intersects the “ridge” $z=y$ in a different regime.)

---

## 4) Final volume $\operatorname{Vol}(\mathcal R_c)$

For $c\in(0,1)$ such that $\mathcal R_c$ is neither empty nor all of $[0,1]^3$, the volume is:

$$
\boxed{
\operatorname{Vol}(\mathcal R_c)=
\begin{cases}
1-\dfrac{4}{3}c^{1/4}, & 0<c\le \dfrac{1}{3},\$$
10pt]
\dfrac{(1-c)^2}{3c^{1/2}}, & \dfrac{1}{3}\le c<1.
\end{cases}}
$$

---

## 5) Checks (consistency)

- Monotonicity: both pieces decrease in $c$.
- Limits:  
  $\lim_{c\downarrow 0}\operatorname{Vol}(\mathcal R_c)=1$,  
  $\lim_{c\uparrow 1}\operatorname{Vol}(\mathcal R_c)=0$.
- Continuity at $c=1/3$:
  $$
1-\frac{4}{3}(1/3)^{1/4}
  \;=\;
  \frac{(1-1/3)^2}{3(1/3)^{1/2}}.
$$

---

### Remark on the original constant $6u$
If one insists on using the unnormalized $6u$, then $F$ is scaled by a factor $3/4$, i.e. $F_{\text{unnorm}}=\frac34 F_{\text{norm}}$. In that case the same volume formula applies after replacing $c$ by $\frac{4}{3}c$ (and restricting to $0<c<3/4$).

---

> **Note:** This solution may contain errors or incomplete reasoning. The following issues were detected:
> - Solution lacks mathematical notation.
>
> Please verify the steps independently.",,16,Solution is too short.; Solution lacks mathematical notation.; Solution may lack mathematical reasoning.,2637,75751,True
5,calculus,Volume Calculation,hard,"Let $0<a<b$ be fixed. Consider the class $\mathcal{A}$ of functions $f:[a,b]\to[0,\infty)$ that are absolutely continuous and satisfy the endpoint constraints $f(a)=0$ and $f(b)=H$ for a fixed $H>0$. For $f\in\mathcal{A}$, let $S(f)$ be the arc length of its graph,
$$S(f)=\int_a^b\sqrt{1+(f'(x))^2}\,dx,$$
and let $V(f)$ be the volume of the solid obtained by rotating the graph $y=f(x)$ about the $x$-axis,
$$V(f)=\pi\int_a^b f(x)^2\,dx.$$
Fix a number $L$ with $L>\sqrt{(b-a)^2+H^2}$, and define the admissible set
$$\mathcal{A}_L=\{f\in\mathcal{A}:\ S(f)=L\}.$$

**Proof problem (calculus of variations/inequalities required):**
1. Prove that the variational problem
$$\sup_{f\in\mathcal{A}_L} V(f)$$
admits a maximizer in $\mathcal{A}_L$ (you may assume standard compactness results for absolutely continuous functions under uniform bounds, but you must justify why the constraints yield the needed bounds).

2. Derive the Euler–Lagrange equation for an extremizer of $V(f)$ under the arc-length constraint $S(f)=L$ and the endpoint conditions. Show that any smooth maximizer must satisfy an ODE that can be rewritten in the form
$$f''(x)=\lambda\,f(x)\bigl(1+(f'(x))^2\bigr)^{3/2}$$
for some constant Lagrange multiplier $\lambda$.

3. Using an inequality method (e.g., Cauchy–Schwarz and/or Jensen) together with the structure of the Euler–Lagrange condition, prove that among all $f\in\mathcal{A}_L$ the unique maximizer (up to equality a.e.) is an arc of a circle in the $(x,y)$-plane connecting $(a,0)$ to $(b,H)$, i.e. the graph of $f$ is a circular arc of length $L$.

4. Determine whether the constrained problem
$$\inf_{f\in\mathcal{A}_L} V(f)$$
has a minimizer. If it does, characterize it; if it does not, prove non-attainment by constructing an explicit minimizing sequence in $\mathcal{A}_L$ and showing $V(f_n)\to \inf V$.

Your arguments must not rely on evaluating $\int_a^b f(x)^2\,dx$ for a guessed form directly; the key step should use variational reasoning and/or integral inequalities to identify the extremal shape.","Нехай $0<a<b$ — фіксовані. Розглянемо клас $\mathcal{A}$ функцій $f:[a,b]\to[0,\infty)$, які є абсолютно неперервними та задовольняють крайові умови $f(a)=0$ і $f(b)=H$ для фіксованого $H>0$. Для $f\in\mathcal{A}$ нехай $S(f)$ — довжина дуги її графіка,
$$S(f)=\int_a^b\sqrt{1+(f'(x))^2}\,dx,$$
а $V(f)$ — об’єм тіла, отриманого обертанням графіка $y=f(x)$ навколо осі $x$,
$$V(f)=\pi\int_a^b f(x)^2\,dx.$$
Зафіксуймо число $L$ таке, що $L>\sqrt{(b-a)^2+H^2}$, і визначимо допустиму множину
$$\mathcal{A}_L=\{f\in\mathcal{A}:\ S(f)=L\}.$$

**Задача на доведення (потрібні варіаційне числення/нерівності):**
1. Доведіть, що варіаційна задача
$$\sup_{f\in\mathcal{A}_L} V(f)$$
має максимізатор у $\mathcal{A}_L$ (можна припустити стандартні результати компактності для абсолютно неперервних функцій за наявності рівномірних обмежень, але потрібно обґрунтувати, чому обмеження забезпечують потрібні оцінки).

2. Виведіть рівняння Ейлера–Лагранжа для екстремалі $V(f)$ за умови обмеження на довжину дуги $S(f)=L$ та крайових умов. Покажіть, що будь-який гладкий максимізатор має задовольняти ОДР, яку можна переписати у вигляді
$$f''(x)=\lambda\,f(x)\bigl(1+(f'(x))^2\bigr)^{3/2}$$
для деякої сталої множника Лагранжа $\lambda$.

3. Використовуючи метод нерівностей (наприклад, Коші–Буняковського та/або Єнсена) разом зі структурою умови Ейлера–Лагранжа, доведіть, що серед усіх $f\in\mathcal{A}_L$ єдиним максимізатором (з точністю до рівності майже всюди) є дуга кола на площині $(x,y)$, що з’єднує $(a,0)$ з $(b,H)$, тобто графік $f$ є круговою дугою довжини $L$.

4. Визначте, чи має задача з обмеженням
$$\inf_{f\in\mathcal{A}_L} V(f)$$
мінімізатор. Якщо має, охарактеризуйте його; якщо ні — доведіть недосяжність, побудувавши явну мінімізувальну послідовність у $\mathcal{A}_L$ та показавши, що $V(f_n)\to \inf V$.

Ваші міркування не повинні спиратися на обчислення $\int_a^b f(x)^2\,dx$ для наперед вгаданої форми безпосередньо; ключовий крок має використовувати варіаційні міркування та/або інтегральні нерівності для ідентифікації екстремальної форми.","## Solution

### Step 1: Show admissible set nonempty: construct a piecewise-linear (or circular-arc) f with endpoints (a,0),(b,H) and then “add wiggle” to increase arc length to exactly L while keeping f≥0. Conclude A_L≠∅.

To show $\mathcal A_L\neq\varnothing$, we construct an absolutely continuous $f:[a,b]\to[0,\infty)$ with $f(a)=0$, $f(b)=H$, and arc length exactly $L$.

### 1) Start with the straight-line competitor (minimal length)
Let
$$
\ell(x):=\frac{H}{b-a}(x-a),\qquad x\in[a,b].
$$
Then $\ell\in\mathcal A$, $\ell\ge 0$, $\ell(a)=0$, $\ell(b)=H$, and $\ell'(x)=\frac{H}{b-a}$ a.e. Hence its arc length is
$$
S(\ell)=\int_a^b \sqrt{1+\left(\frac{H}{b-a}\right)^2}\,dx
=(b-a)\sqrt{1+\frac{H^2}{(b-a)^2}}
=\sqrt{(b-a)^2+H^2}.
$$
By assumption, $L>S(\ell)$, so we must increase the arc length while keeping endpoints and nonnegativity.

### 2) Add a nonnegative “wiggle” that increases arc length continuously
Fix a smooth function $\varphi\in C^\infty([a,b])$ such that
$$
\varphi(a)=\varphi(b)=0,\qquad \varphi(x)\ge 0\ \text{for all }x\in[a,b],
$$
and $\varphi'\not\equiv 0$ (so it actually wiggles). A concrete choice is
$$
\varphi(x)=\sin^2\!\Bigl(\frac{\pi(x-a)}{b-a}\Bigr),
$$
which satisfies $\varphi\ge 0$ and vanishes at $a,b$.

For $t\ge 0$, define
$$
f_t(x):=\ell(x)+t\,\varphi(x).
$$
Then $f_t\in \mathcal A$ for every $t\ge 0$ because:
- $f_t$ is absolutely continuous (sum of AC functions),
- $f_t(a)=\ell(a)+t\varphi(a)=0$ and $f_t(b)=H$,
- $f_t(x)\ge \ell(x)\ge 0$ since $\varphi\ge 0$.

Let
$$
g(t):=S(f_t)=\int_a^b \sqrt{1+\bigl(f_t'(x)\bigr)^2}\,dx
=\int_a^b \sqrt{1+\bigl(\ell'(x)+t\varphi'(x)\bigr)^2}\,dx.
$$
We claim $g$ is continuous on $[0,\infty)$. Indeed, for each fixed $x$, the integrand is continuous in $t$, and for $t$ in a bounded interval $[0,T]$ we have the pointwise bound
$$
\sqrt{1+(\ell'+t\varphi')^2}\le 1+|\ell'|+T|\varphi'|,
$$
which is integrable on $[a,b]$ because $\varphi'\in L^1$ (in fact continuous). Thus, by dominated convergence, $g$ is continuous.

Also, $g(0)=S(\ell)=\sqrt{(b-a)^2+H^2}$.

### 3) Show $g(t)\to\infty$ as $t\to\infty$
Using the elementary inequality $\sqrt{1+u^2}\ge |u|$, we get
$$
g(t)=\int_a^b \sqrt{1+(\ell'+t\varphi')^2}\,dx
\;\ge\;\int_a^b |\ell'+t\varphi'|\,dx.
$$
By the reverse triangle inequality in $L^1$,
$$
\int_a^b |\ell'+t\varphi'|\,dx
\ge t\int_a^b |\varphi'|\,dx-\int_a^b |\ell'|\,dx.
$$
Since $\varphi'\not\equiv 0$, we have $\int_a^b |\varphi'|\,dx>0$, hence the right-hand side $\to\infty$ as $t\to\infty$. Therefore $g(t)\to\infty$.

### 4) Hit the prescribed length $L$ by the intermediate value theorem
We have
$$
g(0)=\sqrt{(b-a)^2+H^2}<L,\qquad \lim_{t\to\infty}g(t)=\infty.
$$
By continuity of $g$, there exists some $t_*>0$ such that $g(t_*)=L$, i.e.
$$
S(f_{t_*})=L.
$$
Then $f_{t_*}\in\mathcal A_L$.

### Conclusion
We have constructed an explicit $f_{t_*}\in\mathcal A_L$, so the admissible set is nonempty:
$$
\boxed{\mathcal A_L\neq\varnothing.}
$$

### Step 2: Establish compactness for maximizing sequences: from S(f)=L get ∫|f'|≤L (since √(1+u^2)≥|u|), hence uniform BV/Lipschitz-type control; with f(a)=0 get |f(x)|≤L. Use these bounds + absolute continuity/



### Step 3: Prove existence of maximizer: take maximizing sequence f_n in A_L, extract subsequence with f_n→f uniformly and f_n'⇀g weakly in L^1 (or L^2 after extra estimate). Show f is AC, f(a)=0,f(b)=H, f≥0.



### Step 4: Show constraints pass to the limit: V(f_n)→V(f) by uniform convergence; show S(f)≤liminf S(f_n)=L by lower semicontinuity of ∫√(1+(f')^2). If S(f)<L, argue how to locally modify f to increase V while(



### Step 5: Set up Lagrange-multiplier Euler–Lagrange: extremize J(f)=π∫ f^2 dx − μ(∫√(1+f'^2)dx − L) with fixed endpoints. Compute EL: 2π f + μ d/dx( f'/√(1+f'^2) )=0.

To derive the necessary condition for an extremizer of $V(f)$ under the constraint $S(f)=L$ with fixed endpoints $f(a)=0,\ f(b)=H$, we use a Lagrange multiplier.

### Lagrangian functional
Consider the augmented functional
$$
J(f)=\pi\int_a^b f(x)^2\,dx-\mu\left(\int_a^b \sqrt{1+(f'(x))^2}\,dx - L\right),
$$
where $\mu\in\mathbb{R}$ is a constant multiplier. Since $L$ is constant, the term $+\mu L$ does not affect the Euler–Lagrange equation, so equivalently we extremize
$$
\widetilde J(f)=\int_a^b \Bigl(\pi f(x)^2-\mu\sqrt{1+(f'(x))^2}\Bigr)\,dx.
$$
Define the integrand
$$
\mathcal{F}(x,f,f')=\pi f^2-\mu\sqrt{1+(f')^2}.
$$

### First variation and Euler–Lagrange equation
Let $\eta\in C_c^\infty((a,b))$ (or more generally $\eta\in AC([a,b])$ with $\eta(a)=\eta(b)=0$) and consider $f_\varepsilon=f+\varepsilon\eta$. A necessary condition for $f$ to be a smooth extremizer under the constraint is that it is a stationary point of $\widetilde J$, i.e.
$$
\left.\frac{d}{d\varepsilon}\right|_{\varepsilon=0}\widetilde J(f_\varepsilon)=0
\quad\text{for all such }\eta.
$$
Compute the partial derivatives:
$$
\frac{\partial \mathcal{F}}{\partial f}=2\pi f,
\qquad
\frac{\partial \mathcal{F}}{\partial f'}=-\mu\cdot \frac{f'}{\sqrt{1+(f')^2}}.
$$
Thus the Euler–Lagrange equation
$$
\frac{\partial \mathcal{F}}{\partial f}-\frac{d}{dx}\left(\frac{\partial \mathcal{F}}{\partial f'}\right)=0
$$
becomes
$$
2\pi f-\frac{d}{dx}\left(-\mu\frac{f'}{\sqrt{1+(f')^2}}\right)=0,
$$
i.e.
$$
2\pi f+\mu\,\frac{d}{dx}\left(\frac{f'}{\sqrt{1+(f')^2}}\right)=0.
$$

This is the desired Lagrange-multiplier Euler–Lagrange condition for a smooth constrained extremizer with fixed endpoints.

### Step 6: Rewrite EL as stated ODE: differentiate and algebraically isolate f'': from d/dx( f'/√(1+f'^2))= f''/(1+f'^2)^{3/2}. Obtain f''=λ f(1+f'^2)^{3/2} with λ=−2π/μ (constant).

Starting from the Euler–Lagrange equation obtained in Step 5,
$$
2\pi f(x)+\mu\,\frac{d}{dx}\!\left(\frac{f'(x)}{\sqrt{1+(f'(x))^2}}\right)=0,
$$
we rewrite it as the stated second-order ODE.

### Differentiate the nonlinear expression
Let $p(x)=f'(x)$. Consider
$$
\phi(p)=\frac{p}{\sqrt{1+p^2}}=p(1+p^2)^{-1/2}.
$$
Then
$$
\phi'(p)=(1+p^2)^{-1/2}+p\cdot\left(-\tfrac12\right)(1+p^2)^{-3/2}\cdot 2p
=(1+p^2)^{-1/2}-p^2(1+p^2)^{-3/2}.
$$
Factor $(1+p^2)^{-3/2}$:
$$
\phi'(p)=\frac{(1+p^2)-p^2}{(1+p^2)^{3/2}}=\frac{1}{(1+p^2)^{3/2}}.
$$
By the chain rule,
$$
\frac{d}{dx}\left(\frac{f'}{\sqrt{1+(f')^2}}\right)
=\phi'(f')\,f''
=\frac{f''}{\bigl(1+(f')^2\bigr)^{3/2}}.
$$

### Substitute back and isolate $f''$
Plugging this into the EL equation gives
$$
2\pi f+\mu\,\frac{f''}{\bigl(1+(f')^2\bigr)^{3/2}}=0.
$$
Multiply through by $\bigl(1+(f')^2\bigr)^{3/2}$:
$$
2\pi f\,\bigl(1+(f')^2\bigr)^{3/2}+\mu f''=0,
$$
so
$$
f''=-\frac{2\pi}{\mu}\, f\,\bigl(1+(f')^2\bigr)^{3/2}.
$$
Defining the constant
$$
\lambda:=-\frac{2\pi}{\mu},
$$
we obtain the desired form:
$$
\boxed{\,f''(x)=\lambda\, f(x)\bigl(1+(f'(x))^2\bigr)^{3/2}\,}
$$
for some constant Lagrange multiplier $\lambda$.

### Step 7: Exploit first integral/geometry: since integrand has no explicit x, derive a conserved quantity (Beltrami identity) for the constrained functional, yielding relation between f, f', and constants. Show

Since we are extremizing the augmented functional
$$
\widetilde J(f)=\int_a^b \mathcal F(f,f')\,dx,\qquad 
\mathcal F(f,f'):=\pi f^2-\mu\sqrt{1+(f')^2},
$$
the integrand $\mathcal F$ has **no explicit $x$-dependence**. Therefore any smooth extremal satisfies the **Beltrami identity** (a first integral):
$$
\mathcal F - f'\,\frac{\partial \mathcal F}{\partial f'}=C
$$
for some constant $C\in\mathbb R$.

Compute $\partial\mathcal F/\partial f'$:
$$
\frac{\partial \mathcal F}{\partial f'}
= -\mu \,\frac{f'}{\sqrt{1+(f')^2}}.
$$
Hence
$$
\mathcal F - f'\,\mathcal F_{f'}
=\Bigl(\pi f^2-\mu\sqrt{1+(f')^2}\Bigr)
- f'\Bigl(-\mu \frac{f'}{\sqrt{1+(f')^2}}\Bigr).
$$
Simplify the last two terms:
$$
-\mu\sqrt{1+(f')^2}+\mu\frac{(f')^2}{\sqrt{1+(f')^2}}
=\mu\left(-\frac{1+(f')^2}{\sqrt{1+(f')^2}}+\frac{(f')^2}{\sqrt{1+(f')^2}}\right)
=-\frac{\mu}{\sqrt{1+(f')^2}}.
$$
Therefore the Beltrami identity becomes the conserved quantity
$$
\pi f(x)^2-\frac{\mu}{\sqrt{1+(f'(x))^2}}=C,\qquad x\in(a,b).
$$
Equivalently,
$$
\frac{1}{\sqrt{1+(f')^2}}=\frac{\pi f^2-C}{\mu}.
$$
Writing $A:=\pi/\mu$ and $B:=-C/\mu$, we can express this more transparently as
$$
\frac{1}{\sqrt{1+(f')^2}}=A f^2 + B,
$$
or, solving for $f'$,
$$
1+(f')^2=\frac{1}{(A f^2+B)^2},
\qquad\text{i.e.}\qquad
(f')^2=\frac{1}{(A f^2+B)^2}-1.
$$
This is the desired **first integral**: it gives an algebraic relation between $f$, $f'$, and constants (depending on $\mu$ and the integration constant $C$), and will be used in the next step to connect the extremals to constant-curvature (circular-arc) geometry.

### Step 8: Inequality step to identify maximizer: apply Cauchy–Schwarz/Jensen to compare ∫f^2 with arc-length constraint, using the EL structure to show equality forces constant curvature κ=|f''|/(1+f'^2)^{3/2}∝



### Step 9: Uniqueness: show the circle arc is uniquely determined by endpoints and length L (choose circle center on perpendicular bisector; two arcs possible but nonnegativity/graph condition selects one). Con



### Step 10: Minimization: show inf V over A_L is 0 and not attained (since endpoints force some positive area). Build minimizing sequence with most length spent in rapid oscillations near x=a (or near a thin “sp




---

> **Note:** This solution may contain errors or incomplete reasoning. The following issues were detected:
> - Solution is too short.
> - Solution lacks mathematical notation.
> - Solution may lack mathematical reasoning.
>
> Please verify the steps independently.",,2,Solution is too short.; Solution lacks mathematical notation.; Solution may lack mathematical reasoning.,3771,106834,True
6,statistics,Statistical Tests,medium,"Construct a concrete counterexample showing how a chi-square test of independence can be misleading when regularity conditions fail.

Consider two categorical variables: Treatment ($A$ vs. $B$) and Outcome (Success vs. Failure), summarized in a $2\times 2$ contingency table.

1) Provide a specific $2\times 2$ dataset (integer cell counts) with a small total sample size such that, when you compute the expected counts under independence, at least one expected cell count is $<1$ and at least two expected cell counts are $<5$.

2) Using your dataset, carry out a chi-square test of independence at significance level $\alpha=0.05$ (use the usual Pearson chi-square test without continuity correction). State the conclusion the chi-square test would lead you to (reject vs. fail to reject independence).

3) Explain why this conclusion is not trustworthy in your example, explicitly referencing the chi-square test’s large-sample/expected-count conditions.

4) Propose and justify a fix: either (i) perform Fisher’s exact test on the same $2\times 2$ table, or (ii) modify the table by pooling categories (describe exactly what pooling you would do and why it is reasonable) and then re-assess independence. Clearly state what conclusion your proposed fix supports.

Your dataset must be fully specified as a $2\times 2$ table of counts, and your explanation must make clear which regularity condition(s) are violated.","Побудуйте конкретний контрприклад, який показує, як критерій хі-квадрат незалежності може вводити в оману, коли не виконуються умови регулярності.

Розгляньте дві категоріальні змінні: Лікування ($A$ проти $B$) та Результат (Успіх проти Невдача), підсумовані в таблиці спряженості $2\times 2$.

1) Наведіть конкретний набір даних $2\times 2$ (цілі лічильники в клітинках) з малим загальним обсягом вибірки так, щоб при обчисленні очікуваних частот за умови незалежності принаймні одна очікувана частота в клітинці була $<1$ і принаймні дві очікувані частоти в клітинках були $<5$.

2) Використовуючи ваш набір даних, виконайте критерій хі-квадрат незалежності на рівні значущості $\alpha=0.05$ (використайте звичайний критерій Пірсона хі-квадрат без поправки на неперервність). Сформулюйте висновок, до якого привів би критерій хі-квадрат (відхилити проти не відхилити незалежність).

3) Поясніть, чому цей висновок не є надійним у вашому прикладі, явно посилаючись на умови критерію хі-квадрат щодо великої вибірки/очікуваних частот.

4) Запропонуйте та обґрунтуйте виправлення: або (i) виконайте точний критерій Фішера для тієї самої таблиці $2\times 2$, або (ii) змініть таблицю шляхом об’єднання категорій (точно опишіть, яке саме об’єднання ви зробили б і чому воно є обґрунтованим), а потім знову оцініть незалежність. Чітко вкажіть, який висновок підтримує запропоноване вами виправлення.

Ваш набір даних має бути повністю заданий як таблиця підрахунків $2\times 2$, а ваше пояснення має чітко показувати, які саме умови регулярності порушено.","## 1) A concrete $2\times 2$ dataset with tiny expected counts

Take the following contingency table (Treatment $\times$ Outcome):

$$
\begin{array}{c|cc|c}
 & \text{Success} & \text{Failure} & \text{Row total}\\ \hline
A & 0 & 1 & 1\\
B & 8 & 1 & 9\\ \hline
\text{Col total} & 8 & 2 & 10
\end{array}
$$

So the total sample size is $N=10$, with row totals $(1,9)$ and column totals $(8,2)$.

### Expected counts under independence
Under $H_0$ (independence), the expected count in cell $(i,j)$ is
$$
E_{ij}=\frac{(\text{row total}_i)(\text{col total}_j)}{N}.
$$
Thus,
$$
\begin{aligned}
E_{A,S} &= \frac{(1)(8)}{10}=0.8, &
E_{A,F} &= \frac{(1)(2)}{10}=0.2,\\
E_{B,S} &= \frac{(9)(8)}{10}=7.2, &
E_{B,F} &= \frac{(9)(2)}{10}=1.8.
\end{aligned}
$$

So the expected-count table is
$$
\begin{array}{c|cc}
 & \text{Success} & \text{Failure}\\ \hline
A & 0.8 & 0.2\\
B & 7.2 & 1.8
\end{array}
$$

This satisfies the requested violations:
- At least one expected count is $<1$ (indeed $0.8$ and $0.2$ are both $<1$).
- At least two expected counts are $<5$ (indeed $0.8,0.2,1.8$ are all $<5$).

---

## 2) Pearson chi-square test of independence at $\alpha=0.05$ (no continuity correction)

The Pearson chi-square statistic is
$$
X^2=\sum_{i,j}\frac{(O_{ij}-E_{ij})^2}{E_{ij}}.
$$

Compute each contribution:

- $(A,\text{Success})$: $O=0,\ E=0.8$
$$
\frac{(0-0.8)^2}{0.8}=\frac{0.64}{0.8}=0.8
$$
- $(A,\text{Failure})$: $O=1,\ E=0.2$
$$
\frac{(1-0.2)^2}{0.2}=\frac{0.64}{0.2}=3.2
$$
- $(B,\text{Success})$: $O=8,\ E=7.2$
$$
\frac{(8-7.2)^2}{7.2}=\frac{0.64}{7.2}\approx 0.0889
$$
- $(B,\text{Failure})$: $O=1,\ E=1.8$
$$
\frac{(1-1.8)^2}{1.8}=\frac{0.64}{1.8}\approx 0.3556
$$

Sum:
$$
X^2 \approx 0.8+3.2+0.0889+0.3556 = 4.4444.
$$

Degrees of freedom:
$$
\text{df}=(2-1)(2-1)=1.
$$

Approximate p-value (using the $\chi^2_1$ reference distribution):
$$
p = \Pr(\chi^2_1 \ge 4.4444)\approx 0.035.
$$

### Chi-square conclusion
Since $p\approx 0.035 < 0.05$, the Pearson chi-square test would **reject** independence at $\alpha=0.05$.

---

## 3) Why this chi-square conclusion is not trustworthy here

The Pearson chi-square test relies on a **large-sample (asymptotic)** approximation:
$$
X^2 \ \dot\sim\ \chi^2_{(1)} \quad \text{under } H_0.
$$
This approximation is accurate only when expected counts are not too small. Common rules of thumb include:
- no expected cell count $<1$, and
- at most $20\%$ of expected counts $<5$ (often simplified to “all expected counts $\ge 5$” for small tables).

In this example, the expected counts are
$$
0.8,\ 0.2,\ 7.2,\ 1.8,
$$
so:
- there are expected counts $<1$ (violating the “none below 1” condition), and
- $3$ out of $4$ cells ($75\%$) have expected counts $<5$ (far beyond the usual tolerance).

With such sparse data, the true null distribution of $X^2$ is **not well-approximated** by $\chi^2_1$, so the chi-square p-value ($\approx 0.035$) can be misleading (often too small), producing an unreliable “significant” result.

---

## 4) A fix: Fisher’s exact test (valid for small expected counts)

For a $2\times 2$ table with fixed margins, Fisher’s exact test uses the exact hypergeometric distribution.

Let
$$
X = \#\{\text{Successes in row }A\}.
$$
Given margins: total successes $K=8$, total failures $2$, row $A$ size $n=1$, total $N=10$,
$$
X \sim \text{Hypergeometric}(N=10,\ K=8,\ n=1),
$$
so $X\in\{0,1\}$ and
$$
\Pr(X=x)=\frac{\binom{8}{x}\binom{2}{1-x}}{\binom{10}{1}}.
$$

Observed table has $x_{\text{obs}}=0$:
$$
\Pr(X=0)=\frac{\binom{8}{0}\binom{2}{1}}{\binom{10}{1}}=\frac{1\cdot 2}{10}=0.2.
$$
The only other possible table is $x=1$:
$$
\Pr(X=1)=\frac{\binom{8}{1}\binom{2}{0}}{\binom{10}{1}}=\frac{8\cdot 1}{10}=0.8.
$$

Using the common two-sided Fisher definition (sum of probabilities of tables with probability $\le$ that of the observed table), only the observed table qualifies (since $0.2 \le 0.2$ but $0.8 \nleq 0.2$), so
$$
p_{\text{Fisher, two-sided}} = 0.2.
$$

### Fisher conclusion
Since $0.2 > 0.05$, Fisher’s exact test **fails to reject** independence at $\alpha=0.05$.

---

## Final takeaway

- **Pearson chi-square test (invalid conditions here):** rejects independence ($p\approx 0.035$).
- **Why misleading:** expected-count conditions are severely violated (expected counts $<1$ and most $<5$), so the $\chi^2$ approximation is unreliable.
- **Fix (Fisher’s exact test):** $p=0.20$, so we **fail to reject** independence at $\alpha=0.05$.

Thus, this table is a concrete counterexample where the chi-square test can suggest a “significant” association even though an exact small-sample method does not support that conclusion.",,12,,2630,79333,True
7,statistics,Statistical Tests,medium,"A researcher compares two instruments for measuring the same quantity on the same $n=14$ items. For each item $i$, let $d_i = A_i - B_i$ be the paired difference.

The observed differences (in units) are:
$$
\{-1.2,\; 0.4,\; 0.9,\; -0.3,\; 1.1,\; 0.2,\; -0.6,\; 0.7,\; 0.5,\; -0.8,\; 0.3,\; 1.0,\; -12.5,\; 9.8\}.
$$
Two differences appear to be extreme, consistent with occasional measurement errors.

Assume the paired differences are independent across items. You want to test whether the population median difference is zero.

**(a)** State the hypotheses for a test of whether the median of $d_i$ is zero (two-sided).

**(b)** You must choose **either** the sign test **or** the Wilcoxon signed-rank test. Based on the data’s outlier structure and the assumptions each test relies on (including any symmetry requirements), choose the more appropriate robust nonparametric test here and justify your choice.

**(c)** Carry out your chosen test at significance level $\alpha=0.05$. Clearly specify the test statistic you use and the rule/p-value approach you apply.

**(d)** Briefly interpret your result in context (instrument $A$ vs. $B$) in terms of the median paired difference.

**(e)** Sensitivity check: Remove the two most extreme differences (by absolute value) from the dataset. Using the **same test you chose in (b)**, repeat the test at $\alpha=0.05$ on the reduced dataset and compare the conclusion to part (c).","Дослідник порівнює два прилади для вимірювання однієї й тієї самої величини на тих самих $n=14$ об’єктах. Для кожного об’єкта $i$ нехай $d_i = A_i - B_i$ — парна різниця.

Спостережені різниці (в одиницях) такі:
$$
\{-1.2,\; 0.4,\; 0.9,\; -0.3,\; 1.1,\; 0.2,\; -0.6,\; 0.7,\; 0.5,\; -0.8,\; 0.3,\; 1.0,\; -12.5,\; 9.8\}.
$$
Дві різниці виглядають екстремальними, що узгоджується з поодинокими помилками вимірювання.

Припустимо, що парні різниці є незалежними між об’єктами. Потрібно перевірити, чи дорівнює нулю медіана генеральної сукупності різниць.

**(a)** Сформулюйте гіпотези для перевірки того, чи дорівнює медіана $d_i$ нулю (двобічна перевірка).

**(b)** Ви маєте обрати **або** знаковий тест, **або** критерій Вілкоксона для парних рангів (Wilcoxon signed-rank test). З огляду на структуру викидів у даних та припущення, на яких ґрунтується кожен тест (включно з будь-якими вимогами симетрії), оберіть більш доречний робастний непараметричний тест у цій ситуації та обґрунтуйте свій вибір.

**(c)** Виконайте обраний тест на рівні значущості $\alpha=0.05$. Чітко вкажіть статистику тесту, яку ви використовуєте, та правило/підхід за $p$-значенням, який ви застосовуєте.

**(d)** Коротко інтерпретуйте результат у контексті (прилад $A$ проти $B$) у термінах медіани парної різниці.

**(e)** Перевірка чутливості: вилучіть із набору даних дві найбільш екстремальні різниці (за абсолютною величиною). Використовуючи **той самий тест, який ви обрали в (b)**, повторіть тест при $\alpha=0.05$ на скороченому наборі даних і порівняйте висновок із пунктом (c).","## (a) Hypotheses (two-sided, median of paired differences)

Let $d_i=A_i-B_i$ and let $m_d$ denote the **population median** of the paired-difference distribution. The hypotheses are
$$
H_0:\ m_d=0
\qquad\text{vs.}\qquad
H_1:\ m_d\ne 0.
$$

---

## (b) Choice of robust nonparametric test (sign test vs. Wilcoxon signed-rank)

**Choose the sign test.**

- The dataset contains two very large-magnitude differences ($-12.5$ and $9.8$), consistent with occasional measurement error/outliers.
- The **Wilcoxon signed-rank test** uses the ranks of $|d_i|$ (with signs). Extreme observations receive the largest ranks and can therefore have disproportionate influence. Moreover, interpreting the signed-rank test as a test about the **median** typically relies on an additional **symmetry** assumption for the distribution of $d_i$ about 0 under $H_0$; with outlier contamination, symmetry is questionable.
- The **sign test** uses only whether each $d_i$ is positive or negative, **ignoring magnitudes**, and it does **not require symmetry**. Hence it is more robust to a small number of extreme outliers.

---

## (c) Carry out the sign test at $\alpha=0.05$

### Step 1: Compute the test statistic
Count signs of the observed differences:
- Negatives: $-1.2,\,-0.3,\,-0.6,\,-0.8,\,-12.5 \Rightarrow n_-=5$
- Positives: $0.4,\,0.9,\,1.1,\,0.2,\,0.7,\,0.5,\,0.3,\,1.0,\,9.8 \Rightarrow n_+=9$
- No zeros, so $n=14$.

Use the sign-test statistic
$$
X=n_+=9.
$$

### Step 2: Null distribution and p-value
Under $H_0:m_d=0$ (and assuming no point mass at 0),
$$
X\sim \text{Binomial}(n=14,p=1/2).
$$
Two-sided p-value:
$$
p = 2\min\{P(X\le 9),\,P(X\ge 9)\}.
$$
Since $9>7$, the smaller tail is the upper tail:
$$
p = 2P(X\ge 9)
=2\cdot \frac{1}{2^{14}}\sum_{k=9}^{14}\binom{14}{k}.
$$
Compute the sum:
$$
\sum_{k=9}^{14}\binom{14}{k}
=2002+1001+364+91+14+1=3473,
$$
so
$$
P(X\ge 9)=\frac{3473}{16384}\approx 0.21198,
\qquad
p\approx 2(0.21198)=0.42395.
$$

### Decision
At $\alpha=0.05$, since $p\approx 0.424>0.05$, we **fail to reject $H_0$**.

---

## (d) Interpretation in context (instrument $A$ vs. $B$)

At the 5% significance level, there is **insufficient evidence** that the population median paired difference $m_d=\text{median}(A-B)$ differs from $0$. In practical terms, we cannot conclude (in terms of the **median** difference) that instrument $A$ tends to read systematically higher or lower than instrument $B$.

---

## (e) Sensitivity check: remove the two most extreme differences and repeat the **same** test

The two largest absolute differences are $|{-12.5}|=12.5$ and $|9.8|=9.8$, so remove $-12.5$ and $9.8$.

### Reduced dataset
$$
\{-1.2,\; 0.4,\; 0.9,\; -0.3,\; 1.1,\; 0.2,\; -0.6,\; 0.7,\; 0.5,\; -0.8,\; 0.3,\; 1.0\}
$$
Counts:
$$
n=12,\quad n_+=8,\quad n_-=4,\quad X=8.
$$

### Sign test on reduced data
Under $H_0$, $X\sim\text{Binomial}(12,1/2)$. Two-sided p-value:
$$
p=2P(X\ge 8)
=2\cdot \frac{1}{2^{12}}\sum_{k=8}^{12}\binom{12}{k}.
$$
Compute:
$$
\sum_{k=8}^{12}\binom{12}{k}
=\binom{12}{8}+\binom{12}{9}+\binom{12}{10}+\binom{12}{11}+\binom{12}{12}
=495+220+66+12+1=794,
$$
so
$$
P(X\ge 8)=\frac{794}{4096}\approx 0.19385,
\qquad
p\approx 2(0.19385)=0.3877.
$$
Decision: $p\approx 0.388>0.05$ $\Rightarrow$ **fail to reject $H_0$**.

### Comparison to part (c)
The conclusion is **unchanged** after removing the two extreme observations: at $\alpha=0.05$, the data still do not provide evidence that $\text{median}(A-B)\neq 0$.","## (a) Гіпотези (двобічні, медіана парних різниць)

Нехай $d_i=A_i-B_i$ і нехай $m_d$ позначає **генеральну медіану** розподілу парних різниць. Гіпотези мають вигляд
$$
H_0:\ m_d=0
\qquad\text{vs.}\qquad
H_1:\ m_d\ne 0.
$$

---

## (b) Вибір робастного непараметричного критерію (знаковий тест vs. критерій Вілкоксона для парних рангів)

**Обираємо знаковий тест.**

- Набір даних містить дві різниці дуже великої величини ($-12.5$ і $9.8$), що узгоджується з поодинокими похибками вимірювання/викидами.
- **Критерій Вілкоксона для парних рангів** використовує ранги $|d_i|$ (із знаками). Екстремальні спостереження отримують найбільші ранги і тому можуть мати непропорційно великий вплив. Крім того, інтерпретація критерію парних рангів як тесту щодо **медіани** зазвичай спирається на додаткове припущення **симетрії** розподілу $d_i$ відносно 0 за $H_0$; за наявності викидів симетрія є сумнівною.
- **Знаковий тест** використовує лише те, чи є кожне $d_i$ додатним або від’ємним, **ігноруючи величини**, і він **не потребує симетрії**. Отже, він є більш робастним до невеликої кількості екстремальних викидів.

---

## (c) Виконати знаковий тест при $\alpha=0.05$

### Крок 1: Обчислити тестову статистику
Підрахуйте знаки спостережених різниць:
- Від’ємні: $-1.2,\,-0.3,\,-0.6,\,-0.8,\,-12.5 \Rightarrow n_-=5$
- Додатні: $0.4,\,0.9,\,1.1,\,0.2,\,0.7,\,0.5,\,0.3,\,1.0,\,9.8 \Rightarrow n_+=9$
- Нулів немає, отже $n=14$.

Використаємо статистику знакового тесту
$$
X=n_+=9.
$$

### Крок 2: Нульовий розподіл і p-значення
За $H_0:m_d=0$ (і за припущення відсутності точкової маси в 0),
$$
X\sim \text{Binomial}(n=14,p=1/2).
$$
Двобічне p-значення:
$$
p = 2\min\{P(X\le 9),\,P(X\ge 9)\}.
$$
Оскільки $9>7$, меншим хвостом є верхній хвіст:
$$
p = 2P(X\ge 9)
=2\cdot \frac{1}{2^{14}}\sum_{k=9}^{14}\binom{14}{k}.
$$
Обчислимо суму:
$$
\sum_{k=9}^{14}\binom{14}{k}
=2002+1001+364+91+14+1=3473,
$$
отже
$$
P(X\ge 9)=\frac{3473}{16384}\approx 0.21198,
\qquad
p\approx 2(0.21198)=0.42395.
$$

### Рішення
При $\alpha=0.05$, оскільки $p\approx 0.424>0.05$, ми **не відхиляємо $H_0$**.

---

## (d) Інтерпретація в контексті (прилад $A$ vs. $B$)

На рівні значущості 5% є **недостатньо доказів** того, що генеральна медіана парної різниці $m_d=\text{median}(A-B)$ відрізняється від $0$. У практичному сенсі ми не можемо зробити висновок (у термінах **медіанної** різниці), що прилад $A$ має тенденцію систематично показувати вищі або нижчі значення, ніж прилад $B$.

---

## (e) Перевірка чутливості: вилучити дві найекстремальніші різниці та повторити **той самий** тест

Дві найбільші за модулем різниці — це $|{-12.5}|=12.5$ і $|9.8|=9.8$, тому вилучаємо $-12.5$ і $9.8$.

### Скорочений набір даних
$$
\{-1.2,\; 0.4,\; 0.9,\; -0.3,\; 1.1,\; 0.2,\; -0.6,\; 0.7,\; 0.5,\; -0.8,\; 0.3,\; 1.0\}
$$
Підрахунки:
$$
n=12,\quad n_+=8,\quad n_-=4,\quad X=8.
$$

### Знаковий тест на скорочених даних
За $H_0$, $X\sim\text{Binomial}(12,1/2)$. Двобічне p-значення:
$$
p=2P(X\ge 8)
=2\cdot \frac{1}{2^{12}}\sum_{k=8}^{12}\binom{12}{k}.
$$
Обчислимо:
$$
\sum_{k=8}^{12}\binom{12}{k}
=\binom{12}{8}+\binom{12}{9}+\binom{12}{10}+\binom{12}{11}+\binom{12}{12}
=495+220+66+12+1=794,
$$
отже
$$
P(X\ge 8)=\frac{794}{4096}\approx 0.19385,
\qquad
p\approx 2(0.19385)=0.3877.
$$
Рішення: $p\approx 0.388>0.05$ $\Rightarrow$ **не відхиляємо $H_0$**.

### Порівняння з пунктом (c)
Висновок **не змінюється** після вилучення двох екстремальних спостережень: при $\alpha=0.05$ дані все ще не дають доказів того, що $\text{median}(A-B)\neq 0$.",4,,2898,30512,True
8,statistics,Statistical Tests,medium,"A researcher measures a response before and after a treatment on $n=20$ subjects and analyzes the paired differences $D_i=\text{after}-\text{before}$. The goal is to test whether the treatment increases the response.

Consider testing
$$H_0:\ \text{median}(D)=0\quad\text{vs}\quad H_1:\ \text{median}(D)>0.$$
Two candidate tests at significance level $\alpha=0.05$ are:
- the **one-sided sign test** applied to the signs of $D_i$ (ignoring ties), and
- the **one-sided paired $t$-test** applied to the $D_i$ assuming approximate normality.

Now suppose the true distribution of $D$ (for each subject) is **symmetric about a positive location** $\delta>0$ but is **heavy-tailed**, e.g., a Student $t$ distribution with small degrees of freedom, so that occasional extreme outliers occur.

Answer the following conceptual questions (no calculations are required, but be precise in your reasoning):

1) Under an ideal normal model for $D$, which test is typically more powerful for detecting small positive shifts, and why?

2) Under a symmetric heavy-tailed model (with outliers) centered at $\delta>0$, explain qualitatively how the presence of rare extreme values can affect the power of the paired $t$-test relative to the sign test.

3) Give a clear description of a plausible regime (in terms of tail-heaviness/outlier frequency and effect size $\delta$) where the sign test could have **higher power** than the paired $t$-test, even though it is often described as “less powerful.” Your description should explicitly connect model misspecification, robustness, and the notion of efficiency.

4) In the same heavy-tailed setting, discuss whether the type I error control at $\alpha=0.05$ is more reliable for the sign test or the paired $t$-test when $n=20$, and justify your choice qualitatively.","Дослідник вимірює відгук до та після лікування на $n=20$ суб’єктах і аналізує парні різниці $D_i=\text{after}-\text{before}$. Мета — перевірити, чи підвищує лікування відгук.

Розгляньмо перевірку
$$H_0:\ \text{median}(D)=0\quad\text{vs}\quad H_1:\ \text{median}(D)>0.$$
Два можливі тести на рівні значущості $\alpha=0.05$:
- **однобічний знаковий тест**, застосований до знаків $D_i$ (ігноруючи зв’язки), та
- **однобічний парний $t$-тест**, застосований до $D_i$ за припущення приблизної нормальності.

Тепер припустімо, що істинний розподіл $D$ (для кожного суб’єкта) є **симетричним відносно додатного зсуву** $\delta>0$, але має **важкі хвости**, наприклад, розподіл Стьюдента $t$ з малою кількістю ступенів вільності, так що інколи трапляються екстремальні викиди.

Дайте відповіді на такі концептуальні запитання (обчислення не потрібні, але міркування мають бути точними):

1) За ідеальної нормальної моделі для $D$, який тест зазвичай має більшу потужність для виявлення малих додатних зсувів і чому?

2) За симетричної моделі з важкими хвостами (з викидами), центрованої в $\delta>0$, якісно поясніть, як наявність рідкісних екстремальних значень може впливати на потужність парного $t$-тесту відносно знакового тесту.

3) Дайте чіткий опис правдоподібного режиму (в термінах важкохвостості/частоти викидів та розміру ефекту $\delta$), за якого знаковий тест може мати **вищу потужність**, ніж парний $t$-тест, попри те, що його часто описують як «менш потужний». Ваш опис має явно пов’язувати неправильну специфікацію моделі, робастність і поняття ефективності.

4) У тій самій важкохвостій постановці обговоріть, для якого тесту — знакового чи парного $t$-тесту — контроль помилки I роду на рівні $\alpha=0.05$ є надійнішим при $n=20$, і якісно обґрунтуйте свій вибір.","## Setup (what each test uses and assumes)

We observe paired differences
$$D_i=\text{after}_i-\text{before}_i,\qquad i=1,\dots,20,$$
and test
$$H_0:\ \operatorname{median}(D)=0
\quad\text{vs}\quad
H_1:\ \operatorname{median}(D)>0.$$

- **One-sided sign test:** uses only the signs via
  $$S=\sum_{i:\,D_i\neq 0}\mathbf 1\{D_i>0\}.$$
  Under a continuous distribution with median $0$ (and, in particular, under symmetry about $0$),
  $$\mathbb P(D_i>0)=\tfrac12,$$
  so $S$ is (approximately) $\text{Binomial}(n,\tfrac12)$ (or $\text{Binomial}(m,\tfrac12)$ if $m$ non-ties). It ignores magnitudes and is robust to outliers.

- **One-sided paired $t$-test:** uses magnitudes through
  $$T=\frac{\bar D}{s_D/\sqrt{n}},\qquad
  \bar D=\frac1n\sum_{i=1}^n D_i,\quad
  s_D^2=\frac{1}{n-1}\sum_{i=1}^n (D_i-\bar D)^2.$$
  Its usual calibration relies on $D_i$ being (approximately) normal (or at least light-tailed enough that $\bar D$ and $s_D$ behave stably at $n=20$).

We now address the questions under (i) an ideal normal model and (ii) a symmetric heavy-tailed model centered at $\delta>0$.

---

## 1) Under an ideal normal model, which test is typically more powerful for small positive shifts, and why?

Under the normal location model
$$D_i \stackrel{iid}{\sim} N(\delta,\sigma^2),$$
the **paired $t$-test is typically more powerful** for detecting small $\delta>0$.

Reason: under normality, the mean $\bar D$ is an (asymptotically) efficient estimator of the location shift, and the $t$-statistic standardizes it using the appropriate variability. The test uses *both* direction and magnitude information, and (in the normal model) it is essentially the likelihood-based optimal procedure for location shifts.

By contrast, the sign test reduces each $D_i$ to $\mathbf 1\{D_i>0\}$, discarding how far above/below $0$ each observation lies. Under normality those magnitudes carry substantial information about $\delta$, so discarding them reduces efficiency and hence power for small shifts.

---

## 2) Under a symmetric heavy-tailed model centered at $\delta>0$, how can rare extremes affect the paired $t$-test’s power relative to the sign test?

Assume a symmetric heavy-tailed location model
$$D=\delta+\varepsilon,\qquad \varepsilon \text{ symmetric about }0,$$
with heavy tails (e.g., $\varepsilon\sim t_\nu$ for small $\nu$), so occasional extreme $|D_i|$ occur.

Rare extremes can hurt the paired $t$-test because they affect **both** parts of
$$T=\frac{\bar D}{s_D/\sqrt{n}}:$$

- **Denominator inflation:** a single outlier can make $s_D$ very large since $s_D^2$ uses squared deviations. This increases $s_D/\sqrt{n}$ and **shrinks $T$**, making rejection less likely even when $\delta>0$.
- **Numerator instability:** because $\bar D$ averages magnitudes, a rare extreme negative draw (possible even when centered at $\delta>0$) can pull $\bar D$ downward, sometimes offsetting the positive shift.

In contrast, the sign test depends only on whether each $D_i$ is positive or negative. An outlier counts as just one sign, so extreme magnitudes do not inflate any variance estimate or dominate the statistic. Thus, heavy tails tend to degrade the $t$-test’s power more than the sign test’s.

---

## 3) A plausible regime where the sign test has *higher power* than the paired $t$-test (robustness, misspecification, efficiency)

A plausible regime is:

- **Very heavy tails / non-negligible outlier probability** (e.g., $t_\nu$ with small $\nu$, or a mixture distribution with a small probability of extremely large values), and
- **Small-to-moderate positive shift $\delta>0$**, so that the “typical” observation is slightly positive but not so large that it overwhelms occasional extreme negatives and the variance inflation.

In this regime:

- The paired $t$-test is **model-misspecified** (normality/light tails fails), and because it relies on $\bar D$ and especially $s_D$, its effective signal-to-noise ratio can collapse: outliers can make $s_D$ huge and/or make $\bar D$ erratic. This reduces power.
- The sign test is **robust**: it targets
  $$p(\delta)=\mathbb P(D>0)=\mathbb P(\varepsilon>-\delta)=F_\varepsilon(\delta),$$
  which exceeds $1/2$ for any $\delta>0$ under symmetry. Its performance depends mainly on how often observations fall above $0$, not on how extreme the tails are.

Interpreting this in terms of **efficiency**: under the *correct* normal model, the $t$-test is more efficient (higher power per sample size), and the sign test is “less powerful.” But under heavy-tailed **misspecification**, the $t$-test’s efficiency can deteriorate sharply (unbounded sensitivity to extremes), while the sign test’s efficiency is comparatively stable (bounded influence). In sufficiently heavy-tailed/outlier-prone settings, that robustness can translate into **higher actual power** at $n=20$.

---

## 4) In the heavy-tailed setting with $n=20$, which test has more reliable type I error control at $\alpha=0.05$, and why?

The **sign test** generally has more reliable type I error control at $\alpha=0.05$ in this setting.

- Under $H_0:\operatorname{median}(D)=0$ with a continuous distribution symmetric about $0$,
  $$\mathbb P(D_i>0)=\tfrac12,$$
  so the sign count $S$ has an (essentially) **exact binomial null distribution**, unaffected by tail heaviness. Outliers do not change the probability of being positive under the null; they only change magnitudes, which the sign test ignores.

- The paired $t$-test uses a $t_{19}$ reference distribution that is exact under normality and only approximate otherwise. With **heavy tails and small $n=20$**, the sampling distribution of $T$ can deviate materially from $t_{19}$ because $\bar D$ and especially $s_D$ are unstable. This can lead to **size distortion** (often liberal, sometimes conservative depending on the exact distribution), making $\alpha=0.05$ less dependable.

---

## Final answers (summary)

1) Under normality, the **paired $t$-test** is typically more powerful for small positive shifts because it uses magnitudes and is (approximately) optimal/most efficient for normal location shifts.

2) Under symmetric heavy tails, rare extremes can **inflate $s_D$** and destabilize $\bar D$, often **reducing the $t$-test’s power**; the sign test is largely unaffected because it uses only signs.

3) When tails are very heavy / outliers occur with nontrivial probability and $\delta$ is small-to-moderate, the $t$-test can lose efficiency due to misspecification and outlier sensitivity, while the sign test’s robustness can yield **higher power**.

4) With $n=20$ and heavy tails, **type I error control is more reliable for the sign test** (exact binomial calibration under symmetry/median conditions) than for the paired $t$-test (approximate calibration that can break under heavy tails).",,0,,2973,33677,True
9,statistics,Statistical Tests,medium,"A factory historically has a defect rate of $p_0=0.08$ (i.e., each item is defective independently with probability $0.08$). After installing a new production line, a quality engineer inspects a random sample of $n=120$ items produced on the new line and finds $x=5$ defectives.

1) Using an **exact one-sided binomial test** of
$$H_0: p = 0.08 \quad \text{vs.} \quad H_a: p < 0.08,$$
compute the exact $p$-value and state your conclusion at significance level $\alpha=0.05$.

2) Using the **normal approximation with continuity correction** for the same hypotheses, compute the approximate one-sided $p$-value and state your conclusion at $\alpha=0.05$.

3) Compare the two conclusions. Based on the numbers in this setting (including the observed $x$ relative to $np_0$), explain briefly **why** the normal approximation with continuity correction could be misleading here even though $n$ is moderately large, and describe what feature(s) of the data make the exact test preferable in this case.","Фабрика історично має частку браку $p_0=0.08$ (тобто кожен виріб є бракованим незалежно з імовірністю $0.08$). Після встановлення нової виробничої лінії інженер з якості перевіряє випадкову вибірку з $n=120$ виробів, виготовлених на новій лінії, і знаходить $x=5$ бракованих.

1) Використовуючи **точний односторонній біноміальний тест** для
$$H_0: p = 0.08 \quad \text{vs.} \quad H_a: p < 0.08,$$
обчисліть точне $p$-значення та сформулюйте висновок на рівні значущості $\alpha=0.05$.

2) Використовуючи **нормальне наближення з поправкою на неперервність** для тих самих гіпотез, обчисліть наближене одностороннє $p$-значення та сформулюйте висновок при $\alpha=0.05$.

3) Порівняйте два висновки. Виходячи з чисел у цій ситуації (зокрема, спостереженого $x$ відносно $np_0$), коротко поясніть, **чому** нормальне наближення з поправкою на неперервність може вводити в оману тут, навіть попри те, що $n$ є помірно великим, і опишіть, яка(і) особливість(ості) даних роблять точний тест кращим у цьому випадку.","## Given
- Historical defect rate: $p_0=0.08$
- Sample size: $n=120$
- Observed defectives: $x=5$
- Hypotheses (left-tailed):
$$
H_0: p=0.08
\quad\text{vs.}\quad
H_a: p<0.08.
$$
Under $H_0$, the count of defectives satisfies
$$
X \sim \text{Binomial}(120,0.08),\qquad \mathbb{E}_{H_0}[X]=np_0=120(0.08)=9.6.
$$
So the observed $x=5$ is below the null mean $9.6$, and we quantify how unlikely $X\le 5$ is under $H_0$.

---

## 1) Exact one-sided binomial test

For a left-tailed binomial test, the exact p-value is
$$
p\text{-value}_{\text{exact}}=\Pr_{H_0}(X\le 5)
=\sum_{k=0}^{5}\binom{120}{k}(0.08)^k(0.92)^{120-k}.
$$

Evaluating the terms (under $p_0=0.08$) gives approximately:
$$
\Pr(X=0)\approx 0.0000450,\;
\Pr(X=1)\approx 0.000469,\;
\Pr(X=2)\approx 0.00242,
$$
$$
\Pr(X=3)\approx 0.00824,\;
\Pr(X=4)\approx 0.0208,\;
\Pr(X=5)\approx 0.0416.
$$
Summing,
$$
p\text{-value}_{\text{exact}}=\Pr(X\le 5)\approx 0.0736.
$$

**Decision at $\alpha=0.05$:** since $0.0736>0.05$, **fail to reject $H_0$**.

**Conclusion (exact test):** At the 5% level, there is **insufficient evidence** that the new line’s defect rate is below $0.08$.

---

## 2) Normal approximation with continuity correction

Under $H_0$, approximate $X$ by a normal with
$$
\mu=np_0=9.6,\qquad
\sigma=\sqrt{np_0(1-p_0)}=\sqrt{120(0.08)(0.92)}=\sqrt{8.832}\approx 2.972.
$$

We want $\Pr(X\le 5)$. With continuity correction for the left tail:
$$
\Pr(X\le 5)\approx \Pr(N\le 5.5),\quad N\sim N(9.6,\,8.832).
$$
Standardize:
$$
z=\frac{5.5-9.6}{2.972}=\frac{-4.1}{2.972}\approx -1.38.
$$
Thus
$$
p\text{-value}_{\text{norm+CC}}\approx \Phi(-1.38)\approx 0.084.
$$

**Decision at $\alpha=0.05$:** since $0.084>0.05$, **fail to reject $H_0$**.

**Conclusion (normal+CC):** At the 5% level, there is **insufficient evidence** that the defect rate is below $0.08$.

---

## 3) Comparison and why the normal approximation can be misleading here

- Exact: $p\text{-value}\approx 0.0736$  $\Rightarrow$ fail to reject at $0.05$  
- Normal+CC: $p\text{-value}\approx 0.084$ $\Rightarrow$ fail to reject at $0.05$

So **both methods agree** here (no rejection), but the normal approximation produces a **noticeably larger** p-value.

### Why normal+CC can be misleading in this setting
Even with $n=120$, the approximation can be poor in the **relevant tail** because:

1. **Small $p_0$ implies skewness.**  
   With $p_0=0.08$ far from $0.5$, $\text{Bin}(120,0.08)$ is not very symmetric; tail areas (especially the left tail) can be distorted by a symmetric normal approximation.

2. **We are in a discrete, small-count tail region.**  
   The p-value is $\Pr(X\le 5)$, which depends on only the integer masses at $0,1,2,3,4,5$. In this range, the binomial CDF moves in **large jumps**, and a continuous normal curve (even with continuity correction) cannot match those jumps exactly.

3. **Observed $x$ is well below $np_0$.**  
   Here $x=5$ versus $np_0=9.6$, so inference depends on an **extreme lower-tail** probability; approximations tend to be least reliable in tails.

### Why the exact test is preferable
The exact binomial test computes $\Pr_{0.08}(X\le 5)$ under the *true* null distribution with no symmetry/continuity assumptions, making it more reliable when $p_0$ is small and the decision hinges on a discrete tail probability.

---

## Final answers
1. **Exact p-value:** $\boxed{0.0736}$; **fail to reject** $H_0$ at $\alpha=0.05$.  
2. **Normal+CC p-value:** $\boxed{0.084}$; **fail to reject** $H_0$ at $\alpha=0.05$.  
3. Both fail to reject, but normal+CC is more conservative here; exact is preferred due to discreteness and skewness with small $p_0$ and a lower-tail, small-count event.","## Дано
- Історичний рівень браку: $p_0=0.08$
- Розмір вибірки: $n=120$
- Спостережена кількість бракованих: $x=5$
- Гіпотези (лівосторонні):
$$
H_0: p=0.08
\quad\text{vs.}\quad
H_a: p<0.08.
$$
За $H_0$ кількість бракованих задовольняє
$$
X \sim \text{Binomial}(120,0.08),\qquad \mathbb{E}_{H_0}[X]=np_0=120(0.08)=9.6.
$$
Отже, спостережене $x=5$ є нижчим за нульове середнє $9.6$, і ми кількісно оцінюємо, наскільки малоймовірним є $X\le 5$ за $H_0$.

---

## 1) Точний односторонній біноміальний тест

Для лівостороннього біноміального тесту точне p-значення дорівнює
$$
p\text{-value}_{\text{exact}}=\Pr_{H_0}(X\le 5)
=\sum_{k=0}^{5}\binom{120}{k}(0.08)^k(0.92)^{120-k}.
$$

Обчислення доданків (за $p_0=0.08$) дає приблизно:
$$
\Pr(X=0)\approx 0.0000450,\;
\Pr(X=1)\approx 0.000469,\;
\Pr(X=2)\approx 0.00242,
$$
$$
\Pr(X=3)\approx 0.00824,\;
\Pr(X=4)\approx 0.0208,\;
\Pr(X=5)\approx 0.0416.
$$
Підсумовуючи,
$$
p\text{-value}_{\text{exact}}=\Pr(X\le 5)\approx 0.0736.
$$

**Рішення при $\alpha=0.05$:** оскільки $0.0736>0.05$, **не відхиляємо $H_0$**.

**Висновок (точний тест):** На рівні 5% є **недостатньо доказів**, що рівень браку на новій лінії нижчий за $0.08$.

---

## 2) Нормальна апроксимація з поправкою на неперервність

За $H_0$ апроксимуємо $X$ нормальним розподілом з
$$
\mu=np_0=9.6,\qquad
\sigma=\sqrt{np_0(1-p_0)}=\sqrt{120(0.08)(0.92)}=\sqrt{8.832}\approx 2.972.
$$

Нам потрібно $\Pr(X\le 5)$. Із поправкою на неперервність для лівого хвоста:
$$
\Pr(X\le 5)\approx \Pr(N\le 5.5),\quad N\sim N(9.6,\,8.832).
$$
Стандартизуємо:
$$
z=\frac{5.5-9.6}{2.972}=\frac{-4.1}{2.972}\approx -1.38.
$$
Отже,
$$
p\text{-value}_{\text{norm+CC}}\approx \Phi(-1.38)\approx 0.084.
$$

**Рішення при $\alpha=0.05$:** оскільки $0.084>0.05$, **не відхиляємо $H_0$**.

**Висновок (нормальна+CC):** На рівні 5% є **недостатньо доказів**, що рівень браку нижчий за $0.08$.

---

## 3) Порівняння та чому нормальна апроксимація тут може вводити в оману

- Точний: $p\text{-value}\approx 0.0736$  $\Rightarrow$ не відхиляємо при $0.05$  
- Нормальна+CC: $p\text{-value}\approx 0.084$ $\Rightarrow$ не відхиляємо при $0.05$

Отже, **обидва методи узгоджуються** тут (відхилення немає), але нормальна апроксимація дає **помітно більше** p-значення.

### Чому нормальна+CC може вводити в оману в цій ситуації
Навіть за $n=120$ апроксимація може бути поганою у **відповідному хвості**, тому що:

1. **Мале $p_0$ спричиняє асиметрію.**  
   Оскільки $p_0=0.08$ далеко від $0.5$, $\text{Bin}(120,0.08)$ не є дуже симетричним; площі хвостів (особливо лівого хвоста) можуть спотворюватися симетричною нормальною апроксимацією.

2. **Ми перебуваємо в дискретній, хвостовій області малих лічильників.**  
   p-значення — це $\Pr(X\le 5)$, яке залежить лише від цілих мас імовірності при $0,1,2,3,4,5$. У цьому діапазоні біноміальна CDF змінюється **великими стрибками**, і неперервна нормальна крива (навіть із поправкою на неперервність) не може точно відтворити ці стрибки.

3. **Спостережене $x$ значно нижче за $np_0$.**  
   Тут $x=5$ проти $np_0=9.6$, тож висновок залежить від **екстремальної ймовірності нижнього хвоста**; апроксимації зазвичай найменш надійні саме в хвостах.

### Чому точний тест є кращим
Точний біноміальний тест обчислює $\Pr_{0.08}(X\le 5)$ за *істинним* нульовим розподілом без припущень про симетрію/неперервність, що робить його надійнішим, коли $p_0$ мале і рішення залежить від дискретної хвостової ймовірності.

---

## Остаточні відповіді
1. **Точне p-значення:** $\boxed{0.0736}$; **не відхиляємо** $H_0$ при $\alpha=0.05$.  
2. **p-значення (нормальна+CC):** $\boxed{0.084}$; **не відхиляємо** $H_0$ при $\alpha=0.05$.  
3. Обидва не відхиляють, але нормальна+CC тут більш консервативна; точний тест є кращим через дискретність і асиметрію при малому $p_0$ та події нижнього хвоста з малими лічильниками.",7,,2416,70136,True
10,statistics,Statistical Tests,medium,"You conduct $m=10$ independent hypothesis tests (one per experiment) and obtain the following p-values:

$$
\{0.0008,\ 0.004,\ 0.009,\ 0.012,\ 0.018,\ 0.031,\ 0.044,\ 0.071,\ 0.13,\ 0.26\}.
$$

Let the desired overall error control level be $\alpha=0.05$.

**(a) Bonferroni (FWER control).** Using the Bonferroni procedure to control the familywise error rate at $\alpha=0.05$, determine which hypotheses are rejected.

**(b) Benjamini–Hochberg (FDR control).** Using the Benjamini–Hochberg step-up procedure to control the false discovery rate at $q=0.05$, determine which hypotheses are rejected.

**(c) Compare outcomes.** Report (i) the number of rejections under Bonferroni and under Benjamini–Hochberg, and (ii) which specific p-values are rejected by Benjamini–Hochberg but not by Bonferroni.

**(d) Tradeoff in error definitions.** In 2–4 sentences, explain why it is possible for Benjamini–Hochberg to reject more hypotheses than Bonferroni at the same nominal level (0.05), referring explicitly to the difference between controlling FWER and controlling FDR.","Ви проводите $m=10$ незалежних перевірок гіпотез (по одній на експеримент) і отримуєте такі p-значення:

$$
\{0.0008,\ 0.004,\ 0.009,\ 0.012,\ 0.018,\ 0.031,\ 0.044,\ 0.071,\ 0.13,\ 0.26\}.
$$

Нехай бажаний загальний рівень контролю помилки дорівнює $\alpha=0.05$.

**(a) Бонферроні (контроль FWER).** Використовуючи процедуру Бонферроні для контролю сімейної ймовірності помилки першого роду (FWER) на рівні $\alpha=0.05$, визначте, які гіпотези відхиляються.

**(b) Бенджаміні–Хохберг (контроль FDR).** Використовуючи покрокову (step-up) процедуру Бенджаміні–Хохберга для контролю частоти хибних відкриттів (FDR) на рівні $q=0.05$, визначте, які гіпотези відхиляються.

**(c) Порівняння результатів.** Повідомте (i) кількість відхилень за Бонферроні та за Бенджаміні–Хохбергом і (ii) які саме p-значення відхиляються за Бенджаміні–Хохбергом, але не відхиляються за Бонферроні.

**(d) Компроміс у визначеннях помилки.** У 2–4 реченнях поясніть, чому процедура Бенджаміні–Хохберга може відхиляти більше гіпотез, ніж процедура Бонферроні, за того самого номінального рівня (0.05), явно посилаючись на різницю між контролем FWER і контролем FDR.","## Given
There are $m=10$ tests with p-values (already sorted):
$$
p_{(1)}=0.0008,\ p_{(2)}=0.004,\ p_{(3)}=0.009,\ p_{(4)}=0.012,\ p_{(5)}=0.018,\ p_{(6)}=0.031,\ p_{(7)}=0.044,\ p_{(8)}=0.071,\ p_{(9)}=0.13,\ p_{(10)}=0.26,
$$
and overall level $\alpha=0.05$ (and $q=0.05$ for BH).

---

## (a) Bonferroni (FWER control at $\alpha=0.05$)
Bonferroni uses the per-test cutoff
$$
\alpha_{\text{Bonf}}=\frac{\alpha}{m}=\frac{0.05}{10}=0.005.
$$
Reject $H_i$ if $p_i \le 0.005$. Comparing:
- $0.0008 \le 0.005$ ⇒ reject,
- $0.004 \le 0.005$ ⇒ reject,
- $0.009 > 0.005$ ⇒ do not reject (and all larger p-values also do not reject).

**Bonferroni rejections:** $\boxed{\{0.0008,\ 0.004\}}$ (2 rejections).

---

## (b) Benjamini–Hochberg (FDR control at $q=0.05$)
BH computes critical values
$$
\frac{i}{m}q=\frac{i}{10}\cdot 0.05 = 0.005\,i,\qquad i=1,\dots,10,
$$
and finds the largest $k$ such that $p_{(k)} \le 0.005k$. The comparisons are:

$$
\begin{array}{c|c|c}
i & p_{(i)} & 0.005i \\ \hline
1 & 0.0008 & 0.005 \ (\checkmark)\\
2 & 0.004  & 0.010 \ (\checkmark)\\
3 & 0.009  & 0.015 \ (\checkmark)\\
4 & 0.012  & 0.020 \ (\checkmark)\\
5 & 0.018  & 0.025 \ (\checkmark)\\
6 & 0.031  & 0.030 \ (\times)\\
7 & 0.044  & 0.035 \ (\times)\\
8 & 0.071  & 0.040 \ (\times)\\
9 & 0.13   & 0.045 \ (\times)\\
10& 0.26   & 0.050 \ (\times)
\end{array}
$$

Thus the largest passing index is $\boxed{k=5}$, and BH rejects $p_{(1)},\dots,p_{(5)}$.

**BH rejections:** $\boxed{\{0.0008,\ 0.004,\ 0.009,\ 0.012,\ 0.018\}}$ (5 rejections).

---

## (c) Compare outcomes
**(i) Number of rejections**
$$
\boxed{\text{Bonferroni: }2 \quad\text{and}\quad \text{BH: }5.}
$$

**(ii) Rejected by BH but not by Bonferroni**
$$
\{0.0008,0.004,0.009,0.012,0.018\}\setminus \{0.0008,0.004\}
= \boxed{\{0.009,\ 0.012,\ 0.018\}}.
$$

---

## (d) Tradeoff in error definitions (2–4 sentences)
Bonferroni controls the familywise error rate (FWER), $\Pr(V\ge 1)$, the probability of making even a single false rejection across all $m$ tests, so it uses the stringent cutoff $\alpha/m$ and is typically conservative. Benjamini–Hochberg controls the false discovery rate (FDR), $\mathbb{E}\!\left[\frac{V}{\max(R,1)}\right]$, the expected proportion of false rejections among the $R$ rejections, which allows more rejections as long as the false discoveries remain a small fraction on average. Because FDR control is weaker than FWER control, BH can legitimately reject more hypotheses at the same nominal level $0.05$.

---

> **Note:** This solution may contain errors or incomplete reasoning. The following issues were detected:
> - Solution lacks mathematical notation.
>
> Please verify the steps independently.","## Дано
Є $m=10$ тестів із p-значеннями (вже відсортованими):
$$
p_{(1)}=0.0008,\ p_{(2)}=0.004,\ p_{(3)}=0.009,\ p_{(4)}=0.012,\ p_{(5)}=0.018,\ p_{(6)}=0.031,\ p_{(7)}=0.044,\ p_{(8)}=0.071,\ p_{(9)}=0.13,\ p_{(10)}=0.26,
$$
та загальний рівень $\alpha=0.05$ (і $q=0.05$ для BH).

---

## (a) Бонферроні (контроль FWER на рівні $\alpha=0.05$)
Метод Бонферроні використовує поріг для кожного тесту
$$
\alpha_{\text{Bonf}}=\frac{\alpha}{m}=\frac{0.05}{10}=0.005.
$$
Відхиляємо $H_i$, якщо $p_i \le 0.005$. Порівнюємо:
- $0.0008 \le 0.005$ ⇒ відхиляємо,
- $0.004 \le 0.005$ ⇒ відхиляємо,
- $0.009 > 0.005$ ⇒ не відхиляємо (і всі більші p-значення також не відхиляємо).

**Відхилення за Бонферроні:** $\boxed{\{0.0008,\ 0.004\}}$ (2 відхилення).

---

## (b) Бенджаміні–Хохберг (контроль FDR на рівні $q=0.05$)
BH обчислює критичні значення
$$
\frac{i}{m}q=\frac{i}{10}\cdot 0.05 = 0.005\,i,\qquad i=1,\dots,10,
$$
і знаходить найбільше $k$ таке, що $p_{(k)} \le 0.005k$. Порівняння такі:

$$
\begin{array}{c|c|c}
i & p_{(i)} & 0.005i \\ \hline
1 & 0.0008 & 0.005 \ (\checkmark)\\
2 & 0.004  & 0.010 \ (\checkmark)\\
3 & 0.009  & 0.015 \ (\checkmark)\\
4 & 0.012  & 0.020 \ (\checkmark)\\
5 & 0.018  & 0.025 \ (\checkmark)\\
6 & 0.031  & 0.030 \ (\times)\\
7 & 0.044  & 0.035 \ (\times)\\
8 & 0.071  & 0.040 \ (\times)\\
9 & 0.13   & 0.045 \ (\times)\\
10& 0.26   & 0.050 \ (\times)
\end{array}
$$

Отже, найбільший індекс, що проходить, дорівнює $\boxed{k=5}$, і BH відхиляє $p_{(1)},\dots,p_{(5)}$.

**Відхилення за BH:** $\boxed{\{0.0008,\ 0.004,\ 0.009,\ 0.012,\ 0.018\}}$ (5 відхилень).

---

## (c) Порівняння результатів
**(i) Кількість відхилень**
$$
\boxed{\text{Бонферроні: }2 \quad\text{та}\quad \text{BH: }5.}
$$

**(ii) Відхилені за BH, але не за Бонферроні**
$$
\{0.0008,0.004,0.009,0.012,0.018\}\setminus \{0.0008,0.004\}
= \boxed{\{0.009,\ 0.012,\ 0.018\}}.
$$

---

## (d) Компроміс у визначеннях помилок (2–4 речення)
Метод Бонферроні контролює сімейну ймовірність помилки (FWER), $\Pr(V\ge 1)$, тобто ймовірність зробити хоча б одне хибне відхилення серед усіх $m$ тестів, тому він використовує суворий поріг $\alpha/m$ і зазвичай є консервативним. Метод Бенджаміні–Хохберга контролює частоту хибних відкриттів (FDR), $\mathbb{E}\!\left[\frac{V}{\max(R,1)}\right]$, тобто математичне сподівання частки хибних відхилень серед $R$ відхилень, що дозволяє робити більше відхилень, доки хибні відкриття в середньому залишаються малою часткою. Оскільки контроль FDR є слабшим, ніж контроль FWER, BH може правомірно відхиляти більше гіпотез за того самого номінального рівня $0.05$.

---

> **Примітка:** Це розв’язання може містити помилки або неповні міркування. Було виявлено такі проблеми:
> - Розв’язання не містить математичної нотації.
>
> Будь ласка, перевірте кроки самостійно.",0,Solution lacks mathematical notation.,2508,55299,True
11,linear_algebra,SVD,hard,"Let $A\in\mathbb{R}^{m\times n}$ with $m\ge n$ have full column rank and singular value decomposition $A=U\Sigma V^T$, where $\Sigma=\operatorname{diag}(\sigma_1,\dots,\sigma_n)$ with $\sigma_1\ge\cdots\ge\sigma_n>0$. Consider the least-squares problem
$$\min_{x\in\mathbb{R}^n}\|Ax-b\|_2,$$
with data $b\in\mathbb{R}^m$. Assume $A$ is ill-conditioned (i.e., $\kappa_2(A)=\sigma_1/\sigma_n\gg 1$).

1) Write the minimum-norm least-squares solution $x_\star$ explicitly in terms of the SVD by first decomposing $b$ in the left singular vector basis: define $\beta=U^Tb\in\mathbb{R}^m$ and denote its first $n$ components by $\beta_{1:n}$. Express $x_\star$ as a sum involving $(\beta_i,\sigma_i,v_i)$.

2) Now suppose the right-hand side is perturbed: $\tilde b=b+\delta b$ with $\|\delta b\|_2\le \varepsilon\|b\|_2$ for some small $\varepsilon>0$. Let $\tilde x_\star$ be the least-squares solution computed from $\tilde b$ using the SVD formula from part (1). Using the decomposition $\delta\beta=U^T\delta b$, derive an exact expression for the forward error $\Delta x:=\tilde x_\star-x_\star$ in the $V$-coordinates, and then show that
$$\|\Delta x\|_2^2=\sum_{i=1}^n \frac{(\delta\beta_i)^2}{\sigma_i^2}.$$

3) Define the relative forward error $E:=\|\Delta x\|_2/\|x_\star\|_2$. In terms of the coefficients $\beta_{1:n}$ and $\delta\beta_{1:n}$, obtain a formula for $E$ and use it to prove the upper bound
$$E\le \frac{\sigma_1}{\sigma_n}\,\frac{\|\delta b\|_2}{\|b_{\parallel}\|_2},\qquad b_{\parallel}:=U_{:,1:n}U_{:,1:n}^T b,$$
where $b_{\parallel}$ is the projection of $b$ onto $\operatorname{Range}(A)$.

4) (Alignment twist) Consider two unit-norm right-hand sides $b^{(a)}$ and $b^{(c)}$ such that their $U$-coordinates satisfy
$$U^Tb^{(a)}=\begin{bmatrix}1\\0\\\vdots\\0\end{bmatrix},\qquad U^Tb^{(c)}=\begin{bmatrix}0\\\vdots\\0\\1\\0\\\vdots\\0\end{bmatrix}$$
(where the lone $1$ in $b^{(c)}$ is in position $n$). For each case, assume an adversarial perturbation $\delta b$ with $\|\delta b\|_2=\varepsilon$ chosen to maximize $\|\Delta x\|_2$. Compute (in closed form) the maximized $\|\Delta x\|_2$ for $b^{(a)}$ and for $b^{(c)}$ and compare their scaling in terms of $\sigma_1,\sigma_n,$ and $\varepsilon$.

5) In floating-point arithmetic, suppose you instead compute $x$ via the normal equations $(A^TA)x=A^Tb$ and model the computed solution $\hat x$ as the exact solution to a perturbed system $(A^TA+\Delta M)\hat x=A^Tb+\Delta c$ with $\|\Delta M\|_2\le \gamma\|A^TA\|_2$ and $\|\Delta c\|_2\le \gamma\|A^T b\|_2$ for some small $\gamma>0$. Using singular values, express $\kappa_2(A^TA)$ in terms of $\kappa_2(A)$, and state a bound (in terms of $\sigma_1,\sigma_n,\gamma$, and suitable norms of $b$) showing how the conditioning of the normal equations amplifies forward error relative to the SVD-based computation.

Your final results should make explicit how components of $b$ (and $\delta b$) aligned with left singular vectors associated with small singular values lead to larger forward error through factors $1/\sigma_i$.","Нехай $A\in\mathbb{R}^{m\times n}$ з $m\ge n$ має повний стовпцевий ранг і сингулярний розклад $A=U\Sigma V^T$, де $\Sigma=\operatorname{diag}(\sigma_1,\dots,\sigma_n)$ з $\sigma_1\ge\cdots\ge\sigma_n>0$. Розглянемо задачу найменших квадратів
$$\min_{x\in\mathbb{R}^n}\|Ax-b\|_2,$$
з даними $b\in\mathbb{R}^m$. Припустімо, що $A$ є погано обумовленою (тобто $\kappa_2(A)=\sigma_1/\sigma_n\gg 1$).

1) Запишіть розв’язок задачі найменших квадратів мінімальної норми $x_\star$ явно через SVD, спочатку розклавши $b$ у базисі лівих сингулярних векторів: визначте $\beta=U^Tb\in\mathbb{R}^m$ і позначте її перші $n$ компонент як $\beta_{1:n}$. Виразіть $x_\star$ як суму, що містить $(\beta_i,\sigma_i,v_i)$.

2) Тепер припустімо, що праву частину збурено: $\tilde b=b+\delta b$ з $\|\delta b\|_2\le \varepsilon\|b\|_2$ для деякого малого $\varepsilon>0$. Нехай $\tilde x_\star$ — розв’язок найменших квадратів, обчислений із $\tilde b$ за допомогою SVD-формули з пункту (1). Використовуючи розклад $\delta\beta=U^T\delta b$, виведіть точний вираз для прямої похибки $\Delta x:=\tilde x_\star-x_\star$ у координатах $V$, а потім покажіть, що
$$\|\Delta x\|_2^2=\sum_{i=1}^n \frac{(\delta\beta_i)^2}{\sigma_i^2}.$$

3) Визначте відносну пряму похибку $E:=\|\Delta x\|_2/\|x_\star\|_2$. У термінах коефіцієнтів $\beta_{1:n}$ та $\delta\beta_{1:n}$ отримайте формулу для $E$ і використайте її, щоб довести верхню оцінку
$$E\le \frac{\sigma_1}{\sigma_n}\,\frac{\|\delta b\|_2}{\|b_{\parallel}\|_2},\qquad b_{\parallel}:=U_{:,1:n}U_{:,1:n}^T b,$$
де $b_{\parallel}$ — це проєкція $b$ на $\operatorname{Range}(A)$.

4) (Твіст вирівнювання) Розгляньте дві праві частини одиничної норми $b^{(a)}$ та $b^{(c)}$ такі, що їхні $U$-координати задовольняють
$$U^Tb^{(a)}=\begin{bmatrix}1\\0\\\vdots\\0\end{bmatrix},\qquad U^Tb^{(c)}=\begin{bmatrix}0\\\vdots\\0\\1\\0\\\vdots\\0\end{bmatrix}$$
(де єдина $1$ у $b^{(c)}$ стоїть на позиції $n$). Для кожного випадку припустіть антагоністичне збурення $\delta b$ з $\|\delta b\|_2=\varepsilon$, вибране так, щоб максимізувати $\|\Delta x\|_2$. Обчисліть (у замкненій формі) максимізоване $\|\Delta x\|_2$ для $b^{(a)}$ та для $b^{(c)}$ і порівняйте їхнє масштабування в термінах $\sigma_1,\sigma_n,$ та $\varepsilon$.

5) В арифметиці з плаваючою комою припустімо, що натомість ви обчислюєте $x$ через нормальні рівняння $(A^TA)x=A^Tb$ і моделюєте обчислений розв’язок $\hat x$ як точний розв’язок збуреної системи $(A^TA+\Delta M)\hat x=A^Tb+\Delta c$ з $\|\Delta M\|_2\le \gamma\|A^TA\|_2$ та $\|\Delta c\|_2\le \gamma\|A^T b\|_2$ для деякого малого $\gamma>0$. Використовуючи сингулярні значення, виразіть $\kappa_2(A^TA)$ через $\kappa_2(A)$ і сформулюйте оцінку (в термінах $\sigma_1,\sigma_n,\gamma$ та відповідних норм $b$), яка показує, як обумовленість нормальних рівнянь підсилює пряму похибку порівняно з обчисленням на основі SVD.

Ваші кінцеві результати мають явно показувати, як компоненти $b$ (та $\delta b$), вирівняні з лівими сингулярними векторами, що відповідають малим сингулярним значенням, призводять до більшої прямої похибки через множники $1/\sigma_i$.","## 1) SVD formula for the least-squares minimizer

Since $A\in\mathbb{R}^{m\times n}$ has full column rank ($\sigma_n>0$), the least-squares minimizer is unique and equals
$$
x_\star=A^+b,
\qquad 
A^+=V\Sigma^{-1}U^T,
\qquad 
\Sigma^{-1}=\operatorname{diag}\!\left(\frac1{\sigma_1},\dots,\frac1{\sigma_n}\right).
$$
Decompose $b$ in the left singular vector basis:
$$
\beta := U^T b\in\mathbb{R}^m,\qquad \beta_{1:n}=(\beta_1,\dots,\beta_n)^T.
$$
Then
$$
x_\star = V\Sigma^{-1}\beta_{1:n}
= \sum_{i=1}^n \frac{\beta_i}{\sigma_i}\,v_i,
\qquad \beta_i=u_i^Tb.
$$
This makes explicit the amplification factor $1/\sigma_i$ on the component of $b$ along $u_i$.

---

## 2) Exact forward error under a perturbation in $b$

Let $\tilde b=b+\delta b$ and define
$$
\delta\beta := U^T\delta b,\qquad \tilde\beta = U^T\tilde b = \beta+\delta\beta.
$$
Using the same SVD formula,
$$
\tilde x_\star = V\Sigma^{-1}\tilde\beta_{1:n}
=V\Sigma^{-1}(\beta_{1:n}+\delta\beta_{1:n}).
$$
Hence the *exact* forward error is
$$
\Delta x:=\tilde x_\star-x_\star
=V\Sigma^{-1}\delta\beta_{1:n}
=\sum_{i=1}^n \frac{\delta\beta_i}{\sigma_i}\,v_i.
$$
In $V$-coordinates,
$$
V^T\Delta x=\Sigma^{-1}\delta\beta_{1:n}.
$$
Because $V$ is orthogonal,
$$
\|\Delta x\|_2^2=\|\Sigma^{-1}\delta\beta_{1:n}\|_2^2
=\sum_{i=1}^n\left(\frac{\delta\beta_i}{\sigma_i}\right)^2
=\sum_{i=1}^n\frac{(\delta\beta_i)^2}{\sigma_i^2}.
$$
Thus perturbations aligned with $u_i$ are amplified by $1/\sigma_i$, worst when $\sigma_i$ is small.

---

## 3) Relative forward error and the bound involving $b_\parallel$

From part (1),
$$
\|x_\star\|_2^2=\|V^Tx_\star\|_2^2=\|\Sigma^{-1}\beta_{1:n}\|_2^2
=\sum_{i=1}^n\frac{\beta_i^2}{\sigma_i^2}.
$$
Therefore the relative forward error
$$
E:=\frac{\|\Delta x\|_2}{\|x_\star\|_2}
=
\frac{\sqrt{\sum_{i=1}^n\frac{(\delta\beta_i)^2}{\sigma_i^2}}}{\sqrt{\sum_{i=1}^n\frac{\beta_i^2}{\sigma_i^2}}}.
$$

Now define the projection of $b$ onto $\operatorname{Range}(A)=\operatorname{span}\{u_1,\dots,u_n\}$:
$$
b_\parallel := U_{:,1:n}U_{:,1:n}^T b.
$$
Since $U_{:,1:n}$ has orthonormal columns and $U_{:,1:n}^Tb=\beta_{1:n}$,
$$
b_\parallel = U_{:,1:n}\beta_{1:n},
\qquad 
\|b_\parallel\|_2=\|\beta_{1:n}\|_2.
$$

To bound $E$, use operator-norm inequalities:
$$
\|\Delta x\|_2=\|\Sigma^{-1}\delta\beta_{1:n}\|_2
\le \|\Sigma^{-1}\|_2\,\|\delta\beta_{1:n}\|_2
=\frac{1}{\sigma_n}\|\delta\beta_{1:n}\|_2
\le \frac{1}{\sigma_n}\|\delta b\|_2,
$$
because $\|\delta\beta\|_2=\|U^T\delta b\|_2=\|\delta b\|_2$ and truncation cannot increase the norm.

Also,
$$
\|x_\star\|_2=\|\Sigma^{-1}\beta_{1:n}\|_2
\ge \frac{1}{\sigma_1}\|\beta_{1:n}\|_2
=\frac{1}{\sigma_1}\|b_\parallel\|_2,
$$
since $1/\sigma_i\ge 1/\sigma_1$ for all $i$.

Combining,
$$
E=\frac{\|\Delta x\|_2}{\|x_\star\|_2}
\le
\frac{\frac{1}{\sigma_n}\|\delta b\|_2}{\frac{1}{\sigma_1}\|b_\parallel\|_2}
=
\frac{\sigma_1}{\sigma_n}\,\frac{\|\delta b\|_2}{\|b_\parallel\|_2}
=
\kappa_2(A)\,\frac{\|\delta b\|_2}{\|b_\parallel\|_2}.
$$

---

## 4) Alignment twist: worst-case $\|\Delta x\|_2$ for two special right-hand sides

Assume $\|b^{(a)}\|_2=\|b^{(c)}\|_2=1$ and
$$
U^Tb^{(a)}=e_1,\qquad U^Tb^{(c)}=e_n.
$$
Then
$$
x_\star^{(a)}=\frac{1}{\sigma_1}v_1,\qquad 
x_\star^{(c)}=\frac{1}{\sigma_n}v_n.
$$

For any perturbation with $\|\delta b\|_2=\varepsilon$, we have $\|\delta\beta\|_2=\varepsilon$, and
$$
\|\Delta x\|_2=\|\Sigma^{-1}\delta\beta_{1:n}\|_2.
$$
The adversary maximizes $\|\Sigma^{-1}y\|_2$ subject to $\|y\|_2=\varepsilon$ by aligning $y$ with the largest diagonal entry of $\Sigma^{-1}$, i.e. $1/\sigma_n$. Choose
$$
\delta\beta_{1:n}=\varepsilon e_n \quad(\text{equivalently } \delta b=\varepsilon u_n).
$$
Then
$$
\max_{\|\delta b\|_2=\varepsilon}\|\Delta x\|_2
=\left\|\Sigma^{-1}(\varepsilon e_n)\right\|_2
=\frac{\varepsilon}{\sigma_n}.
$$
So, in closed form,
$$
\boxed{\max \|\Delta x\|_2=\frac{\varepsilon}{\sigma_n}\ \text{for both } b^{(a)} \text{ and } b^{(c)}.}
$$

(If one compares *relative* errors: $\|x_\star^{(a)}\|_2=1/\sigma_1$ gives $E_{\max}^{(a)}=\varepsilon\,\sigma_1/\sigma_n=\varepsilon\kappa_2(A)$, while $\|x_\star^{(c)}\|_2=1/\sigma_n$ gives $E_{\max}^{(c)}=\varepsilon$.)

---

## 5) Normal equations: squaring of the condition number and amplified forward error

Normal equations solve
$$
(A^TA)x=A^Tb.
$$
Using the SVD $A=U\Sigma V^T$,
$$
A^TA = V\Sigma^2V^T,
$$
so the singular values (eigenvalues) of $A^TA$ are $\sigma_i^2$. Hence
$$
\kappa_2(A^TA)=\frac{\sigma_1^2}{\sigma_n^2}=\left(\frac{\sigma_1}{\sigma_n}\right)^2=\kappa_2(A)^2.
$$

With the floating-point model
$$
(A^TA+\Delta M)\hat x = A^Tb+\Delta c,
\qquad 
\|\Delta M\|_2\le \gamma\|A^TA\|_2,\quad
\|\Delta c\|_2\le \gamma\|A^Tb\|_2,
$$
a standard linear-system perturbation bound (assuming $\gamma\kappa_2(A^TA)<1$) gives
$$
\frac{\|\hat x-x_\star\|_2}{\|x_\star\|_2}
\;\le\;
\frac{\kappa_2(A^TA)}{1-\gamma\kappa_2(A^TA)}
\left(
\frac{\|\Delta c\|_2}{\|A^Tb\|_2}
+
\frac{\|\Delta M\|_2}{\|A^TA\|_2}
\right)
\;\le\;
\frac{2\gamma\,\kappa_2(A^TA)}{1-\gamma\kappa_2(A^TA)}.
$$
Substituting $\kappa_2(A^TA)=\kappa_2(A)^2=(\sigma_1/\sigma_n)^2$,
$$
\boxed{
\frac{\|\hat x-x_\star\|_2}{\|x_\star\|_2}
\;\le\;
\frac{2\gamma\,(\sigma_1/\sigma_n)^2}{1-\gamma(\sigma_1/\sigma_n)^2}
\qquad (\gamma(\sigma_1/\sigma_n)^2<1).
}
$$

**Interpretation:** the SVD-based mapping $b\mapsto x_\star$ amplifies $u_i$-aligned components by $1/\sigma_i$, while the normal equations inherit $\kappa_2(A^TA)=\kappa_2(A)^2$, i.e. an additional factor $\kappa_2(A)$ of sensitivity compared with methods that avoid forming $A^TA$.

---

> **Note:** This solution may contain errors or incomplete reasoning. The following issues were detected:
> - Solution lacks mathematical notation.
>
> Please verify the steps independently.",,0,Solution is too short.; Solution lacks mathematical notation.; Solution may lack mathematical reasoning.,5034,141250,True
12,linear_algebra,SVD,hard,"Let $m=5$ and $n=4$. Let the desired nonzero singular values be the multiset $\{5,2,2\}$ (so $r=3$).

Define the subspace $W\subset \mathbb{R}^5$ by
$$W=\operatorname{span}\{w_1,w_2\},\quad w_1=\tfrac{1}{\sqrt{2}}(1,1,0,0,0)^T,\; w_2=\tfrac{1}{\sqrt{2}}(0,0,1,1,0)^T.$$
Let $P_W$ denote the orthogonal projector onto $W$.

Construct an explicit real matrix $A\in\mathbb{R}^{5\times 4}$ such that:

1. The nonzero singular values of $A$ are exactly $5,2,2$ (counted with multiplicity), and $\operatorname{rank}(A)=3$.
2. In some SVD $A=U\Sigma V^T$ with left singular vectors $u_1,u_2,u_3\in\mathbb{R}^5$ corresponding to the singular values $5,2,2$ respectively, the following linear constraints hold:
   $$P_W u_1=w_1,\qquad P_W u_2=0,\qquad P_W u_3=w_2.$$
3. Additionally, require that the fourth and fifth left singular vectors $u_4,u_5$ (spanning $\ker(A^T)$) satisfy
   $$u_4\in W\quad\text{and}\quad u_5\perp W.$$

Give one concrete matrix $A$ that satisfies all conditions (any valid construction earns full credit).","Нехай $m=5$ і $n=4$. Нехай бажані ненульові сингулярні значення утворюють мультимножину $\{5,2,2\}$ (отже, $r=3$).

Визначимо підпростір $W\subset \mathbb{R}^5$ як
$$W=\operatorname{span}\{w_1,w_2\},\quad w_1=\tfrac{1}{\sqrt{2}}(1,1,0,0,0)^T,\; w_2=\tfrac{1}{\sqrt{2}}(0,0,1,1,0)^T.$$
Нехай $P_W$ позначає ортогональний проектор на $W$.

Побудуйте явну дійсну матрицю $A\in\mathbb{R}^{5\times 4}$ таку, що:

1. Ненульові сингулярні значення $A$ є рівно $5,2,2$ (з урахуванням кратності), і $\operatorname{rank}(A)=3$.
2. У деякому SVD $A=U\Sigma V^T$ з лівими сингулярними векторами $u_1,u_2,u_3\in\mathbb{R}^5$, що відповідають сингулярним значенням $5,2,2$ відповідно, виконуються такі лінійні обмеження:
   $$P_W u_1=w_1,\qquad P_W u_2=0,\qquad P_W u_3=w_2.$$
3. Додатково вимагайте, щоб четвертий і п’ятий ліві сингулярні вектори $u_4,u_5$ (які натягують $\ker(A^T)$) задовольняли
   $$u_4\in W\quad\text{та}\quad u_5\perp W.$$

Наведіть одну конкретну матрицю $A$, яка задовольняє всі умови (будь-яка коректна побудова дає повний бал).","## Construction of a matrix $A\in\mathbb{R}^{5\times 4}$

We will explicitly build an SVD $A=U\Sigma V^T$ that enforces the required singular values and the projection constraints on the left singular vectors.

---

## 1) The subspace $W$ and its projector

Given
$$
w_1=\frac{1}{\sqrt2}(1,1,0,0,0)^T,\qquad
w_2=\frac{1}{\sqrt2}(0,0,1,1,0)^T,
$$
we have $\|w_1\|=\|w_2\|=1$ and $w_1^T w_2=0$, so $\{w_1,w_2\}$ is an orthonormal basis of the 2D subspace $W=\operatorname{span}\{w_1,w_2\}$.

Hence the orthogonal projector onto $W$ is
$$
P_W = w_1w_1^T+w_2w_2^T
=\frac12\begin{pmatrix}
1&1&0&0&0\\
1&1&0&0&0\\
0&0&1&1&0\\
0&0&1&1&0\\
0&0&0&0&0
\end{pmatrix}.
$$

---

## 2) Choose left singular vectors $u_1,u_2,u_3$ to meet the constraints

We need
$$
P_Wu_1=w_1,\qquad P_Wu_2=0,\qquad P_Wu_3=w_2.
$$

- Since $w_1,w_2\in W$ and $P_Wx=x$ for $x\in W$, take
$$
u_1:=w_1,\qquad u_3:=w_2.
$$
Then $P_Wu_1=w_1$ and $P_Wu_3=w_2$.

- To get $P_Wu_2=0$, it suffices to choose $u_2\in W^\perp$. A simple choice is
$$
u_2:=e_5=(0,0,0,0,1)^T,
$$
which is orthogonal to both $w_1$ and $w_2$, hence lies in $W^\perp$, so $P_Wu_2=0$.

Thus,
$$
u_1=\frac{1}{\sqrt2}(1,1,0,0,0)^T,\quad
u_2=(0,0,0,0,1)^T,\quad
u_3=\frac{1}{\sqrt2}(0,0,1,1,0)^T,
$$
and these are mutually orthonormal.

---

## 3) Choose $u_4,u_5$ spanning $\ker(A^T)$ with the extra constraints

We require
$$
u_4\in W\quad\text{and}\quad u_5\perp W,
$$
and $\{u_1,\dots,u_5\}$ should be an orthonormal basis of $\mathbb{R}^5$.

- Pick $u_4\in W$ orthogonal to $u_1=w_1$ and $u_3=w_2$. One convenient choice is
$$
u_4:=\frac{1}{\sqrt2}(1,-1,0,0,0)^T\in W,
$$
since it is orthogonal to $w_1$ and also orthogonal to $w_2$ (disjoint support).

- Pick $u_5\perp W$ and orthogonal to $u_2=e_5$. A convenient choice is
$$
u_5:=\frac{1}{\sqrt2}(0,0,1,-1,0)^T,
$$
which is orthogonal to both $w_1$ and $w_2$ (hence $\perp W$) and has zero 5th component (hence $\perp u_2$).

Now define
$$
U=\begin{bmatrix}u_1&u_2&u_3&u_4&u_5\end{bmatrix}
=
\begin{pmatrix}
\frac1{\sqrt2} & 0 & 0 & \frac1{\sqrt2} & 0\\
\frac1{\sqrt2} & 0 & 0 & -\frac1{\sqrt2} & 0\\
0 & 0 & \frac1{\sqrt2} & 0 & \frac1{\sqrt2}\\
0 & 0 & \frac1{\sqrt2} & 0 & -\frac1{\sqrt2}\\
0 & 1 & 0 & 0 & 0
\end{pmatrix}.
$$
Its columns are orthonormal, so $U^TU=I_5$.

---

## 4) Choose $V$ and $\Sigma$ to enforce the singular values $5,2,2$

Take the simplest orthogonal choice
$$
V:=I_4.
$$

Let $\Sigma\in\mathbb{R}^{5\times 4}$ have diagonal entries $5,2,2,0$:
$$
\Sigma=
\begin{pmatrix}
5 & 0 & 0 & 0\\
0 & 2 & 0 & 0\\
0 & 0 & 2 & 0\\
0 & 0 & 0 & 0\\
0 & 0 & 0 & 0
\end{pmatrix}.
$$
Then $\Sigma$ has rank $3$ and nonzero singular values $5,2,2$.

---

## 5) Form $A=U\Sigma V^T$ and write it explicitly

Since $V=I_4$, we have $A=U\Sigma$. Multiplying by $\Sigma$ scales the first three columns of $U$ by $5,2,2$ and makes the fourth column zero:
$$
A=\big[\,5u_1\ \ 2u_2\ \ 2u_3\ \ 0\,\big].
$$
Compute:
$$
5u_1=\left(\frac{5}{\sqrt2},\frac{5}{\sqrt2},0,0,0\right)^T,\quad
2u_2=(0,0,0,0,2)^T,\quad
2u_3=(0,0,\sqrt2,\sqrt2,0)^T.
$$
Thus one valid concrete matrix is
$$
\boxed{
A=
\begin{pmatrix}
\frac{5}{\sqrt2} & 0 & 0 & 0\\
\frac{5}{\sqrt2} & 0 & 0 & 0\\
0 & 0 & \sqrt2 & 0\\
0 & 0 & \sqrt2 & 0\\
0 & 2 & 0 & 0
\end{pmatrix}\in\mathbb{R}^{5\times 4}.
}
$$

---

## 6) Verification of the required properties

1. **Singular values and rank:** By construction $A=U\Sigma V^T$ with orthogonal $U,V$, so $A$ has the same singular values as $\Sigma$: nonzero singular values $5,2,2$, and $\operatorname{rank}(A)=\operatorname{rank}(\Sigma)=3$.

2. **Projection constraints on $u_1,u_2,u_3$:**
   $$
u_1=w_1\in W \implies P_Wu_1=w_1,\qquad
   u_2\in W^\perp \implies P_Wu_2=0,\qquad
   u_3=w_2\in W \implies P_Wu_3=w_2.
$$

3. **Kernel constraints:** Since $\sigma_4=0$ and $\operatorname{rank}(A)=3$, we have $\dim\ker(A^T)=5-3=2$, and in this SVD it is spanned by the last two left singular vectors:
   $$
\ker(A^T)=\operatorname{span}\{u_4,u_5\},
$$
   with $u_4\in W$ and $u_5\perp W$ by construction.

All conditions are satisfied.

---

> **Note:** This solution may contain errors or incomplete reasoning. The following issues were detected:
> - Solution lacks mathematical notation.
>
> Please verify the steps independently.",,0,Solution lacks mathematical notation.,2981,96350,True
13,linear_algebra,SVD,hard,"Let $H=L^2([0,1])$ and define the integral operator $T:H\to H$ by
$$
(Tf)(x)=\int_0^1 K(x,y)f(y)\,dy,
$$
where $K\in L^2([0,1]^2)$ is a (not necessarily symmetric) square-integrable kernel. Consider the adjoint $T^*$ and the positive self-adjoint operator $A=T^*T$.

1. Prove that $T$ is a compact operator on $H$, and that $A$ is compact, self-adjoint, and positive.

2. Using the spectral theorem for compact self-adjoint operators, let $\{\lambda_n\}_{n\ge 1}$ be the nonzero eigenvalues of $A$ (listed with multiplicity) with orthonormal eigenvectors $\{v_n\}_{n\ge 1}\subset H$, so that $Av_n=\lambda_n v_n$ and $\lambda_n\downarrow 0$. Define $\sigma_n=\sqrt{\lambda_n}$.

   (a) For each $n$ with $\sigma_n>0$, define $u_n:=\sigma_n^{-1}Tv_n$. Show that $\{u_n\}$ is an orthonormal set in $H$ and that $Tv_n=\sigma_n u_n$ and $T^*u_n=\sigma_n v_n$.

   (b) Formulate and justify an “SVD-like” series representation of $T$ in terms of $\{\sigma_n,u_n,v_n\}$ (you may express it as a series of rank-one operators). Specify precisely on which subspace(s) of $H$ the expansion acts as written, and what happens on $\ker(T)$ and $\ker(T^*)$.

3. Let
$$
T_N:=\sum_{n=1}^N \sigma_n\, u_n\otimes v_n,\qquad (u\otimes v)(f):=\langle f,v\rangle u.
$$
Determine which of the following convergences always hold, and which can fail, for the operator $T$ described above:

   (i) $\|T-T_N\|_{\mathrm{HS}}\to 0$ as $N\to\infty$.

   (ii) $\|T-T_N\|_{\mathrm{op}}\to 0$ as $N\to\infty$.

For each mode of convergence, state the sharp condition(s) in terms of $\{\sigma_n\}$ that characterize convergence, and relate them to the fact that $K\in L^2([0,1]^2)$.

4. (Norm comparison twist.) Suppose additionally that $K\in L^\infty([0,1]^2)$. Discuss whether this stronger assumption changes your conclusions in part (3) about convergence in operator norm versus Hilbert–Schmidt norm, and justify your answer.","Нехай $H=L^2([0,1])$ і визначимо інтегральний оператор $T:H\to H$ за формулою
$$
(Tf)(x)=\int_0^1 K(x,y)f(y)\,dy,
$$
де $K\in L^2([0,1]^2)$ — (не обов’язково симетричне) квадратно-інтегровне ядро. Розглянемо спряжений оператор $T^*$ та додатний самоспряжений оператор $A=T^*T$.

1. Доведіть, що $T$ є компактним оператором на $H$, і що $A$ є компактним, самоспряженим і додатним.

2. Використовуючи спектральну теорему для компактних самоспряжених операторів, нехай $\{\lambda_n\}_{n\ge 1}$ — ненульові власні значення $A$ (перелічені з кратністю) з ортонормованими власними векторами $\{v_n\}_{n\ge 1}\subset H$, так що $Av_n=\lambda_n v_n$ і $\lambda_n\downarrow 0$. Визначимо $\sigma_n=\sqrt{\lambda_n}$.

   (a) Для кожного $n$ з $\sigma_n>0$ визначимо $u_n:=\sigma_n^{-1}Tv_n$. Покажіть, що $\{u_n\}$ є ортонормованою множиною в $H$ і що $Tv_n=\sigma_n u_n$ та $T^*u_n=\sigma_n v_n$.

   (b) Сформулюйте та обґрунтуйте «SVD-подібне» серійне подання $T$ в термінах $\{\sigma_n,u_n,v_n\}$ (можна подати його як ряд операторів рангу один). Точно вкажіть, на якому(их) підпросторі(ах) $H$ розклад діє в записаному вигляді, і що відбувається на $\ker(T)$ та $\ker(T^*)$.

3. Нехай
$$
T_N:=\sum_{n=1}^N \sigma_n\, u_n\otimes v_n,\qquad (u\otimes v)(f):=\langle f,v\rangle u.
$$
Визначте, які з наведених збіжностей завжди мають місце, а які можуть не виконуватися, для оператора $T$, описаного вище:

   (i) $\|T-T_N\|_{\mathrm{HS}}\to 0$ при $N\to\infty$.

   (ii) $\|T-T_N\|_{\mathrm{op}}\to 0$ при $N\to\infty$.

Для кожного режиму збіжності сформулюйте точні умови в термінах $\{\sigma_n\}$, що характеризують збіжність, і пов’яжіть їх із фактом, що $K\in L^2([0,1]^2)$.

4. (Поворот із порівнянням норм.) Припустімо додатково, що $K\in L^\infty([0,1]^2)$. Обговоріть, чи змінює це сильніше припущення ваші висновки в пункті (3) щодо збіжності в операторній нормі порівняно з нормою Гільберта–Шмідта, і обґрунтуйте вашу відповідь.","## Solution

### Step 1: Show T is Hilbert–Schmidt: for f∈L^2, use Cauchy–Schwarz to bound |Tf(x)| and then ∥Tf∥_2 ≤ ∥K∥_{L^2([0,1]^2)}∥f∥_2. Conclude T is bounded and ∥T∥_{HS}=∥K∥_2.

Fix $f\in H=L^2([0,1])$. For a.e. $x\in[0,1]$, the function $y\mapsto K(x,y)$ lies in $L^2([0,1])$ (by Fubini/Tonelli since $K\in L^2([0,1]^2)$), so the integral
$$
(Tf)(x)=\int_0^1 K(x,y)f(y)\,dy
$$
is well-defined for a.e. $x$.

### Pointwise (in $x$) Cauchy–Schwarz bound
For a.e. $x$,
$$
|(Tf)(x)|
=\left|\int_0^1 K(x,y)f(y)\,dy\right|
\le \left(\int_0^1 |K(x,y)|^2\,dy\right)^{1/2}\left(\int_0^1 |f(y)|^2\,dy\right)^{1/2}.
$$
Let
$$
k(x):=\left(\int_0^1 |K(x,y)|^2\,dy\right)^{1/2}.
$$
Then the inequality is
$$
|(Tf)(x)|\le k(x)\,\|f\|_{L^2([0,1])}.
$$

### $L^2$-bound for $Tf$
Square and integrate in $x$:
$$
\|Tf\|_{L^2([0,1])}^2
=\int_0^1 |(Tf)(x)|^2\,dx
\le \int_0^1 k(x)^2\,dx\;\|f\|_2^2.
$$
But
$$
\int_0^1 k(x)^2\,dx
=\int_0^1\int_0^1 |K(x,y)|^2\,dy\,dx
=\|K\|_{L^2([0,1]^2)}^2.
$$
Hence
$$
\|Tf\|_2\le \|K\|_{L^2([0,1]^2)}\,\|f\|_2,
$$
so $T$ is bounded and
$$
\|T\|_{\mathrm{op}}\le \|K\|_{L^2([0,1]^2)}.
$$

### $T$ is Hilbert–Schmidt and $\|T\|_{\mathrm{HS}}=\|K\|_2$
Let $\{e_n\}_{n\ge1}$ be any orthonormal basis of $L^2([0,1])$. For a.e. $x$,
$$
(Te_n)(x)=\int_0^1 K(x,y)e_n(y)\,dy=\langle K(x,\cdot),e_n\rangle_{L^2_y}.
$$
By Parseval’s identity in the $y$-variable,
$$
\sum_{n=1}^\infty |(Te_n)(x)|^2
=\sum_{n=1}^\infty |\langle K(x,\cdot),e_n\rangle|^2
=\|K(x,\cdot)\|_{L^2_y}^2
=\int_0^1 |K(x,y)|^2\,dy
\quad\text{for a.e. }x.
$$
Integrate over $x$ and use Tonelli (nonnegative integrand) to swap sum and integral:
$$
\sum_{n=1}^\infty \|Te_n\|_2^2
=\sum_{n=1}^\infty \int_0^1 |(Te_n)(x)|^2\,dx
=\int_0^1 \sum_{n=1}^\infty |(Te_n)(x)|^2\,dx
=\int_0^1\int_0^1 |K(x,y)|^2\,dy\,dx
=\|K\|_{L^2([0,1]^2)}^2.
$$
Therefore $T$ is a Hilbert–Schmidt operator and
$$
\|T\|_{\mathrm{HS}}^2=\sum_{n=1}^\infty \|Te_n\|_2^2=\|K\|_2^2,
\qquad\text{i.e.}\qquad
\|T\|_{\mathrm{HS}}=\|K\|_{L^2([0,1]^2)}.
$$

This completes the step: $T$ is Hilbert–Schmidt (hence bounded), with $\|T\|_{\mathrm{HS}}=\|K\|_2$.

### Step 2: Prove compactness: approximate K in L^2 by finite-rank kernels K_m=∑_{j=1}^{r_m} a_j(x)b_j(y) (e.g., simple functions). Then T_m are finite-rank and ∥T−T_m∥_{HS}=∥K−K_m∥_2→0, hence ∥T−T_m∥_{op}→0, soT

Since $K\in L^2([0,1]^2)$, we can approximate $K$ in $L^2$ by finite-rank (separable) kernels.

### 1) Approximate $K$ by finite-rank kernels
A standard way is to use simple functions on $[0,1]^2$: there exists a sequence of simple functions $S_m$ such that
$$
\|K-S_m\|_{L^2([0,1]^2)}\to 0.
$$
Each simple function has the form
$$
S_m(x,y)=\sum_{k=1}^{N_m} c_{k,m}\,\mathbf 1_{E_{k,m}}(x,y),
$$
where the $E_{k,m}\subset [0,1]^2$ are measurable and disjoint.

Now refine the representation so that each $E_{k,m}$ is a finite union of measurable rectangles $I\times J$. (For instance, approximate each $E_{k,m}$ in measure by a finite union of rectangles; since we only need $L^2$-approximation on a finite measure space, this can be arranged so that the resulting error in $L^2$ is arbitrarily small.) Then we obtain kernels $K_m$ of the form
$$
K_m(x,y)=\sum_{j=1}^{r_m} a_{j,m}(x)\,b_{j,m}(y),
$$
where $a_{j,m},b_{j,m}\in L^2([0,1])$, and
$$
\|K-K_m\|_{L^2([0,1]^2)}\to 0.
$$
(Indeed, $\mathbf 1_{I\times J}(x,y)=\mathbf 1_I(x)\mathbf 1_J(y)$, so finite linear combinations of rectangle indicators are exactly finite sums of separable terms.)

### 2) The associated operators $T_m$ are finite-rank
Define $T_m:H\to H$ by
$$
(T_m f)(x)=\int_0^1 K_m(x,y)f(y)\,dy.
$$
Using the separable form,
$$
(T_m f)(x)=\int_0^1 \left(\sum_{j=1}^{r_m} a_{j,m}(x)b_{j,m}(y)\right) f(y)\,dy
=\sum_{j=1}^{r_m} a_{j,m}(x)\,\int_0^1 b_{j,m}(y)f(y)\,dy.
$$
Equivalently,
$$
T_m f=\sum_{j=1}^{r_m} \langle f,\overline{b_{j,m}}\rangle\, a_{j,m},
$$
so $\operatorname{Ran}(T_m)\subset \operatorname{span}\{a_{1,m},\dots,a_{r_m,m}\}$, which is finite-dimensional. Hence each $T_m$ is finite-rank (therefore compact).

### 3) Convergence in Hilbert–Schmidt norm
From Step 1, the map “kernel $\mapsto$ integral operator” identifies Hilbert–Schmidt operators with $L^2$-kernels, and the Hilbert–Schmidt norm satisfies
$$
\|T-T_m\|_{\mathrm{HS}}=\|K-K_m\|_{L^2([0,1]^2)}.
$$
Therefore,
$$
\|T-T_m\|_{\mathrm{HS}}\to 0.
$$

### 4) Convergence in operator norm and compactness of $T$
For any operator $S$ on a Hilbert space, $\|S\|_{\mathrm{op}}\le \|S\|_{\mathrm{HS}}$ (because $\|S\|_{\mathrm{HS}}^2=\sum_{n}\|Se_n\|^2$ for an ONB $\{e_n\}$, so $\|S\|_{\mathrm{HS}}\ge \|Se_1\|$ and taking supremum over unit vectors gives $\|S\|_{\mathrm{op}}\le \|S\|_{\mathrm{HS}}$). Hence
$$
\|T-T_m\|_{\mathrm{op}}\le \|T-T_m\|_{\mathrm{HS}}=\|K-K_m\|_2\to 0.
$$
So $T$ is a norm limit (in operator norm) of finite-rank operators $T_m$. Since the set of compact operators is closed in operator norm, it follows that $T$ is compact.

(Equivalently: a bounded operator is compact iff it is the operator-norm limit of finite-rank operators.)

This completes the compactness step: $T$ is compact because $K$ can be approximated in $L^2$ by finite-rank kernels, yielding finite-rank $T_m$ with $\|T-T_m\|_{\mathrm{op}}\to 0$.

### Step 3: Analyze A=T*T: show A is compact (product of compact and bounded), self-adjoint ((T*T)*=T* T), and positive (⟨Af,f⟩=∥Tf∥^2≥0). Note ker(A)=ker(T).

Let $H=L^2([0,1])$. We have already shown $T$ is bounded and compact. Consider
$$
A:=T^*T:H\to H.
$$

## Compactness of $A$
Since $T$ is compact and $T^*$ is bounded (adjoint of a bounded operator is bounded), the composition of a bounded operator with a compact operator is compact. Concretely:

- $T$ compact $\implies$ $T(B)$ is relatively compact for every bounded set $B\subset H$.
- Apply the bounded map $T^*$ to the relatively compact set $T(B)$: bounded linear maps send relatively compact sets to relatively compact sets.

Thus $A=T^*T$ is compact.

(Equivalently: “bounded $\circ$ compact” and “compact $\circ$ bounded” are compact, so $T^*T$ is compact.)

## Self-adjointness of $A$
Use $(ST)^* = T^*S^*$ for bounded operators $S,T$. Then
$$
A^* = (T^*T)^* = T^*(T^*)^* = T^*T = A,
$$
so $A$ is self-adjoint.

## Positivity of $A$
For any $f\in H$,
$$
\langle Af,f\rangle
= \langle T^*Tf,f\rangle
= \langle Tf,Tf\rangle
= \|Tf\|^2 \ge 0.
$$
Hence $A$ is positive (positive semidefinite).

## Kernel identity: $\ker(A)=\ker(T)$
First, if $f\in \ker(T)$, then $Tf=0$, so
$$
Af=T^*Tf=T^*0=0,
$$
hence $f\in\ker(A)$. Thus $\ker(T)\subseteq \ker(A)$.

Conversely, if $f\in\ker(A)$, then $Af=0$. Taking inner product with $f$,
$$
0=\langle Af,f\rangle=\|Tf\|^2,
$$
so $Tf=0$, i.e. $f\in\ker(T)$. Thus $\ker(A)\subseteq\ker(T)$.

Therefore,
$$
\ker(A)=\ker(T).
$$

So $A=T^*T$ is compact, self-adjoint, positive, and has the same kernel as $T$.

### Step 4: Invoke spectral theorem for compact self-adjoint A: obtain ONB of eigenvectors for ran(A)⊕ker(A), eigenvalues λ_n>0 with λ_n↓0, and expansion Af=∑ λ_n⟨f,v_n⟩v_n on (ker A)⊥. Define σ_n=√λ_n.

Since $A=T^*T$ is **compact**, **self-adjoint**, and **positive** on the Hilbert space $H=L^2([0,1])$ (Step 3), we may apply the spectral theorem for compact self-adjoint operators.

## Spectral theorem applied to $A$

By the compact self-adjoint spectral theorem, there exists an orthonormal set of eigenvectors $\{v_n\}_{n\ge 1}\subset H$ with corresponding eigenvalues $\lambda_n>0$ such that:

1. Each $v_n$ satisfies
$$
Av_n=\lambda_n v_n,\qquad \lambda_n>0,
$$
and the eigenvalues (listed with multiplicity) satisfy
$$
\lambda_1\ge \lambda_2\ge \cdots \downarrow 0.
$$

2. The space $H$ decomposes orthogonally as
$$
H=\overline{\operatorname{ran}(A)}\oplus \ker(A),
$$
and moreover
$$
\overline{\operatorname{ran}(A)}=(\ker A)^\perp.
$$

3. The eigenvectors $\{v_n\}$ form an orthonormal basis of $\overline{\operatorname{ran}(A)}=(\ker A)^\perp$. Equivalently, every $f\in (\ker A)^\perp$ has the expansion
$$
f=\sum_{n\ge 1}\langle f,v_n\rangle v_n
\quad\text{(convergence in }H\text{)}.
$$

4. On $(\ker A)^\perp$, the operator $A$ admits the diagonal expansion
$$
Af=\sum_{n\ge 1}\lambda_n \langle f,v_n\rangle v_n,
\qquad f\in (\ker A)^\perp,
$$
with convergence in $H$. On $\ker(A)$, we have $Af=0$.

(Here we are not claiming an ONB of all of $H$ from eigenvectors with $\lambda_n>0$ alone; rather, $\{v_n\}$ spans $(\ker A)^\perp$, and one can extend it by an ONB of $\ker A$ to get an ONB of $H$.)

## Define the singular values

For each nonzero eigenvalue $\lambda_n$, define
$$
\sigma_n:=\sqrt{\lambda_n}>0.
$$
Then $\sigma_n\downarrow 0$ as well, and $\lambda_n=\sigma_n^2$. This prepares the “SVD-like” construction in the next step.

### Step 5: (2a) Define u_n=σ_n^{-1}Tv_n. Prove orthonormality: ⟨u_n,u_m⟩=σ_n^{-1}σ_m^{-1}⟨Tv_n,Tv_m⟩=σ_n^{-1}σ_m^{-1}⟨Av_n,v_m⟩=δ_{nm}. Deduce Tv_n=σ_nu_n and T* u_n=σ_n v_n.

Fix $n$ with $\sigma_n=\sqrt{\lambda_n}>0$ and define
$$
u_n:=\sigma_n^{-1}Tv_n\in H.
$$
(This is well-defined because $\sigma_n>0$.)

## Orthonormality of $\{u_n\}$
For $m,n$ with $\sigma_m,\sigma_n>0$,
$$
\langle u_n,u_m\rangle
=\Big\langle \sigma_n^{-1}Tv_n,\ \sigma_m^{-1}Tv_m\Big\rangle
=\sigma_n^{-1}\sigma_m^{-1}\,\langle Tv_n,Tv_m\rangle.
$$
Use the defining property of the adjoint: $\langle Tf,g\rangle=\langle f,T^*g\rangle$. With $f=v_n$ and $g=Tv_m$,
$$
\langle Tv_n,Tv_m\rangle=\langle v_n, T^*Tv_m\rangle=\langle v_n, Av_m\rangle.
$$
Since $v_m$ is an eigenvector of $A$,
$$
Av_m=\lambda_m v_m=\sigma_m^2 v_m,
$$
hence
$$
\langle Tv_n,Tv_m\rangle=\langle v_n,\lambda_m v_m\rangle
=\lambda_m \langle v_n,v_m\rangle.
$$
Therefore
$$
\langle u_n,u_m\rangle
=\sigma_n^{-1}\sigma_m^{-1}\,\lambda_m \langle v_n,v_m\rangle
=\sigma_n^{-1}\sigma_m^{-1}\,\sigma_m^2\,\delta_{nm}
=\frac{\sigma_m}{\sigma_n}\,\delta_{nm}.
$$
If $n\neq m$, this is $0$. If $n=m$, it is $1$. Thus $\{u_n\}$ is an orthonormal set in $H$.

## The identities $Tv_n=\sigma_n u_n$ and $T^*u_n=\sigma_n v_n$
The first identity is immediate from the definition:
$$
u_n=\sigma_n^{-1}Tv_n \quad\Longrightarrow\quad Tv_n=\sigma_n u_n.
$$

For the second, apply $T^*$ to $u_n$:
$$
T^*u_n = T^*(\sigma_n^{-1}Tv_n)=\sigma_n^{-1}T^*Tv_n=\sigma_n^{-1}Av_n.
$$
Using $Av_n=\lambda_n v_n=\sigma_n^2 v_n$, we get
$$
T^*u_n=\sigma_n^{-1}\sigma_n^2 v_n=\sigma_n v_n.
$$

So for each $n$ with $\sigma_n>0$, we have
$$
Tv_n=\sigma_n u_n,\qquad T^*u_n=\sigma_n v_n,
$$
and $\{u_n\}$ is orthonormal.

### Step 6: (2b) Derive SVD-like expansion: for f∈(ker T)⊥=closure(span{v_n}), show Tf=∑ σ_n⟨f,v_n⟩u_n (convergence in H). Express as operator series T=∑ σ_n u_n⊗v_n on (ker T)⊥, and T=0 on ker(T). Similarly, T*g



### Step 7: Relate singular values to HS norm: compute ∥T∥_{HS}^2=tr(T*T)=tr(A)=∑ λ_n=∑ σ_n^2, and identify this with ∥K∥_2^2. Conclude {σ_n}∈ℓ^2 always (since K∈L^2).



### Step 8: (3i) HS convergence of T_N: use orthonormality of {u_n⊗v_n} in HS space to get ∥T−T_N∥_{HS}^2=∑_{n>N} σ_n^2. Hence HS convergence ⇔ ∑ σ_n^2<∞, which always holds here (Hilbert–Schmidt operator).

### Step 8: (3i) Hilbert–Schmidt convergence of $T_N$

Recall that $T$ is Hilbert–Schmidt because $K\in L^2([0,1]^2)$, hence $T\in \mathcal{S}_2(H)$ and
$$
\|T\|_{\mathrm{HS}}^2=\iint_{[0,1]^2} |K(x,y)|^2\,dx\,dy<\infty.
$$
From the SVD-like expansion obtained earlier, on $(\ker T)^\perp$ we have
$$
T=\sum_{n\ge 1}\sigma_n\, u_n\otimes v_n,
\qquad
T_N=\sum_{n=1}^N\sigma_n\, u_n\otimes v_n,
$$
so
$$
T-T_N=\sum_{n>N}\sigma_n\, u_n\otimes v_n.
$$

#### 1) Orthonormality of $\{u_n\otimes v_n\}$ in the Hilbert–Schmidt space
In the Hilbert–Schmidt class $\mathcal{S}_2(H)$, the inner product is
$$
\langle S,R\rangle_{\mathrm{HS}}:=\operatorname{tr}(R^*S).
$$
For rank-one operators $u\otimes v$ and $u'\otimes v'$, one checks
$$
\langle u\otimes v,\ u'\otimes v'\rangle_{\mathrm{HS}}
=\langle u,u'\rangle\,\langle v,v'\rangle.
$$
(Indeed, $(u\otimes v)^*=v\otimes u$, and $(u'\otimes v')^*(u\otimes v)=\langle u,u'\rangle\,(v'\otimes v)$, whose trace is $\langle u,u'\rangle\langle v,v'\rangle$.)

Since $\{u_n\}$ and $\{v_n\}$ are orthonormal sets in $H$, it follows that
$$
\langle u_n\otimes v_n,\ u_m\otimes v_m\rangle_{\mathrm{HS}}
=\langle u_n,u_m\rangle\,\langle v_n,v_m\rangle
=\delta_{nm}.
$$
Thus $\{u_n\otimes v_n\}$ is an orthonormal set in $\mathcal{S}_2(H)$.

#### 2) Compute the Hilbert–Schmidt error
Using orthonormality in $\mathcal{S}_2(H)$,
$$
\|T-T_N\|_{\mathrm{HS}}^2
=\left\|\sum_{n>N}\sigma_n\, u_n\otimes v_n\right\|_{\mathrm{HS}}^2
=\sum_{n>N}\sigma_n^2\,\|u_n\otimes v_n\|_{\mathrm{HS}}^2
=\sum_{n>N}\sigma_n^2,
$$
because $\|u_n\otimes v_n\|_{\mathrm{HS}}^2=\|u_n\|^2\|v_n\|^2=1$.

Therefore,
$$
\boxed{\ \|T-T_N\|_{\mathrm{HS}}^2=\sum_{n>N}\sigma_n^2\ }.
$$

#### 3) Characterize HS convergence
From the identity above,
$$
\|T-T_N\|_{\mathrm{HS}}\to 0
\quad\Longleftrightarrow\quad
\sum_{n=1}^\infty \sigma_n^2<\infty.
$$
But by Step 7,
$$
\sum_{n=1}^\infty \sigma_n^2=\sum_{n=1}^\infty \lambda_n=\operatorname{tr}(T^*T)=\|T\|_{\mathrm{HS}}^2=\|K\|_{L^2([0,1]^2)}^2<\infty.
$$
Hence $\{\sigma_n\}\in \ell^2$ automatically for our integral operator with $K\in L^2$, and so:

$$
\boxed{\ \|T-T_N\|_{\mathrm{HS}}\to 0\ \text{always holds in this setting.}\ }
$$

This completes (3i): Hilbert–Schmidt convergence is guaranteed, with error tail exactly $\sum_{n>N}\sigma_n^2$.

### Step 9: (3ii) Operator-norm convergence: show ∥T−T_N∥_{op}=sup_{∥f∥=1}∥∑_{n>N}σ_n⟨f,v_n⟩u_n∥ = sup_{∥c∥_{ℓ^2}=1}(∑_{n>N}σ_n^2|c_n|^2)^{1/2} = sup_{n>N} σ_n. Thus convergence ⇔ σ_n→0, which holds for compact T

### Step 9: (3ii) Operator-norm convergence of $T_N$

Work on the subspace $(\ker T)^\perp=\overline{\mathrm{span}}\{v_n\}$, since $T=0$ on $\ker T$ and also $T_N=0$ there (because $\langle f,v_n\rangle=0$ for all $n$ when $f\in\ker T$). Thus the operator norm of $T-T_N$ is determined by its action on $(\ker T)^\perp$.

From the SVD-like expansion (Step 6), for $f\in(\ker T)^\perp$,
$$
(T-T_N)f=\sum_{n>N}\sigma_n\langle f,v_n\rangle u_n.
$$
Since $\{u_n\}$ is orthonormal (Step 6/2a), we can compute the norm of this vector in $H$ via Parseval:
$$
\|(T-T_N)f\|^2
=\left\|\sum_{n>N}\sigma_n\langle f,v_n\rangle u_n\right\|^2
=\sum_{n>N}\sigma_n^2\,|\langle f,v_n\rangle|^2.
$$
Let $c_n:=\langle f,v_n\rangle$. Because $\{v_n\}$ is an orthonormal basis of $(\ker T)^\perp$, we have
$$
\|f\|^2=\sum_{n\ge 1}|c_n|^2.
$$
Hence, restricting to $\|f\|=1$ in $(\ker T)^\perp$ is the same as $\|c\|_{\ell^2}=1$. Therefore
$$
\|T-T_N\|_{\mathrm{op}}
=\sup_{\|f\|=1}\|(T-T_N)f\|
=\sup_{\|c\|_{\ell^2}=1}\left(\sum_{n>N}\sigma_n^2|c_n|^2\right)^{1/2}.
$$

Now estimate this supremum. For any $\|c\|_{\ell^2}=1$,
$$
\sum_{n>N}\sigma_n^2|c_n|^2
\le \left(\sup_{n>N}\sigma_n^2\right)\sum_{n>N}|c_n|^2
\le \left(\sup_{n>N}\sigma_n^2\right)\cdot 1,
$$
so
$$
\|T-T_N\|_{\mathrm{op}}\le \sup_{n>N}\sigma_n.
$$
Conversely, choose $c=e_m$ (the standard basis vector) for some $m>N$, i.e. $c_m=1$ and $c_n=0$ for $n\neq m$. This corresponds to $f=v_m$, which has $\|f\|=1$. Then
$$
\|(T-T_N)v_m\|
=\left\|\sum_{n>N}\sigma_n\langle v_m,v_n\rangle u_n\right\|
=\|\sigma_m u_m\|
=\sigma_m.
$$
Taking the supremum over $m>N$ yields
$$
\|T-T_N\|_{\mathrm{op}}\ge \sup_{m>N}\sigma_m.
$$
Combining both inequalities,
$$
\boxed{\ \|T-T_N\|_{\mathrm{op}}=\sup_{n>N}\sigma_n\ }.
$$

#### Convergence criterion
Thus,
$$
\|T-T_N\|_{\mathrm{op}}\to 0
\quad\Longleftrightarrow\quad
\sup_{n>N}\sigma_n\to 0
\quad\Longleftrightarrow\quad
\sigma_n\to 0.
$$
But $\sigma_n=\sqrt{\lambda_n}$ and $\lambda_n\downarrow 0$ for the compact self-adjoint operator $A=T^*T$. Hence $\sigma_n\to 0$, and therefore
$$
\boxed{\ \|T-T_N\|_{\mathrm{op}}\to 0\ \text{always holds for our }T\ (\text{since }T\text{ is compact}).\ }
$$

(Equivalently: for a general bounded operator, $\|T-T_N\|_{\mathrm{op}}\to 0$ holds iff its singular values $\sigma_n\to 0$, i.e. iff $T$ is compact. In our setting $K\in L^2$ makes $T$ Hilbert–Schmidt, hence compact, so the condition is automatically satisfied.)

### Step 10: (4) If K∈L^∞, then T is still HS (since L^∞⊂L^2 on finite measure) and compact; conclusions in (3) unchanged. Extra boundedness may improve crude bounds on ∥T∥_{op} but does not alter criteria: HS iff

### Step 10 (4): Effect of the stronger assumption $K\in L^\infty([0,1]^2)$

Assume now $K\in L^\infty([0,1]^2)$. Since $[0,1]^2$ has finite measure, we have the continuous embedding
$$
L^\infty([0,1]^2)\subset L^2([0,1]^2),
\qquad
\|K\|_{L^2}\le \|K\|_{L^\infty}\,|[0,1]^2|^{1/2}=\|K\|_{L^\infty}.
$$
Hence $K\in L^2$ still holds, so $T$ is still a Hilbert–Schmidt operator, therefore compact. In particular, all conclusions from parts (1)–(3) remain valid without any change in the *criteria* for convergence.

#### 1) What $K\in L^\infty$ gives additionally (a crude operator-norm bound)
For any $f\in L^2([0,1])$, we can estimate pointwise
$$
|(Tf)(x)|
=\left|\int_0^1 K(x,y)f(y)\,dy\right|
\le \|K\|_{L^\infty}\int_0^1 |f(y)|\,dy
= \|K\|_{L^\infty}\,\|f\|_{L^1}.
$$
Taking $L^2$-norm in $x$ and using $\|f\|_{L^1}\le \|f\|_{L^2}$ on $[0,1]$, we get
$$
\|Tf\|_{L^2}\le \|K\|_{L^\infty}\,\|f\|_{L^1}\le \|K\|_{L^\infty}\,\|f\|_{L^2},
$$
so
$$
\|T\|_{\mathrm{op}}\le \|K\|_{L^\infty}.
$$
This is an *extra* bound, but it does not affect the spectral/SVD convergence statements.

#### 2) Why the convergence conclusions in (3) do not change
From Step 7, for any $K\in L^2$ we already have
$$
\|T\|_{\mathrm{HS}}^2=\iint |K|^2=\sum_{n\ge 1}\sigma_n^2<\infty,
$$
so $\{\sigma_n\}\in \ell^2$ automatically. Therefore, as shown in Step 8,
$$
\|T-T_N\|_{\mathrm{HS}}^2=\sum_{n>N}\sigma_n^2\to 0,
$$
and Hilbert–Schmidt convergence always holds. The stronger assumption $K\in L^\infty$ does not strengthen this criterion; it was already satisfied.

For operator norm, Step 9 gave the exact identity
$$
\|T-T_N\|_{\mathrm{op}}=\sup_{n>N}\sigma_n,
$$
so
$$
\|T-T_N\|_{\mathrm{op}}\to 0
\quad\Longleftrightarrow\quad
\sigma_n\to 0.
$$
But $\sigma_n\to 0$ holds for every compact operator $T$, and $T$ is compact already from $K\in L^2$ (hence also from $K\in L^\infty$). Thus operator-norm convergence also still holds, for the same reason and with the same sharp criterion.

#### Conclusion for (4)
The assumption $K\in L^\infty$ does **not** change the conclusions of part (3):

- $\|T-T_N\|_{\mathrm{HS}}\to 0$ always (because $K\in L^2\Rightarrow \sum\sigma_n^2<\infty$).
- $\|T-T_N\|_{\mathrm{op}}\to 0$ also holds (because $T$ is compact $\Rightarrow \sigma_n\to 0$).

What $K\in L^\infty$ adds is only an additional bound like $\|T\|_{\mathrm{op}}\le \|K\|_{L^\infty}$, not a different convergence criterion for the SVD truncations.


---

> **Note:** This solution may contain errors or incomplete reasoning. The following issues were detected:
> - Solution is too short.
> - Solution lacks mathematical notation.
> - Solution may lack mathematical reasoning.
>
> Please verify the steps independently.",,0,Solution is too short.; Solution lacks mathematical notation.; Solution may lack mathematical reasoning.,3694,130617,True
14,linear_algebra,SVD,hard,"You observe $n$ data vectors $x_1,\dots,x_n\in\mathbb{R}^p$ generated by a noisy low-rank model
$$x_i = U s_i + \varepsilon_i,$$
where $U\in\mathbb{R}^{p\times r}$ has orthonormal columns ($U^\top U=I_r$), the latent scores satisfy $s_i\sim \mathcal{N}(0,\Lambda)$ with $\Lambda=\mathrm{diag}(\lambda_1,\dots,\lambda_r)$ and $\lambda_1\ge\cdots\ge\lambda_r>0$, and the noise vectors are i.i.d. $\varepsilon_i\sim\mathcal{N}(0,\sigma^2 I_p)$ independent of $s_i$. The rank $r$ and the spike strengths $\lambda_j$ are unknown.

Let $X=[x_1\ \cdots\ x_n]\in\mathbb{R}^{p\times n}$ be the data matrix. Consider the high-dimensional regime $p,n\to\infty$ with $p/n\to\gamma\in(0,\infty)$.

1) Define an estimator $\widehat U_k$ of the principal subspace using the truncated SVD of the centered data matrix (you may assume the mean is zero so no centering is needed). Specify clearly how $\widehat U_k$ is constructed from $X$, for a user-chosen truncation level $k$.

2) Using the spiked random matrix intuition (signal-plus-isotropic-noise), describe qualitatively how the top singular values of $X$ (or eigenvalues of $\frac1n XX^\top$) are biased by noise relative to the population spikes $\lambda_j+\sigma^2$. In your discussion, include:
   - what the “bulk” of noise-only singular values/eigenvalues looks like asymptotically as a function of $(\sigma^2,\gamma)$,
   - how a sufficiently strong spike separates from the bulk,
   - what happens to weaker spikes.

3) Let $\theta_j$ denote the principal angle between the true spike direction $u_j$ (the $j$th column of $U$) and its empirical counterpart $\widehat u_j$ from the truncated SVD (when defined). Explain asymptotically/qualitatively how $\cos^2\theta_j$ depends on the spike strength $\lambda_j$, the noise level $\sigma^2$, and the aspect ratio $\gamma$ (e.g., behavior below vs. above a detectability threshold).

4) Propose a data-driven rule to choose $k$ (an estimator of $r$) based on the singular values of $X$ in this regime, and justify qualitatively why it should work under the model above.

Your answer should be expressed in terms of asymptotic/random-matrix effects (e.g., bulk edge, spike separation, shrinkage/overestimation of singular values, and alignment of singular vectors), not by computing an explicit SVD for a concrete numeric matrix.","Ви спостерігаєте $n$ векторів даних $x_1,\dots,x_n\in\mathbb{R}^p$, згенерованих зашумленою низькоранговою моделлю
$$x_i = U s_i + \varepsilon_i,$$
де $U\in\mathbb{R}^{p\times r}$ має ортонормовані стовпці ($U^\top U=I_r$), латентні оцінки задовольняють $s_i\sim \mathcal{N}(0,\Lambda)$ з $\Lambda=\mathrm{diag}(\lambda_1,\dots,\lambda_r)$ та $\lambda_1\ge\cdots\ge\lambda_r>0$, а вектори шуму є i.i.d. $\varepsilon_i\sim\mathcal{N}(0,\sigma^2 I_p)$, незалежні від $s_i$. Ранг $r$ та сили «шипів» $\lambda_j$ є невідомими.

Нехай $X=[x_1\ \cdots\ x_n]\in\mathbb{R}^{p\times n}$ — матриця даних. Розгляньте високовимірний режим $p,n\to\infty$ з $p/n\to\gamma\in(0,\infty)$.

1) Визначте оцінювач $\widehat U_k$ головного підпростору, використовуючи усічений SVD центрованої матриці даних (можна вважати, що математичне сподівання дорівнює нулю, тож центрування не потрібне). Чітко вкажіть, як $\widehat U_k$ будується з $X$ для обраного користувачем рівня усічення $k$.

2) Використовуючи інтуїцію «шипуватих» випадкових матриць (сигнал плюс ізотропний шум), якісно опишіть, як найбільші сингулярні значення $X$ (або власні значення $\frac1n XX^\top$) зміщені шумом відносно популяційних «шипів» $\lambda_j+\sigma^2$. У вашому обговоренні включіть:
   - як асимптотично виглядає «масив» (bulk) сингулярних значень/власних значень лише від шуму як функція $(\sigma^2,\gamma)$,
   - як достатньо сильний «шип» відокремлюється від масиву,
   - що відбувається зі слабшими «шипами».

3) Нехай $\theta_j$ позначає головний кут між істинним напрямом «шипа» $u_j$ ( $j$-й стовпець $U$) та його емпіричним відповідником $\widehat u_j$ з усіченого SVD (коли він визначений). Поясніть асимптотично/якісно, як $\cos^2\theta_j$ залежить від сили «шипа» $\lambda_j$, рівня шуму $\sigma^2$ та відношення розмірностей $\gamma$ (наприклад, поведінку нижче та вище порога виявлюваності).

4) Запропонуйте кероване даними правило вибору $k$ (оцінювач $r$) на основі сингулярних значень $X$ у цьому режимі та якісно обґрунтуйте, чому воно має працювати за наведеної вище моделі.

Ваша відповідь має бути сформульована в термінах асимптотичних/випадково-матричних ефектів (наприклад, край масиву, відокремлення «шипа», стискання/переоцінювання сингулярних значень та узгодження сингулярних векторів), а не шляхом обчислення явного SVD для конкретної числової матриці.","## 1) Truncated-SVD estimator of the principal subspace

Stack the observations as columns of the data matrix
$$
X=[x_1\ \cdots\ x_n]\in\mathbb{R}^{p\times n}.
$$
Under the model $x_i=Us_i+\varepsilon_i$, we can write
$$
X = US + E,
$$
with $S=[s_1\ \cdots\ s_n]\in\mathbb{R}^{r\times n}$ and $E=[\varepsilon_1\ \cdots\ \varepsilon_n]\in\mathbb{R}^{p\times n}$.

Compute the (full) SVD of $X$:
$$
X=\widehat U\,\widehat D\,\widehat V^\top,
$$
where $m=\min(p,n)$, $\widehat U=[\widehat u_1,\dots,\widehat u_m]\in\mathbb{R}^{p\times m}$, $\widehat V\in\mathbb{R}^{n\times m}$, and
$$
\widehat D=\mathrm{diag}(\widehat d_1,\dots,\widehat d_m),\qquad \widehat d_1\ge \widehat d_2\ge \cdots\ge \widehat d_m\ge 0.
$$

For a user-chosen truncation level $k\in\{1,\dots,m\}$, define the PCA/truncated-SVD subspace estimator
$$
\boxed{\ \widehat U_k := [\widehat u_1\ \widehat u_2\ \cdots\ \widehat u_k]\in\mathbb{R}^{p\times k},\qquad \widehat{\mathcal U}_k:=\mathrm{span}(\widehat U_k)\ }.
$$
Equivalently, $\widehat u_j$ is the $j$th leading eigenvector of the sample covariance
$$
\widehat\Sigma:=\frac1n XX^\top=\widehat U\left(\frac{\widehat D^2}{n}\right)\widehat U^\top,
$$
with sample eigenvalues
$$
\widehat \ell_j=\frac{\widehat d_j^2}{n}.
$$

---

## 2) How noise biases the top singular values/eigenvalues (bulk, separation, weak spikes)

### Population vs. sample eigenvalues
The population covariance is
$$
\Sigma=\mathbb E[x_i x_i^\top]=U\Lambda U^\top+\sigma^2 I_p,
$$
so its eigenvalues are
$$
\ell^{\mathrm{pop}}_j=\sigma^2+\lambda_j\quad (j=1,\dots,r),\qquad \ell^{\mathrm{pop}}=\sigma^2\ \text{(multiplicity }p-r\text{)}.
$$
In high dimensions, the sample eigenvalues $\widehat \ell_j$ are **not** close to $\ell^{\mathrm{pop}}_j$; they are distorted by random-matrix effects.

### Noise-only “bulk” (Marchenko–Pastur)
If $U=0$ (pure noise), then $\widehat\Sigma=\frac1n EE^\top$ is Wishart. As $p,n\to\infty$ with $p/n\to\gamma\in(0,\infty)$, its eigenvalues concentrate in the Marchenko–Pastur (MP) bulk:
$$
\boxed{\ \mathrm{spec}(\widehat\Sigma)\subseteq \big[\sigma^2(1-\sqrt\gamma)^2,\ \sigma^2(1+\sqrt\gamma)^2\big]\ \text{(asymptotically)}\ }.
$$
In particular, the **upper bulk edge** is
$$
\boxed{\ b=\sigma^2(1+\sqrt\gamma)^2\ }.
$$
Equivalently, the largest noise singular value satisfies $\widehat d_1/\sqrt n\to \sigma(1+\sqrt\gamma)$.

### Spike separation (BBP transition) and bias
With spikes present, a key phenomenon is the BBP (detectability) transition. Define the spike SNR
$$
\alpha_j:=\frac{\lambda_j}{\sigma^2}.
$$
- **Strong spike (supercritical):** if
  $$
\boxed{\ \alpha_j>\sqrt\gamma\quad\Longleftrightarrow\quad \lambda_j>\sigma^2\sqrt\gamma\ },
$$
  then the spike generates an **outlier** sample eigenvalue $\widehat \ell_j$ that separates above $b$. Moreover, this outlier is typically **upward biased** relative to the population spike $\sigma^2+\lambda_j$. A standard asymptotic location is
  $$
\widehat \ell_j \ \to\ (\sigma^2+\lambda_j)\left(1+\frac{\gamma\sigma^2}{\lambda_j}\right),
$$
  which exceeds $b$ exactly in the supercritical regime.

- **Weak spike (subcritical/critical):** if $\alpha_j\le \sqrt\gamma$, then there is **no stable outlier**; the corresponding sample eigenvalue is absorbed into the top of the MP bulk and is asymptotically indistinguishable from noise:
  $$
\widehat \ell_j \approx b\quad \text{(up to edge fluctuations)}.
$$

So, noise creates (i) a **bulk** spread even when the population noise eigenvalue is exactly $\sigma^2$, and (ii) an **inflation** of any separated spike eigenvalue beyond $\sigma^2+\lambda_j$.

---

## 3) Asymptotic behavior of eigenvector alignment via principal angles

Let $u_j$ be the $j$th true spike direction (column of $U$) and $\widehat u_j$ the corresponding empirical eigenvector/left singular vector (when meaningful). Define the principal angle $\theta_j\in[0,\pi/2]$ by
$$
\cos\theta_j:=|u_j^\top \widehat u_j|,\qquad \cos^2\theta_j=|u_j^\top \widehat u_j|^2.
$$

Random matrix theory predicts a **phase transition** matching the eigenvalue separation:

- **Below threshold ($\lambda_j/\sigma^2\le \sqrt\gamma$):**
  $$
\boxed{\ \cos^2\theta_j \ \to\ 0\ },
$$
  i.e., $\widehat u_j$ becomes asymptotically orthogonal to $u_j$ and carries essentially no information about the true direction.

- **Above threshold ($\lambda_j/\sigma^2> \sqrt\gamma$):**
  $$
\boxed{\ \cos^2\theta_j \ \to\ c(\alpha_j,\gamma)\in(0,1)\ },
$$
  where $c(\alpha,\gamma)$ increases with spike strength $\alpha=\lambda/\sigma^2$, decreases as $\gamma$ increases, and tends to $1$ as $\alpha\to\infty$. A commonly cited expression is
  $$
c(\alpha,\gamma)=\frac{1-\gamma/\alpha^2}{1+\gamma/\alpha},\qquad \alpha>\sqrt\gamma,
$$
  and $c(\alpha,\gamma)=0$ for $\alpha\le \sqrt\gamma$.

Thus, only spikes that separate from the bulk yield empirical singular vectors that are nontrivially aligned with the true subspace.

---

## 4) A data-driven rule to choose $k$ (estimate $r$) from singular values

A principled choice in this regime is to estimate how many eigenvalues of $\widehat\Sigma=\frac1nXX^\top$ lie **above the MP upper edge**.

1. Compute the eigenvalues $\widehat\ell_1\ge\cdots\ge \widehat\ell_p$ of $\widehat\Sigma$ (equivalently $\widehat\ell_j=\widehat d_j^2/n$).

2. Estimate $\sigma^2$ from the bulk (so outliers don’t dominate), e.g. by matching a robust bulk statistic to the MP law:
   $$
\widehat\sigma^2 := \frac{\mathrm{median}\{\widehat\ell_j\}}{m_\gamma},
$$
   where $m_\gamma$ is the median of the MP($\gamma$, scale $1$) distribution (known given $\gamma$). Any comparable MP-fit-to-bulk method is acceptable.

3. Form the estimated bulk edge
   $$
\widehat b := \widehat\sigma^2(1+\sqrt\gamma)^2.
$$

4. Choose
   $$
\boxed{\ \widehat k := \#\{j:\widehat\ell_j>\widehat b\}\ } 
   \qquad\text{(optionally use }\widehat\ell_j>\widehat b(1+\delta_n)\text{ to be conservative).}
$$

**Why it works (qualitatively):** asymptotically, noise eigenvalues (and subcritical spikes) remain within the MP bulk below $b$, while only supercritical spikes ($\lambda_j>\sigma^2\sqrt\gamma$) generate stable outliers above $b$. Therefore, counting eigenvalues above the estimated bulk edge selects exactly the **detectable** spikes—precisely those whose singular vectors have nontrivial alignment with the true subspace.

---

### Final takeaway
- $\widehat U_k$ is obtained by taking the top $k$ left singular vectors of $X$.
- The noise creates an MP bulk with upper edge $b=\sigma^2(1+\sqrt\gamma)^2$, inflating top empirical eigenvalues even under pure noise.
- Spikes separate (and eigenvectors align) only when $\lambda_j/\sigma^2>\sqrt\gamma$; otherwise eigenvalues/eigenvectors are indistinguishable from noise.
- A practical $\widehat k$ is the number of sample eigenvalues above an estimated MP bulk edge.

---

> **Note:** This solution may contain errors or incomplete reasoning. The following issues were detected:
> - Solution lacks mathematical notation.
>
> Please verify the steps independently.",,0,Solution lacks mathematical notation.,3961,119711,True
15,linear_algebra,SVD,hard,"You observe $n$ centered samples $x_1,\dots,x_n\in\mathbb{R}^p$ from the model
$$x_i = s_i + \varepsilon_i,$$
where the signal vectors $s_i$ lie (approximately) in a $k$-dimensional subspace ($k\ll p$), and the noise is anisotropic with known positive definite covariance $\Sigma\in\mathbb{R}^{p\times p}$:
$$\mathbb{E}[\varepsilon_i]=0,\qquad \mathbb{E}[\varepsilon_i\varepsilon_i^T]=\Sigma,$$
and $\varepsilon_i$ is independent across $i$. Let the data matrix be $X=[x_1\;\cdots\;x_n]\in\mathbb{R}^{p\times n}$.

Unlike standard PCA, you want a rank-$k$ linear dimension reduction that is optimal under the covariance-weighted inner product
$$\langle u,v\rangle_{\Sigma^{-1}} := u^T\Sigma^{-1}v,\qquad \|u\|_{\Sigma^{-1}}^2=u^T\Sigma^{-1}u.$$

**(a)** Define a linear encoder–decoder pair $(W,A)$ with $W\in\mathbb{R}^{k\times p}$ and $A\in\mathbb{R}^{p\times k}$ producing reconstructions $\hat x_i = AWx_i$. Formulate the optimization problem that chooses $(W,A)$ to minimize the total *weighted* reconstruction error
$$\sum_{i=1}^n \|x_i-\hat x_i\|_{\Sigma^{-1}}^2$$
subject to a constraint that removes the scaling ambiguity between $A$ and $W$ and makes the reduced coordinates comparable (state a concrete constraint, e.g. an orthonormality condition in an appropriate inner product).

**(b)** Let $\Sigma=LL^T$ be any matrix square root with $L$ invertible (e.g. Cholesky). Introduce whitened data $Y:=L^{-1}X$. Show how your weighted objective from (a) can be rewritten as an ordinary Frobenius-norm low-rank approximation problem in the whitened space. Specify explicitly how $(W,A)$ in the original space correspond to an encoder–decoder pair in the whitened space.

**(c)** Suppose you compute the (ordinary) SVD of the whitened data,
$$Y = U\,\Gamma\,V^T,$$
with singular values in nonincreasing order. Using this SVD, write down the theoretically optimal rank-$k$ reconstruction rule for $Y$ (as a function of $U,\Gamma,V$), and then map it back to give the optimal reconstruction rule $\hat X$ for the original data. Express $\hat X$ as a rank-$k$ linear operator applied to $X$.

**(d)** Define the generalized eigenvalue problem associated with the pair $(XX^T,\Sigma)$ and explain how the optimal $k$-dimensional subspace from (c) can be characterized directly in the original coordinates using generalized eigenvectors. State precisely which matrix’s top-$k$ eigenvectors (in what sense) determine the subspace.

**(e)** Give a concrete counterexample (specify $p,n,X,\Sigma$ with small sizes such as $p=2$ or $3$) where performing standard PCA on $X$ (i.e. top left singular vectors of $X$ under the Euclidean inner product) produces a different $k=1$ subspace than the covariance-weighted optimum from (c). Your example must have $\Sigma\neq cI$ and should make clear why standard SVD fails without whitening.

**(f)** If $\Sigma$ is unknown and replaced by an estimate $\hat\Sigma$, state a condition (in words or symbols) under which the whitening-based procedure remains well-posed and numerically stable, and indicate how you would modify the procedure if $\hat\Sigma$ is ill-conditioned or singular (no need to propose a full algorithm, but be specific about the linear-algebraic fix).","Ви спостерігаєте $n$ центрованих вибірок $x_1,\dots,x_n\in\mathbb{R}^p$ з моделі
$$x_i = s_i + \varepsilon_i,$$
де вектори сигналу $s_i$ лежать (приблизно) у $k$-вимірному підпросторі ($k\ll p$), а шум є анізотропним із відомою додатно визначеною коваріацією $\Sigma\in\mathbb{R}^{p\times p}$:
$$\mathbb{E}[\varepsilon_i]=0,\qquad \mathbb{E}[\varepsilon_i\varepsilon_i^T]=\Sigma,$$
і $\varepsilon_i$ є незалежним за $i$. Нехай матриця даних $X=[x_1\;\cdots\;x_n]\in\mathbb{R}^{p\times n}$.

На відміну від стандартного PCA, ви хочете лінійне зменшення розмірності рангу $k$, яке є оптимальним відносно скалярного добутку, зваженого коваріацією,
$$\langle u,v\rangle_{\Sigma^{-1}} := u^T\Sigma^{-1}v,\qquad \|u\|_{\Sigma^{-1}}^2=u^T\Sigma^{-1}u.$$

**(a)** Визначте лінійну пару кодувальник–декодувальник $(W,A)$ з $W\in\mathbb{R}^{k\times p}$ та $A\in\mathbb{R}^{p\times k}$, що породжує реконструкції $\hat x_i = AWx_i$. Сформулюйте задачу оптимізації, яка вибирає $(W,A)$ для мінімізації сумарної *зваженої* похибки реконструкції
$$\sum_{i=1}^n \|x_i-\hat x_i\|_{\Sigma^{-1}}^2$$
за умови обмеження, яке усуває неоднозначність масштабування між $A$ та $W$ і робить зменшені координати порівнюваними (вкажіть конкретне обмеження, наприклад умову ортонормованості у відповідному скалярному добутку).

**(b)** Нехай $\Sigma=LL^T$ — будь-який квадратний корінь матриці з оборотною $L$ (наприклад, розклад Холєцького). Введіть вибілені дані $Y:=L^{-1}X$. Покажіть, як вашу зважену цільову функцію з (a) можна переписати як звичайну задачу низькорангового наближення у нормі Фробеніуса у вибіленому просторі. Явно вкажіть, як $(W,A)$ в початковому просторі відповідають парі кодувальник–декодувальник у вибіленому просторі.

**(c)** Припустімо, що ви обчислюєте (звичайний) SVD вибілених даних,
$$Y = U\,\Gamma\,V^T,$$
із сингулярними значеннями у неспадному порядку. Використовуючи цей SVD, запишіть теоретично оптимальне правило рангу $k$ для реконструкції $Y$ (як функцію від $U,\Gamma,V$), а потім відобразіть його назад, щоб отримати оптимальне правило реконструкції $\hat X$ для початкових даних. Виразіть $\hat X$ як лінійний оператор рангу $k$, застосований до $X$.

**(d)** Визначте задачу узагальнених власних значень, пов’язану з парою $(XX^T,\Sigma)$, і поясніть, як оптимальний $k$-вимірний підпростір з (c) можна охарактеризувати безпосередньо в початкових координатах за допомогою узагальнених власних векторів. Точно вкажіть, власні вектори якої матриці (і в якому сенсі) визначають підпростір.

**(e)** Наведіть конкретний контрприклад (задайте $p,n,X,\Sigma$ малих розмірів, наприклад $p=2$ або $3$), де виконання стандартного PCA на $X$ (тобто провідні ліві сингулярні вектори $X$ відносно евклідового скалярного добутку) дає інший підпростір для $k=1$, ніж коваріаційно-зважений оптимум з (c). Ваш приклад має мати $\Sigma\neq cI$ і повинен чітко показувати, чому стандартний SVD не працює без вибілювання.

**(f)** Якщо $\Sigma$ невідома і замінюється оцінкою $\hat\Sigma$, сформулюйте умову (словами або символами), за якої процедура на основі вибілювання залишається коректно поставленою та чисельно стійкою, і вкажіть, як ви модифікували б процедуру, якщо $\hat\Sigma$ є погано обумовленою або виродженою (не потрібно пропонувати повний алгоритм, але будьте конкретними щодо лінійно-алгебраїчного виправлення).","## (a) Weighted linear autoencoder formulation

Let an encoder–decoder pair be
$$
W\in\mathbb{R}^{k\times p},\qquad A\in\mathbb{R}^{p\times k},\qquad \hat x_i = AWx_i.
$$
Stacking samples into $X=[x_1\ \cdots\ x_n]\in\mathbb{R}^{p\times n}$, the reconstruction is $\hat X=AWX$. The total covariance-weighted reconstruction error is
$$
\sum_{i=1}^n \|x_i-\hat x_i\|_{\Sigma^{-1}}^2
=\operatorname{tr}\!\Big((X-AWX)^T\Sigma^{-1}(X-AWX)\Big)
=\big\|\Sigma^{-1/2}(X-AWX)\big\|_F^2.
$$

To remove the scaling ambiguity $AW=(AR)(R^{-1}W)$, impose a concrete identifiability constraint adapted to the $\Sigma^{-1}$-geometry, e.g. **$\Sigma$-orthonormal rows of $W$**:
$$
W\Sigma W^T = I_k.
$$
(Equivalently one may use $A^T\Sigma^{-1}A=I_k$.)

A valid optimization problem is therefore
$$
\boxed{
\min_{A\in\mathbb{R}^{p\times k},\,W\in\mathbb{R}^{k\times p}}
\ \operatorname{tr}\!\Big((X-AWX)^T\Sigma^{-1}(X-AWX)\Big)
\quad \text{s.t.}\quad W\Sigma W^T=I_k.
}
$$

---

## (b) Whitening reduces the weighted objective to an ordinary Frobenius problem

Let $\Sigma=LL^T$ with $L$ invertible, and define the whitened data
$$
Y:=L^{-1}X\in\mathbb{R}^{p\times n}.
$$
Using $\Sigma^{-1}=L^{-T}L^{-1}$,
$$
\operatorname{tr}\!\Big((X-AWX)^T\Sigma^{-1}(X-AWX)\Big)
=\big\|L^{-1}(X-AWX)\big\|_F^2
=\big\|Y-(L^{-1}AWL)Y\big\|_F^2.
$$

Define the corresponding encoder–decoder in whitened coordinates:
$$
\boxed{\ \widetilde W := WL,\qquad \widetilde A := L^{-1}A\ } \quad\Rightarrow\quad
\widetilde A\widetilde W = L^{-1}AWL.
$$
Then the problem becomes the standard Euclidean reconstruction problem
$$
\boxed{
\min_{\widetilde A,\widetilde W}\ \|Y-\widetilde A\widetilde W Y\|_F^2.
}
$$
Moreover, the constraint $W\Sigma W^T=I_k$ becomes
$$
WLL^T W^T = (WL)(WL)^T \iff \boxed{\ \widetilde W\widetilde W^T=I_k\ },
$$
i.e. orthonormal rows in the usual Euclidean inner product.

---

## (c) Optimal rank-$k$ reconstruction via SVD and mapping back

Compute the SVD of the whitened data:
$$
Y = U\,\Gamma\,V^T,
$$
with singular values in nonincreasing order, and let $U_k,\Gamma_k,V_k$ denote the top-$k$ blocks.

By Eckart–Young, the optimal rank-$k$ approximation (Frobenius norm) is
$$
\boxed{\ \widehat Y = U_k\Gamma_k V_k^T = U_kU_k^T Y\ }.
$$
Mapping back to the original coordinates ($X=LY$) gives
$$
\widehat X = L\widehat Y = L U_kU_k^T L^{-1}X.
$$
Thus the optimal reconstruction is a rank-$k$ linear operator applied to $X$:
$$
\boxed{\ \widehat X = P_\Sigma X,\qquad P_\Sigma := L U_kU_k^T L^{-1}\ }.
$$

One optimal encoder–decoder pair achieving this is obtained from $\widetilde W=U_k^T,\ \widetilde A=U_k$, namely
$$
\boxed{\ W = U_k^T L^{-1},\qquad A = L U_k,\qquad AW = L U_kU_k^T L^{-1}=P_\Sigma. \ }
$$

---

## (d) Characterization via a generalized eigenvalue problem in original coordinates

Note
$$
YY^T = L^{-1}XX^T L^{-T}.
$$
The columns of $U_k$ are the top eigenvectors of $YY^T$. Let $u$ satisfy
$$
(L^{-1}XX^T L^{-T})u = \lambda u.
$$
Define $q:=L^{-T}u$. Multiplying by $L$ and using $\Sigma=LL^T$ yields
$$
XX^T q = \lambda\,\Sigma q.
$$
So the relevant generalized eigenproblem is
$$
\boxed{\ XX^T q = \lambda\,\Sigma q. \ }
$$
The optimal $k$-dimensional subspace in original coordinates is
$$
\boxed{\ \mathcal S_k = \operatorname{span}(q_1,\dots,q_k)\ },
$$
where $q_1,\dots,q_k$ are the generalized eigenvectors corresponding to the **largest** generalized eigenvalues $\lambda_1\ge\cdots\ge\lambda_k$.

Equivalently (symmetric form), the same subspace is obtained from the top eigenvectors of
$$
\boxed{\ M := \Sigma^{-1/2}XX^T\Sigma^{-1/2} \ }
$$
and then mapping back via $q_j=\Sigma^{-1/2}z_j$.

---

## (e) Concrete counterexample where standard PCA differs from the weighted optimum

Take $p=2,\ n=2,\ k=1$ and
$$
X = I_2=\begin{pmatrix}1&0\\0&1\end{pmatrix},\qquad
\Sigma=\begin{pmatrix}100&0\\0&1\end{pmatrix}\neq cI.
$$

**Standard PCA (Euclidean):** $XX^T=I_2$ has a repeated top eigenvalue, so the “first PC” is not unique; a typical choice is $e_1$, giving subspace $\operatorname{span}(e_1)$.

**Weighted optimum (whitening):** take $L=\Sigma^{1/2}=\operatorname{diag}(10,1)$, so
$$
Y=L^{-1}X=\operatorname{diag}(0.1,1),\qquad
YY^T=\operatorname{diag}(0.01,1).
$$
The top eigenvector is uniquely $e_2$, hence the optimal rank-1 whitened reconstruction projects onto $\operatorname{span}(e_2)$, and mapping back preserves this direction. Therefore the covariance-weighted optimal 1D subspace is $\operatorname{span}(e_2)$, which can differ from what standard PCA returns on $X$.

This illustrates why Euclidean SVD on $X$ can be inappropriate when $\Sigma$ is anisotropic: the noisy direction (here coordinate 1) should be downweighted via whitening.

---

## (f) Using an estimated $\hat\Sigma$: well-posedness and fixes for ill-conditioning

The whitening-based procedure is well-posed and numerically stable if $\hat\Sigma$ is symmetric positive definite and not too ill-conditioned, e.g.
$$
\hat\Sigma=\hat\Sigma^T\succ 0,\qquad \lambda_{\min}(\hat\Sigma)\ge \varepsilon>0,\qquad
\kappa(\hat\Sigma)=\frac{\lambda_{\max}(\hat\Sigma)}{\lambda_{\min}(\hat\Sigma)} \ \text{moderate}.
$$
Then $\hat\Sigma^{-1/2}$ exists and does not excessively amplify noise.

If $\hat\Sigma$ is ill-conditioned or singular, replace whitening by a stabilized inverse square root, for example:

* **Ridge/shrinkage whitening:**
  $$
\hat\Sigma_\alpha := \hat\Sigma+\alpha I\quad (\alpha>0),\qquad
  Y_\alpha := \hat\Sigma_\alpha^{-1/2}X,
$$
  then do SVD/PCA on $Y_\alpha$ and map back as in (c).

* **Truncated/pseudoinverse whitening (eigendecomposition $\hat\Sigma=Q\Lambda Q^T$):**
  $$
\hat\Sigma^{\dagger/2} := Q\,\mathrm{diag}\!\left(\hat\lambda_j^{-1/2}\mathbf{1}\{\hat\lambda_j\ge\tau\}\right)Q^T,
$$
  avoiding division by tiny/zero eigenvalues.

Either modification ensures the linear-algebraic steps remain stable.

---

> **Note:** This solution may contain errors or incomplete reasoning. The following issues were detected:
> - Solution lacks mathematical notation.
>
> Please verify the steps independently.",,0,Solution lacks mathematical notation.,4569,127168,True
16,calculus,Deep Learning Inspired,medium,"Let $z\in\mathbb{R}^n$ and define the softmax map $s:\mathbb{R}^n\to\mathbb{R}^n$ by
$$s_i(z)=\frac{e^{z_i}}{\sum_{j=1}^n e^{z_j}},\qquad i=1,\dots,n.$$
Answer the following.

1) (Jacobian entries) Compute the partial derivatives $\frac{\partial s_i}{\partial z_k}(z)$ for all $i,k\in\{1,\dots,n\}$ and assemble them into the Jacobian matrix $J(z)=\frac{\partial s}{\partial z}(z)\in\mathbb{R}^{n\times n}$.

2) (Matrix form) Rewrite your result from part (1) in a compact linear-algebraic form involving $s(z)$, the identity matrix $I$, and the diagonal matrix $\operatorname{Diag}(s(z))$.

3) (Simplex constraint \u2192 rank deficiency) Prove that $\sum_{i=1}^n s_i(z)=1$ for all $z$, and use this to show that $J(z)$ has a nontrivial vector in its left nullspace. Conclude that $\operatorname{rank}(J(z))\le n-1$ for all $z$.

4) (Optional strengthening) Determine for which $z$ (if any) the rank is strictly less than $n-1$, and justify your claim.","Нехай $z\in\mathbb{R}^n$ і визначимо відображення softmax $s:\mathbb{R}^n\to\mathbb{R}^n$ за формулою
$$s_i(z)=\frac{e^{z_i}}{\sum_{j=1}^n e^{z_j}},\qquad i=1,\dots,n.$$
Дайте відповіді на наведені нижче запитання.

1) (Елементи якобіана) Обчисліть частинні похідні $\frac{\partial s_i}{\partial z_k}(z)$ для всіх $i,k\in\{1,\dots,n\}$ та зберіть їх у матрицю Якобі $J(z)=\frac{\partial s}{\partial z}(z)\in\mathbb{R}^{n\times n}$.

2) (Матрична форма) Перепишіть ваш результат із пункту (1) у компактній лінійно-алгебраїчній формі, що містить $s(z)$, одиничну матрицю $I$ та діагональну матрицю $\operatorname{Diag}(s(z))$.

3) (Обмеження симплекса $\u2192$ дефіцит рангу) Доведіть, що $\sum_{i=1}^n s_i(z)=1$ для всіх $z$, і використайте це, щоб показати, що $J(z)$ має нетривіальний вектор у своєму лівому нульовому просторі. Зробіть висновок, що $\operatorname{rank}(J(z))\le n-1$ для всіх $z$.

4) (Необов’язкове посилення) Визначте, для яких $z$ (якщо такі існують) ранг є строго меншим за $n-1$, і обґрунтуйте вашу відповідь.","## 1) Jacobian entries

Let
$$
S(z):=\sum_{j=1}^n e^{z_j},\qquad s_i(z)=\frac{e^{z_i}}{S(z)}.
$$
Then
$$
\frac{\partial}{\partial z_k}e^{z_i}=\delta_{ik}e^{z_i},
\qquad
\frac{\partial S}{\partial z_k}=e^{z_k},
$$
and
$$
\frac{\partial}{\partial z_k}\big(S^{-1}\big)=-S^{-2}\frac{\partial S}{\partial z_k}=-S^{-2}e^{z_k}.
$$
Differentiate $s_i=e^{z_i}S^{-1}$:
$$
\frac{\partial s_i}{\partial z_k}
=\delta_{ik}e^{z_i}S^{-1}+e^{z_i}\left(-S^{-2}e^{z_k}\right)
=\delta_{ik}\frac{e^{z_i}}{S}-\frac{e^{z_i}e^{z_k}}{S^2}.
$$
Since $s_i=\frac{e^{z_i}}{S}$ and $s_k=\frac{e^{z_k}}{S}$, this becomes
$$
\boxed{\;\frac{\partial s_i}{\partial z_k}(z)=\delta_{ik}\,s_i(z)-s_i(z)s_k(z)
=s_i(z)\big(\delta_{ik}-s_k(z)\big).\;}
$$

Equivalently, in cases:
$$
\frac{\partial s_i}{\partial z_k}(z)=
\begin{cases}
s_i(z)\big(1-s_i(z)\big), & k=i,\$$
4pt]
-\,s_i(z)s_k(z), & k\neq i.
\end{cases}
$$
Thus the Jacobian $J(z)\in\mathbb{R}^{n\times n}$ has entries
$$
\boxed{\;J_{ik}(z)=\frac{\partial s_i}{\partial z_k}(z)=s_i(z)\big(\delta_{ik}-s_k(z)\big).\;}
$$

---

## 2) Matrix form

Let $s:=s(z)\in\mathbb{R}^n$. Note that
$$
(\operatorname{Diag}(s))_{ik}=s_i\delta_{ik},
\qquad
(ss^\top)_{ik}=s_is_k.
$$
Therefore,
$$
J_{ik}=s_i\delta_{ik}-s_is_k
=\big(\operatorname{Diag}(s)-ss^\top\big)_{ik},
$$
so
$$
\boxed{\;J(z)=\operatorname{Diag}(s(z)) - s(z)s(z)^\top.\;}
$$
A useful equivalent factorization is
$$
\boxed{\;J(z)=\operatorname{Diag}(s(z))\big(I-\mathbf{1}s(z)^\top\big),\;}
$$
since $\operatorname{Diag}(s)\mathbf{1}=s$.

---

## 3) Simplex constraint $\Rightarrow$ rank deficiency

First,
$$
\sum_{i=1}^n s_i(z)=\sum_{i=1}^n \frac{e^{z_i}}{S(z)}
=\frac{\sum_{i=1}^n e^{z_i}}{S(z)}
=\frac{S(z)}{S(z)}=1
\qquad\forall z\in\mathbb{R}^n.
$$
In vector form this is
$$
\mathbf{1}^\top s(z)=1 \qquad \forall z.
$$
Differentiate both sides with respect to $z$. The right-hand side is constant, hence has derivative $0$, while the left-hand side gives
$$
\frac{\partial}{\partial z}\big(\mathbf{1}^\top s(z)\big)=\mathbf{1}^\top J(z).
$$
Thus
$$
\boxed{\;\mathbf{1}^\top J(z)=\mathbf{0}^\top,\;}
$$
so $\mathbf{1}\neq 0$ lies in the **left nullspace** of $J(z)$. Consequently,
$$
\dim\ker(J(z)^\top)\ge 1
\quad\Longrightarrow\quad
\operatorname{rank}(J(z))=\operatorname{rank}(J(z)^\top)\le n-1.
$$
Hence
$$
\boxed{\;\operatorname{rank}(J(z))\le n-1\quad \forall z\in\mathbb{R}^n.\;}
$$

---

## 4) Optional strengthening: when is $\operatorname{rank}(J(z))<n-1$?

For any finite $z\in\mathbb{R}^n$, softmax satisfies $s_i(z)>0$ for all $i$. Using
$$
J=\operatorname{Diag}(s)-ss^\top,
$$
suppose $Jv=0$. Then
$$
(\operatorname{Diag}(s)-ss^\top)v=0
\quad\Longleftrightarrow\quad
\operatorname{Diag}(s)v=s(s^\top v).
$$
Let $\alpha:=s^\top v$. Componentwise,
$$
s_i v_i=\alpha s_i \quad (i=1,\dots,n).
$$
Since $s_i>0$, we can divide to obtain $v_i=\alpha$ for all $i$, i.e.
$$
v=\alpha \mathbf{1}.
$$
Thus, for finite $z$,
$$
\ker(J(z))=\operatorname{span}\{\mathbf{1}\}
\quad\Longrightarrow\quad
\dim\ker(J(z))=1
\quad\Longrightarrow\quad
\boxed{\;\operatorname{rank}(J(z))=n-1\;}\quad (n\ge 2).
$$

So **within the domain $z\in\mathbb{R}^n$** and for $n\ge 2$, the rank is **never** strictly less than $n-1$. A rank drop can occur only in degenerate *limits* where some $s_i\to 0$ (e.g. some $z_i\to -\infty$), which are not attained at finite $z$. (For completeness: if $n=1$, then $s(z)=(1)$ and $J(z)=[0]$, so $\operatorname{rank}(J(z))=0$.)

---

## Final answers (collected)

- **(1)** $\displaystyle \frac{\partial s_i}{\partial z_k}(z)=\delta_{ik}s_i(z)-s_i(z)s_k(z)=s_i(z)\big(\delta_{ik}-s_k(z)\big)$.
- **(2)** $\displaystyle J(z)=\operatorname{Diag}(s(z)) - s(z)s(z)^\top$ (also $=\operatorname{Diag}(s(z))(I-\mathbf{1}s(z)^\top)$).
- **(3)** $\sum_i s_i(z)=1$ and $\mathbf{1}^\top J(z)=0^\top$, hence $\operatorname{rank}(J(z))\le n-1$.
- **(4)** For $n\ge 2$ and all finite $z$, $\operatorname{rank}(J(z))=n-1$; it is strictly smaller only in boundary/limit cases where some $s_i\to 0$ (not at finite $z$).

---

> **Note:** This solution may contain errors or incomplete reasoning. The following issues were detected:
> - Solution lacks mathematical notation.
>
> Please verify the steps independently.","## 1) Елементи якобіана

Нехай
$$
S(z):=\sum_{j=1}^n e^{z_j},\qquad s_i(z)=\frac{e^{z_i}}{S(z)}.
$$
Тоді
$$
\frac{\partial}{\partial z_k}e^{z_i}=\delta_{ik}e^{z_i},
\qquad
\frac{\partial S}{\partial z_k}=e^{z_k},
$$
і
$$
\frac{\partial}{\partial z_k}\big(S^{-1}\big)=-S^{-2}\frac{\partial S}{\partial z_k}=-S^{-2}e^{z_k}.
$$
Продиференціюємо $s_i=e^{z_i}S^{-1}$:
$$
\frac{\partial s_i}{\partial z_k}
=\delta_{ik}e^{z_i}S^{-1}+e^{z_i}\left(-S^{-2}e^{z_k}\right)
=\delta_{ik}\frac{e^{z_i}}{S}-\frac{e^{z_i}e^{z_k}}{S^2}.
$$
Оскільки $s_i=\frac{e^{z_i}}{S}$ і $s_k=\frac{e^{z_k}}{S}$, це набуває вигляду
$$
\boxed{\;\frac{\partial s_i}{\partial z_k}(z)=\delta_{ik}\,s_i(z)-s_i(z)s_k(z)
=s_i(z)\big(\delta_{ik}-s_k(z)\big).\;}
$$

Еквівалентно, покомпонентно:
$$
\frac{\partial s_i}{\partial z_k}(z)=
\begin{cases}
s_i(z)\big(1-s_i(z)\big), & k=i,\$$
4pt]
-\,s_i(z)s_k(z), & k\neq i.
\end{cases}
$$
Отже, якобіан $J(z)\in\mathbb{R}^{n\times n}$ має елементи
$$
\boxed{\;J_{ik}(z)=\frac{\partial s_i}{\partial z_k}(z)=s_i(z)\big(\delta_{ik}-s_k(z)\big).\;}
$$

---

## 2) Матрична форма

Нехай $s:=s(z)\in\mathbb{R}^n$. Зауважимо, що
$$
(\operatorname{Diag}(s))_{ik}=s_i\delta_{ik},
\qquad
(ss^\top)_{ik}=s_is_k.
$$
Тому
$$
J_{ik}=s_i\delta_{ik}-s_is_k
=\big(\operatorname{Diag}(s)-ss^\top\big)_{ik},
$$
отже
$$
\boxed{\;J(z)=\operatorname{Diag}(s(z)) - s(z)s(z)^\top.\;}
$$
Корисна еквівалентна факторизація:
$$
\boxed{\;J(z)=\operatorname{Diag}(s(z))\big(I-\mathbf{1}s(z)^\top\big),\;}
$$
оскільки $\operatorname{Diag}(s)\mathbf{1}=s$.

---

## 3) Обмеження симплекса $\Rightarrow$ дефіцит рангу

Спочатку
$$
\sum_{i=1}^n s_i(z)=\sum_{i=1}^n \frac{e^{z_i}}{S(z)}
=\frac{\sum_{i=1}^n e^{z_i}}{S(z)}
=\frac{S(z)}{S(z)}=1
\qquad\forall z\in\mathbb{R}^n.
$$
У векторній формі це
$$
\mathbf{1}^\top s(z)=1 \qquad \forall z.
$$
Продиференціюємо обидві частини за $z$. Права частина є сталою, отже має похідну $0$, тоді як ліва частина дає
$$
\frac{\partial}{\partial z}\big(\mathbf{1}^\top s(z)\big)=\mathbf{1}^\top J(z).
$$
Отже
$$
\boxed{\;\mathbf{1}^\top J(z)=\mathbf{0}^\top,\;}
$$
тому $\mathbf{1}\neq 0$ належить **лівому ядру** $J(z)$. Як наслідок,
$$
\dim\ker(J(z)^\top)\ge 1
\quad\Longrightarrow\quad
\operatorname{rank}(J(z))=\operatorname{rank}(J(z)^\top)\le n-1.
$$
Звідси
$$
\boxed{\;\operatorname{rank}(J(z))\le n-1\quad \forall z\in\mathbb{R}^n.\;}
$$

---

## 4) Необов’язкове посилення: коли $\operatorname{rank}(J(z))<n-1$?

Для будь-якого скінченного $z\in\mathbb{R}^n$ softmax задовольняє $s_i(z)>0$ для всіх $i$. Використовуючи
$$
J=\operatorname{Diag}(s)-ss^\top,
$$
припустімо, що $Jv=0$. Тоді
$$
(\operatorname{Diag}(s)-ss^\top)v=0
\quad\Longleftrightarrow\quad
\operatorname{Diag}(s)v=s(s^\top v).
$$
Нехай $\alpha:=s^\top v$. Покомпонентно,
$$
s_i v_i=\alpha s_i \quad (i=1,\dots,n).
$$
Оскільки $s_i>0$, можемо поділити й отримати $v_i=\alpha$ для всіх $i$, тобто
$$
v=\alpha \mathbf{1}.
$$
Отже, для скінченного $z$,
$$
\ker(J(z))=\operatorname{span}\{\mathbf{1}\}
\quad\Longrightarrow\quad
\dim\ker(J(z))=1
\quad\Longrightarrow\quad
\boxed{\;\operatorname{rank}(J(z))=n-1\;}\quad (n\ge 2).
$$

Отже, **в межах області $z\in\mathbb{R}^n$** та для $n\ge 2$ ранг **ніколи** не є строго меншим за $n-1$. Зменшення рангу можливе лише в вироджених *границях*, де деякі $s_i\to 0$ (наприклад, деякі $z_i\to -\infty$), яких не досягають при скінченному $z$. (Для повноти: якщо $n=1$, тоді $s(z)=(1)$ і $J(z)=[0]$, тож $\operatorname{rank}(J(z))=0$.)

---

## Остаточні відповіді (зібрано)

- **(1)** $\displaystyle \frac{\partial s_i}{\partial z_k}(z)=\delta_{ik}s_i(z)-s_i(z)s_k(z)=s_i(z)\big(\delta_{ik}-s_k(z)\big)$.
- **(2)** $\displaystyle J(z)=\operatorname{Diag}(s(z)) - s(z)s(z)^\top$ (також $=\operatorname{Diag}(s(z))(I-\mathbf{1}s(z)^\top)$).
- **(3)** $\sum_i s_i(z)=1$ і $\mathbf{1}^\top J(z)=0^\top$, отже $\operatorname{rank}(J(z))\le n-1$.
- **(4)** Для $n\ge 2$ і всіх скінченних $z$, $\operatorname{rank}(J(z))=n-1$; він є строго меншим лише у граничних/крайових випадках, де деякі $s_i\to 0$ (не при скінченному $z$).

---

> **Примітка:** Це розв’язання може містити помилки або неповні міркування. Було виявлено такі проблеми:
> - Розв’язання не містить математичних позначень.
>
> Будь ласка, перевірте кроки незалежно.",2,Solution lacks mathematical notation.,2725,86254,True
17,calculus,Deep Learning Inspired,medium,"Consider a dataset of $n$ input vectors $x_1,\dots,x_n\in\mathbb{R}^d$ and corresponding targets $y_1,\dots,y_n\in\mathbb{R}^m$. A simple linear network is $f_W(x)=Wx$ with parameter matrix $W\in\mathbb{R}^{m\times d}$. Define the squared loss
$$
L(W)=\frac12\sum_{i=1}^n \|Wx_i-y_i\|_2^2.
$$
Let $X\in\mathbb{R}^{d\times n}$ be the data matrix whose $i$th column is $x_i$, and let $Y\in\mathbb{R}^{m\times n}$ be the target matrix whose $i$th column is $y_i$.

1) Rewrite the loss in matrix form using the Frobenius norm (i.e., express $L(W)$ in terms of $WX-Y$).

2) Compute the gradient $\nabla_W L(W)$.

3) View $L$ as a function of $w=\mathrm{vec}(W)\in\mathbb{R}^{md}$ (column-stacking vectorization). Compute the Hessian matrix $H=\nabla_w^2 L(W)$ and express it using a Kronecker product involving $X$.

4) Prove that $H$ is positive semidefinite for all $X$.

5) Determine necessary and sufficient conditions on the rank of $X$ for $H$ to be strictly positive definite (i.e., for $L$ to be strictly convex in $W$ / $w$). State the condition explicitly in terms of $\mathrm{rank}(X)$ and the dimensions $d,n$.

6) If $X$ does not satisfy the condition in part (5), describe (in terms of linear algebra, e.g., a null space condition) a nonzero perturbation $\Delta W\neq 0$ such that $L(W+\Delta W)=L(W)$ for all $W$.","Розглянемо набір даних із $n$ вхідних векторів $x_1,\dots,x_n\in\mathbb{R}^d$ та відповідних цільових значень $y_1,\dots,y_n\in\mathbb{R}^m$. Проста лінійна мережа має вигляд $f_W(x)=Wx$ з матрицею параметрів $W\in\mathbb{R}^{m\times d}$. Визначимо квадратичну функцію втрат
$$
L(W)=\frac12\sum_{i=1}^n \|Wx_i-y_i\|_2^2.
$$
Нехай $X\in\mathbb{R}^{d\times n}$ — матриця даних, $i$-й стовпець якої дорівнює $x_i$, і нехай $Y\in\mathbb{R}^{m\times n}$ — матриця цільових значень, $i$-й стовпець якої дорівнює $y_i$.

1) Перепишіть функцію втрат у матричному вигляді, використовуючи норму Фробеніуса (тобто виразіть $L(W)$ через $WX-Y$).

2) Обчисліть градієнт $\nabla_W L(W)$.

3) Розгляньте $L$ як функцію від $w=\mathrm{vec}(W)\in\mathbb{R}^{md}$ (векторизація шляхом послідовного запису стовпців). Обчисліть матрицю Гессе $H=\nabla_w^2 L(W)$ та виразіть її за допомогою добутку Кронекера, що включає $X$.

4) Доведіть, що $H$ є додатно напіввизначеною для всіх $X$.

5) Визначте необхідні та достатні умови на ранг $X$, за яких $H$ є строго додатно визначеною (тобто $L$ є строго опуклою за $W$ / $w$). Сформулюйте умову явно через $\mathrm{rank}(X)$ та розміри $d,n$.

6) Якщо $X$ не задовольняє умову з пункту (5), опишіть (у термінах лінійної алгебри, наприклад, через умову на нульовий простір) ненульове збурення $\Delta W\neq 0$ таке, що $L(W+\Delta W)=L(W)$ для всіх $W$.","## 1) Matrix form of the loss

Stack inputs and targets as
$$
X=[x_1,\dots,x_n]\in\mathbb{R}^{d\times n},\qquad
Y=[y_1,\dots,y_n]\in\mathbb{R}^{m\times n}.
$$
Then
$$
WX=[Wx_1,\dots,Wx_n],\qquad WX-Y=[Wx_1-y_1,\dots,Wx_n-y_n].
$$
Using $\|A\|_F^2=\sum_{i=1}^n\|a_i\|_2^2$ for columns $a_i$ of $A$,
$$
\boxed{\,L(W)=\frac12\|WX-Y\|_F^2\, }.
$$
Equivalently,
$$
L(W)=\frac12\,\mathrm{tr}\big((WX-Y)^\top(WX-Y)\big).
$$

---

## 2) Gradient $\nabla_W L(W)$

Let $E:=WX-Y$. Then $L(W)=\tfrac12\|E\|_F^2$. The differential satisfies
$$
dL=\langle E,dE\rangle_F=\mathrm{tr}(E^\top dE),\qquad dE=d(WX)=(dW)X.
$$
So
$$
dL=\mathrm{tr}\big(E^\top(dW)X\big)=\mathrm{tr}\big((EX^\top)^\top dW\big)=\langle EX^\top,dW\rangle_F.
$$
Hence
$$
\boxed{\,\nabla_W L(W)=(WX-Y)X^\top\, } \in \mathbb{R}^{m\times d}.
$$

---

## 3) Hessian in vectorized coordinates $w=\mathrm{vec}(W)$

Use the identity $\mathrm{vec}(AXB)=(B^\top\otimes A)\mathrm{vec}(X)$. Since $WX=I_m\cdot W\cdot X$,
$$
\mathrm{vec}(WX)=(X^\top\otimes I_m)\mathrm{vec}(W)=(X^\top\otimes I_m)w.
$$
Thus
$$
\mathrm{vec}(WX-Y)=(X^\top\otimes I_m)w-\mathrm{vec}(Y).
$$
Because $\|A\|_F=\|\mathrm{vec}(A)\|_2$,
$$
L(w)=\frac12\left\|(X^\top\otimes I_m)w-\mathrm{vec}(Y)\right\|_2^2.
$$
Let $A:=X^\top\otimes I_m$ and $b:=\mathrm{vec}(Y)$. For $L(w)=\tfrac12\|Aw-b\|_2^2$, the Hessian is $H=A^\top A$. Therefore
$$
H=(X^\top\otimes I_m)^\top(X^\top\otimes I_m)=(X\otimes I_m)(X^\top\otimes I_m).
$$
Using $(P\otimes Q)(R\otimes S)=(PR)\otimes(QS)$,
$$
\boxed{\,H=\nabla_w^2 L(W)=(XX^\top)\otimes I_m\, } \in \mathbb{R}^{(md)\times(md)}.
$$

---

## 4) $H$ is positive semidefinite for all $X$

Since $H=A^\top A$ with $A=X^\top\otimes I_m$, for any $v\in\mathbb{R}^{md}$,
$$
v^\top H v=v^\top A^\top A v=\|Av\|_2^2\ge 0.
$$
Hence
$$
\boxed{\,H\succeq 0\ \text{for all }X\, }.
$$

---

## 5) When is $H$ strictly positive definite? (strict convexity)

We have $H=A^\top A$, so
$$
H\succ 0 \iff \mathrm{null}(A)=\{0\} \iff \mathrm{rank}(A)=md.
$$
Now use $\mathrm{rank}(B\otimes C)=\mathrm{rank}(B)\mathrm{rank}(C)$:
$$
\mathrm{rank}(A)=\mathrm{rank}(X^\top\otimes I_m)=\mathrm{rank}(X^\top)\mathrm{rank}(I_m)=\mathrm{rank}(X)\cdot m.
$$
Thus
$$
\mathrm{rank}(A)=md \iff m\,\mathrm{rank}(X)=md \iff \boxed{\,\mathrm{rank}(X)=d\, }.
$$
Because $\mathrm{rank}(X)\le \min\{d,n\}$, this is possible only if $n\ge d$. So the necessary and sufficient condition is:
$$
\boxed{\,H\succ 0 \iff \mathrm{rank}(X)=d\ (\text{full row rank, hence }n\ge d)\, }.
$$

---

## 6) If $\mathrm{rank}(X)<d$: a nonzero $\Delta W$ with $L(W+\Delta W)=L(W)$

If $\mathrm{rank}(X)<d$, then $\mathrm{null}(X^\top)\neq\{0\}$. Pick $u\in\mathbb{R}^d$, $u\neq 0$, such that
$$
X^\top u=0 \quad\Longleftrightarrow\quad u^\top X=0^\top.
$$
Choose any $a\in\mathbb{R}^m$, $a\neq 0$, and define
$$
\boxed{\,\Delta W:=a\,u^\top\neq 0\, }.
$$
Then
$$
\Delta W X=(a u^\top)X=a(u^\top X)=a\cdot 0^\top=0,
$$
so $(W+\Delta W)X=WX$, hence the residual and loss are unchanged:
$$
L(W+\Delta W)=\frac12\|(W+\Delta W)X-Y\|_F^2=\frac12\|WX-Y\|_F^2=L(W)
\quad\text{for all }W.
$$

---

### Final summary
- $L(W)=\tfrac12\|WX-Y\|_F^2$.
- $\nabla_W L(W)=(WX-Y)X^\top$.
- With $w=\mathrm{vec}(W)$, $H=(XX^\top)\otimes I_m\succeq 0$.
- $H\succ 0$ (strict convexity) iff $\mathrm{rank}(X)=d$ (thus $n\ge d$).
- If $\mathrm{rank}(X)<d$, any $\Delta W=a u^\top$ with $u\in\mathrm{null}(X^\top)\setminus\{0\}$, $a\neq 0$, satisfies $L(W+\Delta W)=L(W)$ for all $W$.

---

> **Note:** This solution may contain errors or incomplete reasoning. The following issues were detected:
> - Solution lacks mathematical notation.
>
> Please verify the steps independently.","## 1) Матрична форма функції втрат

Складемо входи та цілі як
$$
X=[x_1,\dots,x_n]\in\mathbb{R}^{d\times n},\qquad
Y=[y_1,\dots,y_n]\in\mathbb{R}^{m\times n}.
$$
Тоді
$$
WX=[Wx_1,\dots,Wx_n],\qquad WX-Y=[Wx_1-y_1,\dots,Wx_n-y_n].
$$
Використовуючи $\|A\|_F^2=\sum_{i=1}^n\|a_i\|_2^2$ для стовпців $a_i$ матриці $A$,
$$
\boxed{\,L(W)=\frac12\|WX-Y\|_F^2\, }.
$$
Еквівалентно,
$$
L(W)=\frac12\,\mathrm{tr}\big((WX-Y)^\top(WX-Y)\big).
$$

---

## 2) Градієнт $\nabla_W L(W)$

Нехай $E:=WX-Y$. Тоді $L(W)=\tfrac12\|E\|_F^2$. Диференціал задовольняє
$$
dL=\langle E,dE\rangle_F=\mathrm{tr}(E^\top dE),\qquad dE=d(WX)=(dW)X.
$$
Отже
$$
dL=\mathrm{tr}\big(E^\top(dW)X\big)=\mathrm{tr}\big((EX^\top)^\top dW\big)=\langle EX^\top,dW\rangle_F.
$$
Звідси
$$
\boxed{\,\nabla_W L(W)=(WX-Y)X^\top\, } \in \mathbb{R}^{m\times d}.
$$

---

## 3) Гессіан у векторизованих координатах $w=\mathrm{vec}(W)$

Використаємо тотожність $\mathrm{vec}(AXB)=(B^\top\otimes A)\mathrm{vec}(X)$. Оскільки $WX=I_m\cdot W\cdot X$,
$$
\mathrm{vec}(WX)=(X^\top\otimes I_m)\mathrm{vec}(W)=(X^\top\otimes I_m)w.
$$
Отже
$$
\mathrm{vec}(WX-Y)=(X^\top\otimes I_m)w-\mathrm{vec}(Y).
$$
Оскільки $\|A\|_F=\|\mathrm{vec}(A)\|_2$,
$$
L(w)=\frac12\left\|(X^\top\otimes I_m)w-\mathrm{vec}(Y)\right\|_2^2.
$$
Нехай $A:=X^\top\otimes I_m$ та $b:=\mathrm{vec}(Y)$. Для $L(w)=\tfrac12\|Aw-b\|_2^2$ гессіан дорівнює $H=A^\top A$. Тому
$$
H=(X^\top\otimes I_m)^\top(X^\top\otimes I_m)=(X\otimes I_m)(X^\top\otimes I_m).
$$
Використовуючи $(P\otimes Q)(R\otimes S)=(PR)\otimes(QS)$,
$$
\boxed{\,H=\nabla_w^2 L(W)=(XX^\top)\otimes I_m\, } \in \mathbb{R}^{(md)\times(md)}.
$$

---

## 4) $H$ є додатно напіввизначеною для всіх $X$

Оскільки $H=A^\top A$ з $A=X^\top\otimes I_m$, для будь-якого $v\in\mathbb{R}^{md}$,
$$
v^\top H v=v^\top A^\top A v=\|Av\|_2^2\ge 0.
$$
Отже
$$
\boxed{\,H\succeq 0\ \text{для всіх }X\, }.
$$

---

## 5) Коли $H$ є строго додатно визначеною? (строга опуклість)

Маємо $H=A^\top A$, тому
$$
H\succ 0 \iff \mathrm{null}(A)=\{0\} \iff \mathrm{rank}(A)=md.
$$
Тепер використаємо $\mathrm{rank}(B\otimes C)=\mathrm{rank}(B)\mathrm{rank}(C)$:
$$
\mathrm{rank}(A)=\mathrm{rank}(X^\top\otimes I_m)=\mathrm{rank}(X^\top)\mathrm{rank}(I_m)=\mathrm{rank}(X)\cdot m.
$$
Отже
$$
\mathrm{rank}(A)=md \iff m\,\mathrm{rank}(X)=md \iff \boxed{\,\mathrm{rank}(X)=d\, }.
$$
Оскільки $\mathrm{rank}(X)\le \min\{d,n\}$, це можливо лише якщо $n\ge d$. Тож необхідна і достатня умова така:
$$
\boxed{\,H\succ 0 \iff \mathrm{rank}(X)=d\ (\text{повний рядковий ранг, отже }n\ge d)\, }.
$$

---

## 6) Якщо $\mathrm{rank}(X)<d$: ненульова $\Delta W$ з $L(W+\Delta W)=L(W)$

Якщо $\mathrm{rank}(X)<d$, тоді $\mathrm{null}(X^\top)\neq\{0\}$. Виберемо $u\in\mathbb{R}^d$, $u\neq 0$, таке що
$$
X^\top u=0 \quad\Longleftrightarrow\quad u^\top X=0^\top.
$$
Виберемо будь-яке $a\in\mathbb{R}^m$, $a\neq 0$, і визначимо
$$
\boxed{\,\Delta W:=a\,u^\top\neq 0\, }.
$$
Тоді
$$
\Delta W X=(a u^\top)X=a(u^\top X)=a\cdot 0^\top=0,
$$
отже $(W+\Delta W)X=WX$, тому нев’язка та функція втрат не змінюються:
$$
L(W+\Delta W)=\frac12\|(W+\Delta W)X-Y\|_F^2=\frac12\|WX-Y\|_F^2=L(W)
\quad\text{для всіх }W.
$$

---

### Підсумок
- $L(W)=\tfrac12\|WX-Y\|_F^2$.
- $\nabla_W L(W)=(WX-Y)X^\top$.
- За $w=\mathrm{vec}(W)$, $H=(XX^\top)\otimes I_m\succeq 0$.
- $H\succ 0$ (строга опуклість) тоді й лише тоді, коли $\mathrm{rank}(X)=d$ (отже $n\ge d$).
- Якщо $\mathrm{rank}(X)<d$, будь-яка $\Delta W=a u^\top$ з $u\in\mathrm{null}(X^\top)\setminus\{0\}$, $a\neq 0$, задовольняє $L(W+\Delta W)=L(W)$ для всіх $W$.

---

> **Примітка:** Це розв’язання може містити помилки або неповні міркування. Було виявлено такі проблеми:
> - Розв’язання не містить математичних позначень.
>
> Будь ласка, перевірте кроки самостійно.",0,Solution lacks mathematical notation.,3139,74800,True
18,calculus,Deep Learning Inspired,medium,"In a simplified “dropout” model, let $m\sim\mathrm{Bernoulli}(p)$ with $\mathbb{P}(m=1)=p$ and $\mathbb{P}(m=0)=1-p$, independent of $x\in\mathbb{R}$. For a given activation function $\sigma$, define
$$f(x)=\mathbb{E}[\sigma(mx)].$$

1) Take the smooth activation $\sigma_\mathrm{smooth}(t)=\tanh(t)$. Compute $f(x)$ explicitly in terms of $p$ and $x$, and then compute $f'(0)$.

2) Take the non-smooth activation $\sigma_\mathrm{ReLU}(t)=\max\{0,t\}$. Compute $f(x)$ explicitly in terms of $p$ and $x$.

3) For the ReLU case, compute the one-sided derivatives $f'(0^-)$ and $f'(0^+)$ and determine whether $f'(0)$ exists.

4) Briefly compare your results for $f'(0)$ (or one-sided derivatives) in parts (1) and (3), emphasizing how the non-smoothness of ReLU affects the expected gradient at $0$ under dropout.","У спрощеній моделі «dropout» нехай $m\sim\mathrm{Bernoulli}(p)$ з $\mathbb{P}(m=1)=p$ та $\mathbb{P}(m=0)=1-p$, незалежна від $x\in\mathbb{R}$. Для заданої функції активації $\sigma$ визначимо
$$f(x)=\mathbb{E}[\sigma(mx)].$$

1) Візьміть гладку активацію $\sigma_\mathrm{smooth}(t)=\tanh(t)$. Обчисліть $f(x)$ явно в термінах $p$ та $x$, а потім обчисліть $f'(0)$.

2) Візьміть негладку активацію $\sigma_\mathrm{ReLU}(t)=\max\{0,t\}$. Обчисліть $f(x)$ явно в термінах $p$ та $x$.

3) Для випадку ReLU обчисліть однобічні похідні $f'(0^-)$ та $f'(0^+)$ і визначте, чи існує $f'(0)$.

4) Коротко порівняйте ваші результати для $f'(0)$ (або однобічних похідних) у пунктах (1) та (3), підкреслюючи, як негладкість ReLU впливає на математичне сподівання градієнта в точці $0$ за наявності dropout.","## Key simplification (Bernoulli dropout)

Since $m\sim\mathrm{Bernoulli}(p)$ takes values in $\{0,1\}$,
$$
f(x)=\mathbb{E}[\sigma(mx)]
=\sigma(x)\,\mathbb{P}(m=1)+\sigma(0)\,\mathbb{P}(m=0)
= p\,\sigma(x)+(1-p)\,\sigma(0).
$$
All parts follow by plugging in the chosen $\sigma$.

---

## 1) Smooth activation $\sigma_{\mathrm{smooth}}(t)=\tanh(t)$

Using $\tanh(0)=0$,
$$
f(x)=p\,\tanh(x)+(1-p)\tanh(0)=p\,\tanh(x).
$$

Differentiate:
$$
f'(x)=p\,\frac{d}{dx}\tanh(x)=p\,\operatorname{sech}^2(x).
$$
Since $\operatorname{sech}(0)=1$,
$$
\boxed{f'(0)=p.}
$$

---

## 2) ReLU activation $\sigma_{\mathrm{ReLU}}(t)=\max\{0,t\}$

Using $\mathrm{ReLU}(0)=0$,
$$
f(x)=p\,\max\{0,x\}+(1-p)\max\{0,0\}=p\,\max\{0,x\}.
$$
Equivalently,
$$
\boxed{
f(x)=
\begin{cases}
0, & x\le 0,\$$
4pt]
p\,x, & x>0.
\end{cases}}
$$

---

## 3) One-sided derivatives at $0$ for ReLU

From the piecewise form and $f(0)=0$:

- Left derivative:
$$
f'(0^-)=\lim_{h\to 0^-}\frac{f(h)-f(0)}{h}
=\lim_{h\to 0^-}\frac{0-0}{h}=0.
$$

- Right derivative:
$$
f'(0^+)=\lim_{h\to 0^+}\frac{f(h)-f(0)}{h}
=\lim_{h\to 0^+}\frac{p h-0}{h}=p.
$$

So
$$
\boxed{f'(0^-)=0,\qquad f'(0^+)=p.}
$$
Hence $f'(0)$ exists iff $f'(0^-)=f'(0^+)$, i.e. iff $p=0$. For any $p\in(0,1]$, the two-sided derivative does **not** exist:
$$
\boxed{f'(0)\ \text{does not exist for }p>0.}
$$

---

## 4) Comparison: smooth vs. non-smooth at $0$ under dropout

- **Smooth $\tanh$:** the expectation remains smooth:
  $$
f(x)=p\,\tanh(x)\quad\Rightarrow\quad \boxed{f'(0)=p}.
$$
- **ReLU:** the kink at $0$ survives taking the expectation:
  $$
f(x)=p\,\max\{0,x\}\quad\Rightarrow\quad \boxed{f'(0^-)=0,\ f'(0^+)=p},
$$
  so dropout scales the positive-side slope by $p$ but does **not** “smooth out” the nondifferentiability at $0$ (unless $p=0$, in which case $f\equiv 0$).","## Ключове спрощення (бернуллівський dropout)

Оскільки $m\sim\mathrm{Bernoulli}(p)$ набуває значень у $\{0,1\}$,
$$
f(x)=\mathbb{E}[\sigma(mx)]
=\sigma(x)\,\mathbb{P}(m=1)+\sigma(0)\,\mathbb{P}(m=0)
= p\,\sigma(x)+(1-p)\,\sigma(0).
$$
Усі частини отримуються підстановкою вибраної $\sigma$.

---

## 1) Гладка активація $\sigma_{\mathrm{smooth}}(t)=\tanh(t)$

Використовуючи $\tanh(0)=0$,
$$
f(x)=p\,\tanh(x)+(1-p)\tanh(0)=p\,\tanh(x).
$$

Диференціюємо:
$$
f'(x)=p\,\frac{d}{dx}\tanh(x)=p\,\operatorname{sech}^2(x).
$$
Оскільки $\operatorname{sech}(0)=1$,
$$
\boxed{f'(0)=p.}
$$

---

## 2) Активація ReLU $\sigma_{\mathrm{ReLU}}(t)=\max\{0,t\}$

Використовуючи $\mathrm{ReLU}(0)=0$,
$$
f(x)=p\,\max\{0,x\}+(1-p)\max\{0,0\}=p\,\max\{0,x\}.
$$
Еквівалентно,
$$
\boxed{
f(x)=
\begin{cases}
0, & x\le 0,\$$
4pt]
p\,x, & x>0.
\end{cases}}
$$

---

## 3) Однобічні похідні в точці $0$ для ReLU

Із кусочно-заданого вигляду та $f(0)=0$:

- Ліва похідна:
$$
f'(0^-)=\lim_{h\to 0^-}\frac{f(h)-f(0)}{h}
=\lim_{h\to 0^-}\frac{0-0}{h}=0.
$$

- Права похідна:
$$
f'(0^+)=\lim_{h\to 0^+}\frac{f(h)-f(0)}{h}
=\lim_{h\to 0^+}\frac{p h-0}{h}=p.
$$

Отже
$$
\boxed{f'(0^-)=0,\qquad f'(0^+)=p.}
$$
Тому $f'(0)$ існує тоді й лише тоді, коли $f'(0^-)=f'(0^+)$, тобто тоді й лише тоді, коли $p=0$. Для будь-якого $p\in(0,1]$ двобічна похідна **не** існує:
$$
\boxed{f'(0)\ \text{не існує для }p>0.}
$$

---

## 4) Порівняння: гладка vs. негладка в точці $0$ під dropout

- **Гладка $\tanh$:** математичне сподівання залишається гладким:
  $$
f(x)=p\,\tanh(x)\quad\Rightarrow\quad \boxed{f'(0)=p}.
$$
- **ReLU:** злам у точці $0$ зберігається після взяття математичного сподівання:
  $$
f(x)=p\,\max\{0,x\}\quad\Rightarrow\quad \boxed{f'(0^-)=0,\ f'(0^+)=p},
$$
  тож dropout масштабує нахил з додатного боку на $p$, але **не** «згладжує» недиференційовність у точці $0$ (якщо тільки $p=0$, у якому разі $f\equiv 0$).",11,,2670,49663,True
19,calculus,Deep Learning Inspired,medium,"Consider the ReLU function $f(t)=\max\{0,t\}$.

1) Using the definition of derivative, compute the left-hand derivative $f'_-(0)$ and the right-hand derivative $f'_+(0)$ at $t=0$, and use these to explain (in words) why $f$ is not differentiable at $0$.

2) Define the set-valued subgradient at $0$ by
$$\partial f(0)=\{g\in\mathbb{R}: f(t)\ge f(0)+g(t-0)\ \text{for all }t\in\mathbb{R}\}.$$
Show that this definition yields an interval of valid slopes at $0$, and determine that interval.

3) Consider the one-dimensional subgradient descent update
$$t_{k+1}=t_k-\alpha g_k,$$
where $\alpha>0$ is fixed and $g_k\in\partial f(t_k)$.

(a) For $t_k>0$ and for $t_k<0$, specify what $\partial f(t_k)$ is and describe what the update does to $t_k$.

(b) For the nondifferentiable point $t_k=0$, take an arbitrary choice $g\in\partial f(0)$ and analyze when the update guarantees a non-increase in the objective, i.e., $f(t_{k+1})\le f(t_k)$.

4) In a short conceptual paragraph, connect your results to the deep-learning practice of assigning a ""gradient"" to ReLU at $0$ (often choosing $g=0$ or $g=1$): explain why classical differentiability fails there, and why a subgradient choice can still provide a valid descent direction for minimizing $f$ in one dimension.","Розгляньмо функцію ReLU $f(t)=\max\{0,t\}$.

1) Використовуючи означення похідної, обчисліть лівобічну похідну $f'_-(0)$ та правобічну похідну $f'_+(0)$ у точці $t=0$, і, спираючись на них, поясніть (словами), чому $f$ не є диференційовною в $0$.

2) Визначимо множиннозначний субградієнт у точці $0$ як
$$\partial f(0)=\{g\in\mathbb{R}: f(t)\ge f(0)+g(t-0)\ \text{для всіх }t\in\mathbb{R}\}.$$
Покажіть, що це означення дає інтервал допустимих нахилів у точці $0$, і визначте цей інтервал.

3) Розгляньмо одновимірне оновлення методу субградієнтного спуску
$$t_{k+1}=t_k-\alpha g_k,$$
де $\alpha>0$ є фіксованим і $g_k\in\partial f(t_k)$.

(a) Для $t_k>0$ та для $t_k<0$ вкажіть, чим є $\partial f(t_k)$, і опишіть, що це оновлення робить з $t_k$.

(b) Для недиференційовної точки $t_k=0$ візьміть довільний вибір $g\in\partial f(0)$ та проаналізуйте, коли оновлення гарантує незбільшення цільової функції, тобто $f(t_{k+1})\le f(t_k)$.

4) У короткому концептуальному абзаці пов’яжіть ваші результати з практикою глибинного навчання призначати «градієнт» для ReLU в точці $0$ (часто обираючи $g=0$ або $g=1$): поясніть, чому там не виконується класична диференційовність, і чому вибір субградієнта все ж може давати коректний напрям спуску для мінімізації $f$ в одному вимірі.","## 1) One-sided derivatives at $t=0$ and nondifferentiability

Write ReLU in piecewise form:
$$
f(t)=\max\{0,t\}=
\begin{cases}
0, & t\le 0,\\
t, & t\ge 0,
\end{cases}
\qquad\text{so } f(0)=0.
$$

### Left-hand derivative
$$
f'_-(0)=\lim_{h\to 0^-}\frac{f(h)-f(0)}{h}.
$$
For $h<0$, $f(h)=0$, so
$$
\frac{f(h)-f(0)}{h}=\frac{0-0}{h}=0
\quad\Longrightarrow\quad
f'_-(0)=0.
$$

### Right-hand derivative
$$
f'_+(0)=\lim_{h\to 0^+}\frac{f(h)-f(0)}{h}.
$$
For $h>0$, $f(h)=h$, so
$$
\frac{f(h)-f(0)}{h}=\frac{h-0}{h}=1
\quad\Longrightarrow\quad
f'_+(0)=1.
$$

Since $f'_-(0)\ne f'_+(0)$ (left slope $0$, right slope $1$), there is no single tangent slope at $0$. In words: ReLU has a “kink” at $0$, so it is **not differentiable** there.

---

## 2) Subgradient set $\partial f(0)$ and the interval of valid slopes

By definition,
$$
\partial f(0)=\left\{g\in\mathbb{R}:\ f(t)\ge f(0)+g(t-0)\ \forall t\in\mathbb{R}\right\}.
$$
Since $f(0)=0$, this becomes
$$
\partial f(0)=\{g\in\mathbb{R}:\ f(t)\ge gt\ \forall t\in\mathbb{R}\}.
$$

Check the inequality on the two regions:

- If $t>0$, then $f(t)=t$, so $t\ge gt$. Dividing by $t>0$ gives $1\ge g$, i.e. $g\le 1$.
- If $t<0$, then $f(t)=0$, so $0\ge gt$. Dividing by $t<0$ flips the inequality, giving $0\le g$, i.e. $g\ge 0$.

Combining,
$$
0\le g\le 1
\quad\Longrightarrow\quad
\partial f(0)=[0,1].
$$
Thus the subgradient at $0$ is an **interval** of supporting slopes, exactly between the left and right derivatives.

---

## 3) Subgradient descent $t_{k+1}=t_k-\alpha g_k$, $\alpha>0$, $g_k\in\partial f(t_k)$

### (a) Cases $t_k>0$ and $t_k<0$

**If $t_k>0$:** here $f(t)=t$, so $f$ is differentiable with derivative $1$, hence
$$
\partial f(t_k)=\{1\}.
$$
Therefore $g_k=1$ and the update is
$$
t_{k+1}=t_k-\alpha.
$$
So $t_k$ moves left by $\alpha$ each step, and while it stays positive the objective decreases by $\alpha$:
$$
f(t_{k+1})=t_{k+1}=t_k-\alpha=f(t_k)-\alpha.
$$
If the step crosses to $t_{k+1}\le 0$, then $f(t_{k+1})=0$.

**If $t_k<0$:** here $f(t)=0$ (constant), so the derivative is $0$, hence
$$
\partial f(t_k)=\{0\}.
$$
Thus $g_k=0$ and
$$
t_{k+1}=t_k.
$$
The iterate does not move, and $f(t_k)=0$ remains unchanged (non-increasing).

---

### (b) Nondifferentiable point $t_k=0$: when is $f(t_{k+1})\le f(t_k)$?

At $t_k=0$, we have $\partial f(0)=[0,1]$. Pick any $g\in[0,1]$. Then
$$
t_{k+1}=0-\alpha g=-\alpha g\le 0
\quad(\text{since }\alpha>0,\ g\ge 0).
$$
Hence
$$
f(t_{k+1})=\max\{0,t_{k+1}\}=\max\{0,-\alpha g\}=0.
$$
But $f(t_k)=f(0)=0$, so
$$
f(t_{k+1})=0\le 0=f(t_k).
$$
Therefore, **for every** $\alpha>0$ and **every** choice $g\in\partial f(0)$, the update guarantees non-increase (indeed, it keeps the objective at its minimum value $0$).

---

## 4) Conceptual connection to “choosing a gradient” for ReLU at $0$

Classical differentiability fails at $0$ because the left and right derivatives disagree: $f'_-(0)=0$ while $f'_+(0)=1$. Deep-learning code still needs a value to backpropagate, so it often assigns a “gradient” at $0$ (commonly $0$ or $1$). Subgradient theory explains why this is reasonable for optimization: at a kink of a convex function, the derivative is replaced by a **set** of valid slopes, and for ReLU this set is exactly
$$
\partial f(0)=[0,1].
$$
Picking $g=0$ or $g=1$ is simply selecting an element of this valid interval. In one-dimensional subgradient descent, any such choice at $0$ yields $t_{k+1}=-\alpha g\le 0$, so $f(t_{k+1})=0=f(0)$, i.e. a valid (non-increasing) descent behavior even though the classical derivative does not exist at that point.

---

### Final answers (collected)

1. $f'_-(0)=0$, $f'_+(0)=1$; since they differ, $f$ is not differentiable at $0$.  
2. $\partial f(0)=[0,1]$.  
3. (a) If $t_k>0$, $\partial f(t_k)=\{1\}$ and $t_{k+1}=t_k-\alpha$. If $t_k<0$, $\partial f(t_k)=\{0\}$ and $t_{k+1}=t_k$.  
   (b) If $t_k=0$, any $g\in[0,1]$ gives $t_{k+1}=-\alpha g\le 0$ and $f(t_{k+1})=0\le f(t_k)$.  
4. Deep-learning “gradients” at $0$ correspond to choosing a subgradient in $[0,1]$, which remains valid for descent despite nondifferentiability.

---

> **Note:** This solution may contain errors or incomplete reasoning. The following issues were detected:
> - Solution lacks mathematical notation.
>
> Please verify the steps independently.",,0,Solution lacks mathematical notation.,3032,73121,True
20,calculus,Deep Learning Inspired,medium,"Let $\sigma:\mathbb{R}\to(0,1)$ be the logistic sigmoid
$$\sigma(x)=\frac{1}{1+e^{-x}}.$$

1) Compute $\sigma'(x)$ and prove (using a calculus extremum argument) that $\sigma'(x)$ attains a global maximum value of $\tfrac14$ over all $x\in\mathbb{R}$.

2) For an integer $n\ge 1$, define the $n$-fold composition
$$f_n(x)=\underbrace{\sigma\circ\sigma\circ\cdots\circ\sigma}_{n\text{ times}}(x).$$
Use the chain rule together with the bound from part (1) to prove that for every $x\in\mathbb{R}$,
$$\bigl|f_n'(x)\bigr|\le \left(\tfrac14\right)^n.$$

3) Interpret the inequality in part (2) as a calculus-based explanation of the “vanishing gradient” phenomenon in deep networks built from many sigmoid layers: explain why gradients propagated through $n$ successive sigmoid nonlinearities must become very small as $n$ grows, even before considering any weights.","Нехай $\sigma:\mathbb{R}\to(0,1)$ — логістична сигмоїда
$$\sigma(x)=\frac{1}{1+e^{-x}}.$$

1) Обчисліть $\sigma'(x)$ і доведіть (використовуючи аргумент про екстремум із математичного аналізу), що $\sigma'(x)$ досягає глобального максимального значення $\tfrac14$ для всіх $x\in\mathbb{R}$.

2) Для цілого числа $n\ge 1$ визначимо $n$-кратну композицію
$$f_n(x)=\underbrace{\sigma\circ\sigma\circ\cdots\circ\sigma}_{n\text{ разів}}(x).$$
Використайте правило ланцюга разом з оцінкою з пункту (1), щоб довести, що для кожного $x\in\mathbb{R}$,
$$\bigl|f_n'(x)\bigr|\le \left(\tfrac14\right)^n.$$

3) Інтерпретуйте нерівність із пункту (2) як пояснення на основі математичного аналізу феномену «зникання градієнта» у глибоких мережах, побудованих з багатьох сигмоїдних шарів: поясніть, чому градієнти, що поширюються через $n$ послідовних сигмоїдних нелінійностей, мають ставати дуже малими зі зростанням $n$, навіть ще до врахування будь-яких ваг.","## 1) Compute $\sigma'(x)$ and show $\max_{x\in\mathbb{R}}\sigma'(x)=\tfrac14$

The logistic sigmoid is
$$
\sigma(x)=\frac{1}{1+e^{-x}}=(1+e^{-x})^{-1}.
$$
Differentiate:
$$
\sigma'(x)=-(1+e^{-x})^{-2}\cdot \frac{d}{dx}(1+e^{-x})
=-(1+e^{-x})^{-2}\cdot(-e^{-x})
=\frac{e^{-x}}{(1+e^{-x})^{2}}.
$$
A useful equivalent form is obtained by expressing it in terms of $\sigma(x)$. Note
$$
1-\sigma(x)=1-\frac{1}{1+e^{-x}}=\frac{e^{-x}}{1+e^{-x}},
$$
so
$$
\sigma(x)\bigl(1-\sigma(x)\bigr)
=\frac{1}{1+e^{-x}}\cdot \frac{e^{-x}}{1+e^{-x}}
=\frac{e^{-x}}{(1+e^{-x})^{2}}
=\sigma'(x).
$$
Hence
$$
\boxed{\;\sigma'(x)=\frac{e^{-x}}{(1+e^{-x})^{2}}=\sigma(x)\bigl(1-\sigma(x)\bigr)\;}
$$
and in particular $\sigma'(x)>0$ for all $x$.

To find the global maximum of $\sigma'(x)$, differentiate again using $\sigma'(x)=\sigma(x)(1-\sigma(x))$:
$$
\sigma''(x)=\frac{d}{dx}\bigl(\sigma(x)(1-\sigma(x))\bigr)
=\sigma'(x)(1-\sigma(x))-\sigma(x)\sigma'(x)
=\sigma'(x)\bigl(1-2\sigma(x)\bigr).
$$
Critical points satisfy $\sigma''(x)=0$, i.e.
$$
\sigma'(x)\bigl(1-2\sigma(x)\bigr)=0.
$$
Since $\sigma'(x)>0$ everywhere, we must have $1-2\sigma(x)=0$, so $\sigma(x)=\tfrac12$. Solving
$$
\frac{1}{1+e^{-x}}=\frac12 \iff e^{-x}=1 \iff x=0,
$$
the only critical point is $x=0$.

Now check the end behavior:
$$
\sigma'(x)=\frac{e^{-x}}{(1+e^{-x})^2}\to 0 \quad \text{as }x\to +\infty
$$
because $e^{-x}\to 0$, and also $\sigma'(x)\to 0$ as $x\to -\infty$ since with $t=e^{-x}\to\infty$,
$$
\frac{t}{(1+t)^2}=\frac{1/t}{(1+1/t)^2}\to 0.
$$
Thus $\sigma'$ is continuous, tends to $0$ at both $\pm\infty$, and has only one critical point, so the global maximum occurs at $x=0$. Evaluate:
$$
\sigma'(0)=\frac{e^{0}}{(1+e^{0})^{2}}=\frac{1}{(1+1)^2}=\frac14.
$$
Therefore,
$$
\boxed{\;\max_{x\in\mathbb{R}}\sigma'(x)=\frac14\ \text{(attained at }x=0\text{)}\;}
\quad\text{and}\quad
\boxed{\;0<\sigma'(x)\le \frac14\ \text{for all }x\in\mathbb{R}. \;}
$$

---

## 2) Bound $\lvert f_n'(x)\rvert$ for $f_n=\sigma^{\circ n}$

Define the iterates recursively:
$$
f_1(x)=\sigma(x),\qquad f_{k+1}(x)=\sigma(f_k(x)) \quad (k\ge 1).
$$
By the chain rule,
$$
f_{k+1}'(x)=\sigma'(f_k(x))\,f_k'(x),
$$
so taking absolute values,
$$
|f_{k+1}'(x)|=|\sigma'(f_k(x))|\,|f_k'(x)|.
$$
From part (1), $|\sigma'(u)|\le \tfrac14$ for every real $u$, hence in particular
$$
|\sigma'(f_k(x))|\le \frac14.
$$
Therefore, for all $x$,
$$
|f_{k+1}'(x)|\le \frac14\,|f_k'(x)|.
$$

Now prove by induction that $|f_n'(x)|\le (\tfrac14)^n$ for all $n\ge 1$.

- Base case $n=1$: $|f_1'(x)|=|\sigma'(x)|\le \tfrac14=(\tfrac14)^1$.
- Inductive step: if $|f_k'(x)|\le (\tfrac14)^k$, then
  $$
|f_{k+1}'(x)|\le \frac14\,|f_k'(x)|
  \le \frac14\left(\frac14\right)^k
  =\left(\frac14\right)^{k+1}.
$$

Thus, for every $n\ge 1$ and every $x\in\mathbb{R}$,
$$
\boxed{\;|f_n'(x)|\le \left(\frac14\right)^n.\;}
$$

---

## 3) Interpretation: vanishing gradients through many sigmoid layers

Backpropagation repeatedly applies the chain rule, so the gradient through a stack of $n$ sigmoid nonlinearities becomes a product of $n$ local derivatives:
$$
f_n'(x)=\sigma'(f_{n-1}(x))\cdot \sigma'(f_{n-2}(x))\cdots \sigma'(f_1(x))\cdot \sigma'(x).
$$
Each factor satisfies $\sigma'(\cdot)\le \tfrac14$, so the magnitude of the product is bounded by
$$
|f_n'(x)|\le \left(\frac14\right)^n,
$$
which decays to $0$ exponentially fast as $n\to\infty$. This is a calculus-based explanation of **vanishing gradients**: even *without* any weights, repeatedly composing sigmoids forces gradients propagated backward through many layers to become extremely small, making learning in early layers slow or ineffective.

---

> **Note:** This solution may contain errors or incomplete reasoning. The following issues were detected:
> - Solution lacks mathematical notation.
>
> Please verify the steps independently.","## 1) Обчислити $\sigma'(x)$ і показати $\max_{x\in\mathbb{R}}\sigma'(x)=\tfrac14$

Логістична сигмоїда має вигляд
$$
\sigma(x)=\frac{1}{1+e^{-x}}=(1+e^{-x})^{-1}.
$$
Диференціюємо:
$$
\sigma'(x)=-(1+e^{-x})^{-2}\cdot \frac{d}{dx}(1+e^{-x})
=-(1+e^{-x})^{-2}\cdot(-e^{-x})
=\frac{e^{-x}}{(1+e^{-x})^{2}}.
$$
Корисну еквівалентну форму отримуємо, виразивши похідну через $\sigma(x)$. Зауважимо, що
$$
1-\sigma(x)=1-\frac{1}{1+e^{-x}}=\frac{e^{-x}}{1+e^{-x}},
$$
тому
$$
\sigma(x)\bigl(1-\sigma(x)\bigr)
=\frac{1}{1+e^{-x}}\cdot \frac{e^{-x}}{1+e^{-x}}
=\frac{e^{-x}}{(1+e^{-x})^{2}}
=\sigma'(x).
$$
Отже,
$$
\boxed{\;\sigma'(x)=\frac{e^{-x}}{(1+e^{-x})^{2}}=\sigma(x)\bigl(1-\sigma(x)\bigr)\;}
$$
і, зокрема, $\sigma'(x)>0$ для всіх $x$.

Щоб знайти глобальний максимум $\sigma'(x)$, продиференціюємо ще раз, використовуючи $\sigma'(x)=\sigma(x)(1-\sigma(x))$:
$$
\sigma''(x)=\frac{d}{dx}\bigl(\sigma(x)(1-\sigma(x))\bigr)
=\sigma'(x)(1-\sigma(x))-\sigma(x)\sigma'(x)
=\sigma'(x)\bigl(1-2\sigma(x)\bigr).
$$
Критичні точки задовольняють $\sigma''(x)=0$, тобто
$$
\sigma'(x)\bigl(1-2\sigma(x)\bigr)=0.
$$
Оскільки $\sigma'(x)>0$ всюди, мусимо мати $1-2\sigma(x)=0$, отже $\sigma(x)=\tfrac12$. Розв’язуючи
$$
\frac{1}{1+e^{-x}}=\frac12 \iff e^{-x}=1 \iff x=0,
$$
бачимо, що єдина критична точка — це $x=0$.

Тепер перевіримо поведінку на нескінченності:
$$
\sigma'(x)=\frac{e^{-x}}{(1+e^{-x})^2}\to 0 \quad \text{коли }x\to +\infty
$$
оскільки $e^{-x}\to 0$, і також $\sigma'(x)\to 0$ коли $x\to -\infty$, бо при $t=e^{-x}\to\infty$ маємо
$$
\frac{t}{(1+t)^2}=\frac{1/t}{(1+1/t)^2}\to 0.
$$
Отже, $\sigma'$ є неперервною, прямує до $0$ при обох $\pm\infty$ і має лише одну критичну точку, тому глобальний максимум досягається при $x=0$. Обчислюємо:
$$
\sigma'(0)=\frac{e^{0}}{(1+e^{0})^{2}}=\frac{1}{(1+1)^2}=\frac14.
$$
Тому
$$
\boxed{\;\max_{x\in\mathbb{R}}\sigma'(x)=\frac14\ \text{(досягається при }x=0\text{)}\;}
\quad\text{і}\quad
\boxed{\;0<\sigma'(x)\le \frac14\ \text{для всіх }x\in\mathbb{R}. \;}
$$

---

## 2) Оцінити $\lvert f_n'(x)\rvert$ для $f_n=\sigma^{\circ n}$

Визначимо ітерації рекурсивно:
$$
f_1(x)=\sigma(x),\qquad f_{k+1}(x)=\sigma(f_k(x)) \quad (k\ge 1).
$$
За правилом ланцюга,
$$
f_{k+1}'(x)=\sigma'(f_k(x))\,f_k'(x),
$$
тому, беручи модулі,
$$
|f_{k+1}'(x)|=|\sigma'(f_k(x))|\,|f_k'(x)|.
$$
З пункту (1) маємо $|\sigma'(u)|\le \tfrac14$ для кожного дійсного $u$, отже, зокрема,
$$
|\sigma'(f_k(x))|\le \frac14.
$$
Тому для всіх $x$,
$$
|f_{k+1}'(x)|\le \frac14\,|f_k'(x)|.
$$

Тепер доведемо індукцією, що $|f_n'(x)|\le (\tfrac14)^n$ для всіх $n\ge 1$.

- База $n=1$: $|f_1'(x)|=|\sigma'(x)|\le \tfrac14=(\tfrac14)^1$.
- Індукційний крок: якщо $|f_k'(x)|\le (\tfrac14)^k$, тоді
  $$
|f_{k+1}'(x)|\le \frac14\,|f_k'(x)|
  \le \frac14\left(\frac14\right)^k
  =\left(\frac14\right)^{k+1}.
$$

Отже, для кожного $n\ge 1$ і кожного $x\in\mathbb{R}$,
$$
\boxed{\;|f_n'(x)|\le \left(\frac14\right)^n.\;}
$$

---

## 3) Інтерпретація: зникання градієнтів через багато сигмоїдних шарів

Алгоритм зворотного поширення помилки багаторазово застосовує правило ланцюга, тому градієнт через стек із $n$ сигмоїдних нелінійностей стає добутком $n$ локальних похідних:
$$
f_n'(x)=\sigma'(f_{n-1}(x))\cdot \sigma'(f_{n-2}(x))\cdots \sigma'(f_1(x))\cdot \sigma'(x).
$$
Кожний множник задовольняє $\sigma'(\cdot)\le \tfrac14$, тому модуль добутку обмежений зверху:
$$
|f_n'(x)|\le \left(\frac14\right)^n,
$$
що спадає до $0$ експоненційно швидко при $n\to\infty$. Це є поясненням на основі математичного аналізу явища **зникання градієнтів**: навіть *без* будь-яких ваг багаторазова композиція сигмоїд змушує градієнти, що поширюються назад через багато шарів, ставати надзвичайно малими, роблячи навчання в ранніх шарах повільним або неефективним.

---

> **Примітка:** Це розв’язання може містити помилки або неповні міркування. Було виявлено такі проблеми:
> - Розв’язання не містить математичних позначень.
>
> Будь ласка, перевірте кроки самостійно.",7,Solution lacks mathematical notation.,2477,72050,True
